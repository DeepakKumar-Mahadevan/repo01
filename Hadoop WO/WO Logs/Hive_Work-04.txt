hive (dkmdb01)> insert into table players1 values (7,'Raikkonen','Finland',36);              
Query ID = hduser_20160713195656_73ab6b46-1d6d-4b2c-aefc-6f0cc1ea2b9f
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1468419167049_0001, Tracking URL = http://Inceptez:8088/proxy/application_1468419167049_0001/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1468419167049_0001
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2016-07-13 19:57:24,176 Stage-1 map = 0%,  reduce = 0%
2016-07-13 19:57:50,912 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.88 sec
MapReduce Total cumulative CPU time: 3 seconds 880 msec
Ended Job = job_1468419167049_0001
Stage-4 is selected by condition resolver.
Stage-3 is filtered out by condition resolver.
Stage-5 is filtered out by condition resolver.
Moving data to: hdfs://localhost:54310/tmp/hive/hduser/1ee57378-3e5d-4923-96d5-2a568e55b11c/hive_2016-07-13_19-56-42_634_23397981967431666-1/-ext-10000
Loading data to table dkmdb01.players1
[Error 30017]: Skipping stats aggregation by error org.apache.hadoop.hive.ql.metadata.HiveException: [Error 30001]: StatsPublisher cannot be initialized. There was a error in the initialization of StatsPublisher, and retrying might help. If you dont want the query to fail because accurate statistics could not be collected, set hive.stats.reliable=false
Table dkmdb01.players1 stats: [numFiles=4, numRows=0, totalSize=191, rawDataSize=0]
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1   Cumulative CPU: 4.88 sec   HDFS Read: 298 HDFS Write: 23 SUCCESS
Total MapReduce CPU Time Spent: 4 seconds 880 msec
OK
_col0	_col1	_col2	_col3
Time taken: 73.404 seconds

hive (dkmdb01)> select * from players1;                                                      
OK
players1.pl_id	players1.pl_name	players1.pl_country	players1.pl_age
23	Beckham	England	42
10	Sachin	India	42
7	Raikkonen	Finland	36
10	Messi	Argentina	29
17	Federer	Switzerland	34
10	Rooney	England	30
7	Ronaldo	Portugal	31
7	Dhoni	India	35
12	Yuvraj	India	34
Time taken: 0.184 seconds, Fetched: 9 row(s)
hive (dkmdb01)> dfs -ls -R /user/hive/warehouse/dkmdb01.db/players1
              > ;
-rw-r--r--   1 hduser supergroup         22 2016-07-10 00:56 /user/hive/warehouse/dkmdb01.db/players1/000000_0
-rw-r--r--   1 hduser supergroup         19 2016-07-10 00:57 /user/hive/warehouse/dkmdb01.db/players1/000000_0_copy_1
-rw-r--r--   1 hduser supergroup         23 2016-07-13 19:57 /user/hive/warehouse/dkmdb01.db/players1/000000_0_copy_2
-rw-r--r--   1 hduser supergroup        127 2016-07-10 00:47 /user/hive/warehouse/dkmdb01.db/players1/players1v2.txt
hive (dkmdb01)> dfs -cat /user/hive/warehouse/dkmdb01.db/players1/*_2  
              > ;
7,Raikkonen,Finland,36
====================================================================================================
hive (dkmdb01)> insert into table employee1 values (7,'Raikkonen',2100000,current_timestamp);      
FAILED: SemanticException [Error 10293]: Unable to create temp file for insert values Expression of type TOK_TABLE_OR_COL not supported in insert/values

hive (dkmdb01)> insert into table employee1 values (7,'Raikkonen',2100000,unix_timestamp()); 
FAILED: SemanticException [Error 10293]: Unable to create temp file for insert values Expression of type TOK_FUNCTION not supported in insert/values

hive (dkmdb01)> insert into table employee1 select 7,'Raikkonen',2100000,current_timestamp();
FAILED: NullPointerException null

hive (dkmdb01)> insert into table employee1 select '7','Raikkonen','2100000',current_timestamp();
FAILED: NullPointerException null

hive (dkmdb01)> insert into employee1 select '7','Raikkonen','2100000',current_timestamp();        
FAILED: NullPointerException null

hive (dkmdb01)> insert into employee1 select 7,'Raikkonen',2100000,current_timestamp() from foo;
FAILED: ParseException line 1:12 missing TABLE at 'employee1' near '<EOF>'

hive (dkmdb01)> insert into table employee1 select 7,'Raikkonen',2100000,current_timestamp() from foo;
FAILED: SemanticException [Error 10001]: Line 1:82 Table not found 'foo'

hive (dkmdb01)> insert into table employee1 select 7,'Raikkonen',2100000,current_timestamp() from players1;
FAILED: SemanticException [Error 10011]: Line 1:57 Invalid function 'current_timestamp'

hive (dkmdb01)> insert into table employee1 select 7,'Raikkonen',2100000,from_unixtime(unix_timestamp()) from players1;
Query ID = hduser_20160713202222_8c6ef01b-fd9e-48d2-bdc7-d81064376f4f
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1468419167049_0002, Tracking URL = http://Inceptez:8088/proxy/application_1468419167049_0002/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1468419167049_0002
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2016-07-13 20:22:59,683 Stage-1 map = 0%,  reduce = 0%
2016-07-13 20:23:25,322 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 4.14 sec
MapReduce Total cumulative CPU time: 4 seconds 140 msec
Ended Job = job_1468419167049_0002
Stage-4 is selected by condition resolver.
Stage-3 is filtered out by condition resolver.
Stage-5 is filtered out by condition resolver.
Moving data to: hdfs://localhost:54310/tmp/hive/hduser/1ee57378-3e5d-4923-96d5-2a568e55b11c/hive_2016-07-13_20-22-35_119_5924945900900651425-1/-ext-10000
Loading data to table dkmdb01.employee1
[Error 30017]: Skipping stats aggregation by error org.apache.hadoop.hive.ql.metadata.HiveException: [Error 30001]: StatsPublisher cannot be initialized. There was a error in the initialization of StatsPublisher, and retrying might help. If you dont want the query to fail because accurate statistics could not be collected, set hive.stats.reliable=false
Table dkmdb01.employee1 stats: [numFiles=3, numRows=-1, totalSize=599, rawDataSize=-1]
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1   Cumulative CPU: 4.91 sec   HDFS Read: 700 HDFS Write: 360 SUCCESS
Total MapReduce CPU Time Spent: 4 seconds 910 msec
OK
_col0	_col1	_col2	_col3
Time taken: 53.835 seconds
hive (dkmdb01)> select * from employee1;
OK
employee1.emp_id	employee1.emp_name	employee1.emp_sal	employee1.row_ts
7	Raikkonen	2100000	2016-07-13 20:23:23
7	Raikkonen	2100000	2016-07-13 20:23:23
7	Raikkonen	2100000	2016-07-13 20:23:23
7	Raikkonen	2100000	2016-07-13 20:23:23
7	Raikkonen	2100000	2016-07-13 20:23:23
7	Raikkonen	2100000	2016-07-13 20:23:23
7	Raikkonen	2100000	2016-07-13 20:23:23
7	Raikkonen	2100000	2016-07-13 20:23:23
7	Raikkonen	2100000	2016-07-13 20:23:23
1	Deepak	75000	2016-06-13 11:36:27
2	Sachin	1500000	2016-06-13 11:37:51
3	Messi	2500000	2016-06-13 11:38:04
4	Federer	2000000	2016-06-13 12:18:19
5	Ronaldo	2750000	2016-06-18 02:58:31
6	Rooney	2000000	2016-07-09 10:41:36
Time taken: 0.113 seconds, Fetched: 15 row(s)
hive (dkmdb01)> delete from employee1 where emp_id = 7;
FAILED: SemanticException [Error 10297]: Attempt to do update or delete on table dkmdb01.employee1 that does not use an AcidOutputFormat or is not bucketed

hive (dkmdb01)> dfs -ls hdfs://localhost:54310/user/hduser/sqphv01/employee1
              > ;
Found 4 items
-rw-r--r--   1 hduser supergroup        360 2016-07-13 20:23 hdfs://localhost:54310/user/hduser/sqphv01/employee1/000000_0
-rw-r--r--   1 hduser supergroup          0 2016-07-09 16:04 hdfs://localhost:54310/user/hduser/sqphv01/employee1/_SUCCESS
-rw-r--r--   1 hduser supergroup        199 2016-07-09 16:04 hdfs://localhost:54310/user/hduser/sqphv01/employee1/part-m-00000
-rw-r--r--   1 hduser supergroup         40 2016-07-09 16:14 hdfs://localhost:54310/user/hduser/sqphv01/employee1/part-m-00001
hive (dkmdb01)> dfs -cat hdfs://localhost:54310/user/hduser/sqphv01/employee1/0*
              > ;
7,Raikkonen,2100000,2016-07-13 20:23:23
7,Raikkonen,2100000,2016-07-13 20:23:23
7,Raikkonen,2100000,2016-07-13 20:23:23
7,Raikkonen,2100000,2016-07-13 20:23:23
7,Raikkonen,2100000,2016-07-13 20:23:23
7,Raikkonen,2100000,2016-07-13 20:23:23
7,Raikkonen,2100000,2016-07-13 20:23:23
7,Raikkonen,2100000,2016-07-13 20:23:23
7,Raikkonen,2100000,2016-07-13 20:23:23
====================================================================================================
hive (dkmdb01)> dfs -rm hdfs://localhost:54310/user/hduser/sqphv01/employee1/0* 
              > ;
Deleted hdfs://localhost:54310/user/hduser/sqphv01/employee1/000000_0
hive (dkmdb01)> select * from employee1;                                        
OK
employee1.emp_id	employee1.emp_name	employee1.emp_sal	employee1.row_ts
1	Deepak	75000	2016-06-13 11:36:27
2	Sachin	1500000	2016-06-13 11:37:51
3	Messi	2500000	2016-06-13 11:38:04
4	Federer	2000000	2016-06-13 12:18:19
5	Ronaldo	2750000	2016-06-18 02:58:31
6	Rooney	2000000	2016-07-09 10:41:36
Time taken: 0.062 seconds, Fetched: 6 row(s)

hive (dkmdb01)> insert into table employee1 select 7,'Raikkonen',2100000,from_unixtime(unix_timestamp()) from players1;
Query ID = hduser_20160713203333_5c48f0ba-14df-41ff-910e-852def648d36
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1468419167049_0003, Tracking URL = http://Inceptez:8088/proxy/application_1468419167049_0003/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1468419167049_0003
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2016-07-13 20:33:16,840 Stage-1 map = 0%,  reduce = 0%
2016-07-13 20:33:25,549 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.75 sec
MapReduce Total cumulative CPU time: 1 seconds 750 msec
Ended Job = job_1468419167049_0003
Stage-4 is selected by condition resolver.
Stage-3 is filtered out by condition resolver.
Stage-5 is filtered out by condition resolver.
Moving data to: hdfs://localhost:54310/tmp/hive/hduser/1ee57378-3e5d-4923-96d5-2a568e55b11c/hive_2016-07-13_20-33-07_257_1249729247583590466-1/-ext-10000
Loading data to table dkmdb01.employee1
[Error 30017]: Skipping stats aggregation by error org.apache.hadoop.hive.ql.metadata.HiveException: [Error 30001]: StatsPublisher cannot be initialized. There was a error in the initialization of StatsPublisher, and retrying might help. If you dont want the query to fail because accurate statistics could not be collected, set hive.stats.reliable=false
Table dkmdb01.employee1 stats: [numFiles=3, numRows=-1, totalSize=599, rawDataSize=-1]
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1   Cumulative CPU: 1.75 sec   HDFS Read: 700 HDFS Write: 360 SUCCESS
Total MapReduce CPU Time Spent: 1 seconds 750 msec
OK
_col0	_col1	_col2	_col3
Time taken: 19.624 seconds
hive (dkmdb01)> select * from employee1;                                                                               
OK
employee1.emp_id	employee1.emp_name	employee1.emp_sal	employee1.row_ts
7	Raikkonen	2100000	2016-07-13 20:33:24
7	Raikkonen	2100000	2016-07-13 20:33:24
7	Raikkonen	2100000	2016-07-13 20:33:24
7	Raikkonen	2100000	2016-07-13 20:33:24
7	Raikkonen	2100000	2016-07-13 20:33:24
7	Raikkonen	2100000	2016-07-13 20:33:24
7	Raikkonen	2100000	2016-07-13 20:33:24
7	Raikkonen	2100000	2016-07-13 20:33:24
7	Raikkonen	2100000	2016-07-13 20:33:24
1	Deepak	75000	2016-06-13 11:36:27
2	Sachin	1500000	2016-06-13 11:37:51
3	Messi	2500000	2016-06-13 11:38:04
4	Federer	2000000	2016-06-13 12:18:19
5	Ronaldo	2750000	2016-06-18 02:58:31
6	Rooney	2000000	2016-07-09 10:41:36
Time taken: 0.081 seconds, Fetched: 15 row(s)
hive (dkmdb01)> dfs -ls hdfs://localhost:54310/user/hduser/sqphv01/employee1                                           
              > ;
Found 4 items
-rw-r--r--   1 hduser supergroup        360 2016-07-13 20:33 hdfs://localhost:54310/user/hduser/sqphv01/employee1/000000_0
-rw-r--r--   1 hduser supergroup          0 2016-07-09 16:04 hdfs://localhost:54310/user/hduser/sqphv01/employee1/_SUCCESS
-rw-r--r--   1 hduser supergroup        199 2016-07-09 16:04 hdfs://localhost:54310/user/hduser/sqphv01/employee1/part-m-00000
-rw-r--r--   1 hduser supergroup         40 2016-07-09 16:14 hdfs://localhost:54310/user/hduser/sqphv01/employee1/part-m-00001
hive (dkmdb01)> dfs -rm hdfs://localhost:54310/user/hduser/sqphv01/employee1/0*                                        
              > ;
Deleted hdfs://localhost:54310/user/hduser/sqphv01/employee1/000000_0
hive (dkmdb01)> select * from employee1;                                       
OK
employee1.emp_id	employee1.emp_name	employee1.emp_sal	employee1.row_ts
1	Deepak	75000	2016-06-13 11:36:27
2	Sachin	1500000	2016-06-13 11:37:51
3	Messi	2500000	2016-06-13 11:38:04
4	Federer	2000000	2016-06-13 12:18:19
5	Ronaldo	2750000	2016-06-18 02:58:31
6	Rooney	2000000	2016-07-09 10:41:36
Time taken: 0.085 seconds, Fetched: 6 row(s)

hive (dkmdb01)> select 7,'Raikkonen',2100000,from_unixtime(unix_timestamp()) from players1;
OK
_c0	_c1	_c2	_c3
7	Raikkonen	2100000	2016-07-13 20:36:37
7	Raikkonen	2100000	2016-07-13 20:36:37
7	Raikkonen	2100000	2016-07-13 20:36:37
7	Raikkonen	2100000	2016-07-13 20:36:37
7	Raikkonen	2100000	2016-07-13 20:36:37
7	Raikkonen	2100000	2016-07-13 20:36:37
7	Raikkonen	2100000	2016-07-13 20:36:37
7	Raikkonen	2100000	2016-07-13 20:36:37
7	Raikkonen	2100000	2016-07-13 20:36:37
Time taken: 0.517 seconds, Fetched: 9 row(s)
hive (dkmdb01)> select count(1) from players1;                                             
Query ID = hduser_20160713203737_ce7a4405-5bef-48d2-b017-5aea5eb4ae38
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1468419167049_0004, Tracking URL = http://Inceptez:8088/proxy/application_1468419167049_0004/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1468419167049_0004
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2016-07-13 20:37:24,315 Stage-1 map = 0%,  reduce = 0%
2016-07-13 20:37:38,587 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.32 sec
2016-07-13 20:38:11,927 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.95 sec
MapReduce Total cumulative CPU time: 2 seconds 950 msec
Ended Job = job_1468419167049_0004
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 2.95 sec   HDFS Read: 700 HDFS Write: 2 SUCCESS
Total MapReduce CPU Time Spent: 2 seconds 950 msec
OK
_c0
9
Time taken: 59.961 seconds, Fetched: 1 row(s)

hive (dkmdb01)> select 7,'Raikkonen',2100000,from_unixtime(unix_timestamp()) from players1 limit 1;
OK
_c0	_c1	_c2	_c3
7	Raikkonen	2100000	2016-07-13 20:39:18
Time taken: 0.207 seconds, Fetched: 1 row(s)
hive (dkmdb01)> insert into table employee1 select 7,'Raikkonen',2100000,from_unixtime(unix_timestamp()) from players1 limit 1;
Query ID = hduser_20160713203939_1b88f8f9-b160-4334-93b0-306d82597378
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1468419167049_0005, Tracking URL = http://Inceptez:8088/proxy/application_1468419167049_0005/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1468419167049_0005
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2016-07-13 20:39:50,507 Stage-1 map = 0%,  reduce = 0%
2016-07-13 20:40:02,396 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.73 sec
2016-07-13 20:40:21,620 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.71 sec
MapReduce Total cumulative CPU time: 3 seconds 710 msec
Ended Job = job_1468419167049_0005
Loading data to table dkmdb01.employee1
[Error 30017]: Skipping stats aggregation by error org.apache.hadoop.hive.ql.metadata.HiveException: [Error 30001]: StatsPublisher cannot be initialized. There was a error in the initialization of StatsPublisher, and retrying might help. If you dont want the query to fail because accurate statistics could not be collected, set hive.stats.reliable=false
Table dkmdb01.employee1 stats: [numFiles=3, numRows=-1, totalSize=279, rawDataSize=-1]
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 3.71 sec   HDFS Read: 700 HDFS Write: 40 SUCCESS
Total MapReduce CPU Time Spent: 3 seconds 710 msec
OK
_col0	_col1	_col2	_col3
Time taken: 47.366 seconds
hive (dkmdb01)> select * from employee1;                                                                                       
OK
employee1.emp_id	employee1.emp_name	employee1.emp_sal	employee1.row_ts
7	Raikkonen	2100000	2016-07-13 20:40:00
1	Deepak	75000	2016-06-13 11:36:27
2	Sachin	1500000	2016-06-13 11:37:51
3	Messi	2500000	2016-06-13 11:38:04
4	Federer	2000000	2016-06-13 12:18:19
5	Ronaldo	2750000	2016-06-18 02:58:31
6	Rooney	2000000	2016-07-09 10:41:36
Time taken: 0.126 seconds, Fetched: 7 row(s)
hive (dkmdb01)> dfs -ls hdfs://localhost:54310/user/hduser/sqphv01/employee1;
Found 4 items
-rw-r--r--   1 hduser supergroup         40 2016-07-13 20:40 hdfs://localhost:54310/user/hduser/sqphv01/employee1/000000_0
-rw-r--r--   1 hduser supergroup          0 2016-07-09 16:04 hdfs://localhost:54310/user/hduser/sqphv01/employee1/_SUCCESS
-rw-r--r--   1 hduser supergroup        199 2016-07-09 16:04 hdfs://localhost:54310/user/hduser/sqphv01/employee1/part-m-00000
-rw-r--r--   1 hduser supergroup         40 2016-07-09 16:14 hdfs://localhost:54310/user/hduser/sqphv01/employee1/part-m-00001


hive (dkmdb01)> select current_timestamp from employee1;
FAILED: SemanticException [Error 10004]: Line 1:7 Invalid table alias or column reference 'current_timestamp': (possible column names are: emp_id, emp_name, emp_sal, row_ts)
hive (dkmdb01)> select current_timestamp() from employee1;
FAILED: SemanticException [Error 10011]: Line 1:7 Invalid function 'current_timestamp'
hive (dkmdb01)> select from_unixtime(unix_timestamp()) from employee1;
OK
_c0
2016-07-13 20:43:13
2016-07-13 20:43:13
2016-07-13 20:43:13
2016-07-13 20:43:13
2016-07-13 20:43:13
2016-07-13 20:43:13
2016-07-13 20:43:13
Time taken: 0.102 seconds, Fetched: 7 row(s)
====================================================================================================
hive (dkmdb01)> create table emp1_like like employee1;
OK
Time taken: 0.793 seconds
hive (dkmdb01)> show create table employee1;
OK
createtab_stmt
CREATE EXTERNAL TABLE `employee1`(
  `emp_id` int, 
  `emp_name` varchar(25), 
  `emp_sal` decimal(11,2), 
  `row_ts` timestamp)
ROW FORMAT DELIMITED 
  FIELDS TERMINATED BY ',' 
STORED AS INPUTFORMAT 
  'org.apache.hadoop.mapred.TextInputFormat' 
OUTPUTFORMAT 
  'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
LOCATION
  'hdfs://localhost:54310/user/hduser/sqphv01/employee1'
TBLPROPERTIES (
  'COLUMN_STATS_ACCURATE'='true', 
  'numFiles'='0', 
  'numRows'='-1', 
  'rawDataSize'='-1', 
  'totalSize'='0', 
  'transient_lastDdlTime'='1468422624')
Time taken: 0.189 seconds, Fetched: 20 row(s)
hive (dkmdb01)> show create table emp1_like;
OK
createtab_stmt
CREATE TABLE `emp1_like`(
  `emp_id` int, 
  `emp_name` varchar(25), 
  `emp_sal` decimal(11,2), 
  `row_ts` timestamp)
ROW FORMAT DELIMITED 
  FIELDS TERMINATED BY ',' 
STORED AS INPUTFORMAT 
  'org.apache.hadoop.mapred.TextInputFormat' 
OUTPUTFORMAT 
  'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
LOCATION
  'hdfs://localhost:54310/user/hive/warehouse/dkmdb01.db/emp1_like'
TBLPROPERTIES (
  'transient_lastDdlTime'='1468423450')
Time taken: 0.186 seconds, Fetched: 15 row(s)
====================================================================================================
[hduser@Inceptez ~]$ sqoop create-hive-table --connect jdbc:mysql://localhost/DKMDB01 --username root --password root --table student --hive-table dkmdb01.student;
16/07/13 21:09:00 INFO sqoop.Sqoop: Running Sqoop version: 1.4.5
16/07/13 21:09:00 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
16/07/13 21:09:00 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override
16/07/13 21:09:00 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.
16/07/13 21:09:00 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
16/07/13 21:09:01 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `student` AS t LIMIT 1
16/07/13 21:09:01 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `student` AS t LIMIT 1
16/07/13 21:09:01 WARN hive.TableDefWriter: Column row_ts had to be cast to a less precise type in Hive
16/07/13 21:09:01 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/07/13 21:09:02 INFO hive.HiveImport: Loading uploaded data into Hive
16/07/13 21:09:07 INFO hive.HiveImport: 
16/07/13 21:09:07 INFO hive.HiveImport: Logging initialized using configuration in jar:file:/usr/local/hive/lib/hive-common-0.14.0.jar!/hive-log4j.properties
16/07/13 21:09:08 INFO hive.HiveImport: SLF4J: Class path contains multiple SLF4J bindings.
16/07/13 21:09:08 INFO hive.HiveImport: SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
16/07/13 21:09:08 INFO hive.HiveImport: SLF4J: Found binding in [jar:file:/usr/local/hive/lib/hive-jdbc-0.14.0-standalone.jar!/org/slf4j/impl/StaticLoggerBinder.class]
16/07/13 21:09:08 INFO hive.HiveImport: SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
16/07/13 21:09:08 INFO hive.HiveImport: SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
16/07/13 21:09:12 INFO hive.HiveImport: OK
16/07/13 21:09:12 INFO hive.HiveImport: Time taken: 2.08 seconds
16/07/13 21:09:12 INFO hive.HiveImport: Hive import complete.

hive (dkmdb01)> show tables;
OK
tab_name
emp1_like
employee1
players1
players2
players3
salesorders
salesorders2
salesorders3
student
testdml1
testdml2
testdml3
values__tmp__table__4
Time taken: 0.08 seconds, Fetched: 13 row(s)
hive (dkmdb01)> show create table student;
OK
createtab_stmt
CREATE TABLE `student`(
  `stu_id` int, 
  `stu_name` string, 
  `stu_dep` int, 
  `row_ts` string)
COMMENT 'Imported by sqoop on 2016/07/13 21:09:01'
ROW FORMAT DELIMITED 
  FIELDS TERMINATED BY '\u0001' 
  LINES TERMINATED BY '\n' 
STORED AS INPUTFORMAT 
  'org.apache.hadoop.mapred.TextInputFormat' 
OUTPUTFORMAT 
  'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
LOCATION
  'hdfs://localhost:54310/user/hive/warehouse/dkmdb01.db/student'
TBLPROPERTIES (
  'transient_lastDdlTime'='1468424352')
Time taken: 0.161 seconds, Fetched: 17 row(s)

hive (dkmdb01)> select * from student;    
OK
student.stu_id	student.stu_name	student.stu_dep	student.row_ts
Time taken: 0.1 seconds
====================================================================================================
[hduser@Inceptez ~]$ sqoop job --create sqp2hv_inc_imp_stud -- import --connect jdbc:mysql://localhost/DKMDB01 --direct --username root --password root --table student --m 1 --hive-import --hive-table dkmdb01.student --incremental append --check-column stu_id --last-value 0;
16/07/13 21:17:24 INFO sqoop.Sqoop: Running Sqoop version: 1.4.5
16/07/13 21:17:24 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
16/07/13 21:17:24 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override
16/07/13 21:17:24 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.

[hduser@Inceptez ~]$ sqoop job --list
Available jobs:
  exp_job1
  import_all_DKMDB01
  inc_imp_lm
  myjob1
  myjob2
  sqp2hv_inc_imp_stud

[hduser@Inceptez ~]$ sqoop job --exec sqp2hv_inc_imp_stud
Enter password: 
16/07/13 21:18:30 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
16/07/13 21:18:30 INFO tool.CodeGenTool: Beginning code generation
16/07/13 21:18:30 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `student` AS t LIMIT 1
16/07/13 21:18:30 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `student` AS t LIMIT 1
16/07/13 21:18:30 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop
Note: /tmp/sqoop-hduser/compile/eafaaf8dfc35d6610de7fe7a01fd276c/student.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
16/07/13 21:18:36 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hduser/compile/eafaaf8dfc35d6610de7fe7a01fd276c/student.jar
16/07/13 21:18:37 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/07/13 21:18:37 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(`stu_id`) FROM student
16/07/13 21:18:37 INFO tool.ImportTool: Incremental import based on column `stu_id`
16/07/13 21:18:37 INFO tool.ImportTool: Lower bound value: 0
16/07/13 21:18:37 INFO tool.ImportTool: Upper bound value: 6
16/07/13 21:18:37 INFO manager.DirectMySQLManager: Beginning mysqldump fast path import
16/07/13 21:18:37 INFO mapreduce.ImportJobBase: Beginning import of student
16/07/13 21:18:38 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
16/07/13 21:18:38 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
16/07/13 21:18:38 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
16/07/13 21:18:40 INFO db.DBInputFormat: Using read commited transaction isolation
16/07/13 21:18:40 INFO mapreduce.JobSubmitter: number of splits:1
16/07/13 21:18:40 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1468419167049_0006
16/07/13 21:18:41 INFO impl.YarnClientImpl: Submitted application application_1468419167049_0006
16/07/13 21:18:41 INFO mapreduce.Job: The url to track the job: http://Inceptez:8088/proxy/application_1468419167049_0006/
16/07/13 21:18:41 INFO mapreduce.Job: Running job: job_1468419167049_0006
16/07/13 21:18:52 INFO mapreduce.Job: Job job_1468419167049_0006 running in uber mode : false
16/07/13 21:18:52 INFO mapreduce.Job:  map 0% reduce 0%
16/07/13 21:19:04 INFO mapreduce.Job:  map 100% reduce 0%
16/07/13 21:19:06 INFO mapreduce.Job: Job job_1468419167049_0006 completed successfully
16/07/13 21:19:06 INFO mapreduce.Job: Counters: 30
..
16/07/13 21:19:06 INFO mapreduce.ImportJobBase: Transferred 185 bytes in 28.2558 seconds (6.5473 bytes/sec)
16/07/13 21:19:06 INFO mapreduce.ImportJobBase: Retrieved 6 records.
16/07/13 21:19:06 INFO util.AppendUtils: Creating missing output directory - student
16/07/13 21:19:06 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `student` AS t LIMIT 1
16/07/13 21:19:06 WARN hive.TableDefWriter: Column row_ts had to be cast to a less precise type in Hive
16/07/13 21:19:06 INFO hive.HiveImport: Loading uploaded data into Hive
16/07/13 21:19:13 INFO hive.HiveImport: 
16/07/13 21:19:13 INFO hive.HiveImport: Logging initialized using configuration in jar:file:/usr/local/hive/lib/hive-common-0.14.0.jar!/hive-log4j.properties
16/07/13 21:19:14 INFO hive.HiveImport: SLF4J: Class path contains multiple SLF4J bindings.
16/07/13 21:19:14 INFO hive.HiveImport: SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
16/07/13 21:19:14 INFO hive.HiveImport: SLF4J: Found binding in [jar:file:/usr/local/hive/lib/hive-jdbc-0.14.0-standalone.jar!/org/slf4j/impl/StaticLoggerBinder.class]
16/07/13 21:19:14 INFO hive.HiveImport: SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
16/07/13 21:19:14 INFO hive.HiveImport: SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
16/07/13 21:19:17 INFO hive.HiveImport: OK
16/07/13 21:19:17 INFO hive.HiveImport: Time taken: 1.088 seconds
16/07/13 21:19:17 INFO hive.HiveImport: Loading data to table dkmdb01.student
16/07/13 21:19:17 INFO hive.HiveImport: Table dkmdb01.student stats: [numFiles=1, totalSize=185]
16/07/13 21:19:17 INFO hive.HiveImport: OK
16/07/13 21:19:17 INFO hive.HiveImport: Time taken: 0.886 seconds
16/07/13 21:19:18 INFO hive.HiveImport: Hive import complete.
16/07/13 21:19:18 INFO hive.HiveImport: Export directory is empty, removing it.
16/07/13 21:19:18 INFO tool.ImportTool: Saving incremental import state to the metastore
16/07/13 21:19:18 INFO tool.ImportTool: Updated data for job: sqp2hv_inc_imp_stud

hive (dkmdb01)> select * from student;
OK
student.stu_id	student.stu_name	student.stu_dep	student.row_ts
1	Deepak	1	2016-06-21 14:54:56
2	Farooq	1	2016-06-21 14:55:21
3	Vadivel	2	2016-06-21 14:55:43
4	Hari	3	2016-06-21 14:56:05
5	Karthick	1	2016-06-21 14:56:21
6	Mani	4	2016-06-21 15:11:08
Time taken: 0.11 seconds, Fetched: 6 row(s)
hive (dkmdb01)> dfs -ls hdfs://localhost:54310/user/hive/warehouse/dkmdb01.db/student
              > ;
Found 1 items
-rw-r--r--   1 hduser supergroup        185 2016-07-13 21:19 hdfs://localhost:54310/user/hive/warehouse/dkmdb01.db/student/part-m-00000
====================================================================================================
mysql> insert into student values (7,'Syed',1,current_timestamp);
Query OK, 1 row affected (0.10 sec)

mysql> insert into student values (8,'Rajesh',1,current_timestamp);
Query OK, 1 row affected (0.00 sec)

mysql> insert into student values (9,'Vignesh',3,current_timestamp);
Query OK, 1 row affected (0.00 sec)

mysql> select * from student;
+--------+----------+---------+---------------------+
| stu_id | stu_name | stu_dep | row_ts              |
+--------+----------+---------+---------------------+
|      1 | Deepak   |       1 | 2016-06-21 20:24:56 |
|      2 | Farooq   |       1 | 2016-06-21 20:25:21 |
|      3 | Vadivel  |       2 | 2016-06-21 20:25:43 |
|      4 | Hari     |       3 | 2016-06-21 20:26:05 |
|      5 | Karthick |       1 | 2016-06-21 20:26:21 |
|      6 | Mani     |       4 | 2016-06-21 20:41:08 |
|      7 | Syed     |       1 | 2016-07-13 21:23:48 |
|      8 | Rajesh   |       1 | 2016-07-13 21:24:44 |
|      9 | Vignesh  |       3 | 2016-07-13 21:25:01 |
+--------+----------+---------+---------------------+
9 rows in set (0.00 sec)

[hduser@Inceptez ~]$ sqoop job --exec sqp2hv_inc_imp_stud
Enter password: 
16/07/13 21:26:28 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
16/07/13 21:26:28 INFO tool.CodeGenTool: Beginning code generation
16/07/13 21:26:28 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `student` AS t LIMIT 1
16/07/13 21:26:28 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `student` AS t LIMIT 1
16/07/13 21:26:28 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop
Note: /tmp/sqoop-hduser/compile/d314ef4f06253cca46f8f5da215dd497/student.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
16/07/13 21:26:33 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hduser/compile/d314ef4f06253cca46f8f5da215dd497/student.jar
16/07/13 21:26:33 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/07/13 21:26:34 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(`stu_id`) FROM student
16/07/13 21:26:34 INFO tool.ImportTool: Incremental import based on column `stu_id`
16/07/13 21:26:34 INFO tool.ImportTool: Lower bound value: 6
16/07/13 21:26:34 INFO tool.ImportTool: Upper bound value: 9
16/07/13 21:26:34 INFO manager.DirectMySQLManager: Beginning mysqldump fast path import
16/07/13 21:26:34 INFO mapreduce.ImportJobBase: Beginning import of student
16/07/13 21:26:34 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
16/07/13 21:26:34 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
16/07/13 21:26:35 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
16/07/13 21:26:36 INFO db.DBInputFormat: Using read commited transaction isolation
16/07/13 21:26:37 INFO mapreduce.JobSubmitter: number of splits:1
16/07/13 21:26:37 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1468419167049_0007
16/07/13 21:26:37 INFO impl.YarnClientImpl: Submitted application application_1468419167049_0007
16/07/13 21:26:37 INFO mapreduce.Job: The url to track the job: http://Inceptez:8088/proxy/application_1468419167049_0007/
16/07/13 21:26:37 INFO mapreduce.Job: Running job: job_1468419167049_0007
16/07/13 21:26:47 INFO mapreduce.Job: Job job_1468419167049_0007 running in uber mode : false
16/07/13 21:26:47 INFO mapreduce.Job:  map 0% reduce 0%
16/07/13 21:26:56 INFO mapreduce.Job:  map 100% reduce 0%
16/07/13 21:26:56 INFO mapreduce.Job: Job job_1468419167049_0007 completed successfully
16/07/13 21:26:57 INFO mapreduce.Job: Counters: 30
..
16/07/13 21:26:57 INFO mapreduce.ImportJobBase: Transferred 92 bytes in 22.2643 seconds (4.1322 bytes/sec)
16/07/13 21:26:57 INFO mapreduce.ImportJobBase: Retrieved 3 records.
16/07/13 21:26:57 INFO util.AppendUtils: Creating missing output directory - student
16/07/13 21:26:57 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `student` AS t LIMIT 1
16/07/13 21:26:57 WARN hive.TableDefWriter: Column row_ts had to be cast to a less precise type in Hive
16/07/13 21:26:57 INFO hive.HiveImport: Loading uploaded data into Hive
16/07/13 21:27:01 INFO hive.HiveImport: 
16/07/13 21:27:01 INFO hive.HiveImport: Logging initialized using configuration in jar:file:/usr/local/hive/lib/hive-common-0.14.0.jar!/hive-log4j.properties
16/07/13 21:27:01 INFO hive.HiveImport: SLF4J: Class path contains multiple SLF4J bindings.
16/07/13 21:27:01 INFO hive.HiveImport: SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
16/07/13 21:27:01 INFO hive.HiveImport: SLF4J: Found binding in [jar:file:/usr/local/hive/lib/hive-jdbc-0.14.0-standalone.jar!/org/slf4j/impl/StaticLoggerBinder.class]
16/07/13 21:27:01 INFO hive.HiveImport: SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
16/07/13 21:27:01 INFO hive.HiveImport: SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
16/07/13 21:27:04 INFO hive.HiveImport: OK
16/07/13 21:27:04 INFO hive.HiveImport: Time taken: 1.307 seconds
16/07/13 21:27:05 INFO hive.HiveImport: Loading data to table dkmdb01.student
16/07/13 21:27:05 INFO hive.HiveImport: Table dkmdb01.student stats: [numFiles=2, totalSize=277]
16/07/13 21:27:05 INFO hive.HiveImport: OK
16/07/13 21:27:05 INFO hive.HiveImport: Time taken: 1.211 seconds
16/07/13 21:27:05 INFO hive.HiveImport: Hive import complete.
16/07/13 21:27:06 INFO hive.HiveImport: Export directory is empty, removing it.
16/07/13 21:27:06 INFO tool.ImportTool: Saving incremental import state to the metastore
16/07/13 21:27:06 INFO tool.ImportTool: Updated data for job: sqp2hv_inc_imp_stud

hive (dkmdb01)> dfs -ls hdfs://localhost:54310/user/hive/warehouse/dkmdb01.db/student;
Found 2 items
-rw-r--r--   1 hduser supergroup        185 2016-07-13 21:19 hdfs://localhost:54310/user/hive/warehouse/dkmdb01.db/student/part-m-00000
-rw-r--r--   1 hduser supergroup         92 2016-07-13 21:26 hdfs://localhost:54310/user/hive/warehouse/dkmdb01.db/student/part-m-00000_copy_1
hive (dkmdb01)> select * from student;                                                
OK
student.stu_id	student.stu_name	student.stu_dep	student.row_ts
1	Deepak	1	2016-06-21 14:54:56
2	Farooq	1	2016-06-21 14:55:21
3	Vadivel	2	2016-06-21 14:55:43
4	Hari	3	2016-06-21 14:56:05
5	Karthick	1	2016-06-21 14:56:21
6	Mani	4	2016-06-21 15:11:08
7	Syed	1	2016-07-13 15:53:48
8	Rajesh	1	2016-07-13 15:54:44
9	Vignesh	3	2016-07-13 15:55:01
Time taken: 0.091 seconds, Fetched: 9 row(s)
====================================================================================================
