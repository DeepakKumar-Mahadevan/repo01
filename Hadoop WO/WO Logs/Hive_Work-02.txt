hive> create external table dkmdb01.employee1 (emp_id int, emp_name varchar(25), emp_sal decimal(11,2), row_ts timestamp)
    > row format delimited                                                                                               
    > location '/user/hduser/sqphv01/employee1';                                                                         

hive> show create table dkmdb01.employee1;
OK
CREATE EXTERNAL TABLE `dkmdb01.employee1`(
  `emp_id` int, 
  `emp_name` varchar(25), 
  `emp_sal` decimal(11,2), 
  `row_ts` timestamp)
ROW FORMAT SERDE 
  'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe' 
STORED AS INPUTFORMAT 
  'org.apache.hadoop.mapred.TextInputFormat' 
OUTPUTFORMAT 
  'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
LOCATION
  'hdfs://localhost:54310/user/hduser/sqphv01/employee1'
TBLPROPERTIES (
  'COLUMN_STATS_ACCURATE'='false', 
  'numFiles'='0', 
  'numRows'='-1', 
  'rawDataSize'='-1', 
  'totalSize'='0', 
  'transient_lastDdlTime'='1468059595')

hive> create external table dkmdb01.employee1 (emp_id int, emp_name varchar(25), emp_sal decimal(11,2), row_ts timestamp)
    > row format delimited                                                                                               
    > fields terminated by ','
    > location '/user/hduser/sqphv01/employee1';                                                                         
hive> use dkmdb01;      
hive> show create table employee1;              
CREATE EXTERNAL TABLE `employee1`(
  `emp_id` int, 
  `emp_name` varchar(25), 
  `emp_sal` decimal(11,2), 
  `row_ts` timestamp)
ROW FORMAT DELIMITED 
  FIELDS TERMINATED BY ',' 
STORED AS INPUTFORMAT 
  'org.apache.hadoop.mapred.TextInputFormat' 
OUTPUTFORMAT 
  'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
LOCATION
  'hdfs://localhost:54310/user/hduser/sqphv01/employee1'
TBLPROPERTIES (
  'COLUMN_STATS_ACCURATE'='false', 
  'numFiles'='0', 
  'numRows'='-1', 
  'rawDataSize'='-1', 
  'totalSize'='0', 
  'transient_lastDdlTime'='1468059874')
Time taken: 0.116 seconds, Fetched: 20 row(s)

[hduser@Inceptez ~]$ sqoop import --connect jdbc:mysql://localhost/DKMDB01 --direct --username root --password root --table Employee --m 1 --target-dir /user/hduser/sqphv01/employee1;
Note: /tmp/sqoop-hduser/compile/3cce09384c600d1a3806ca95cc2ca4ca/Employee.java uses or overrides a deprecated API.
16/07/09 16:03:50 INFO db.DBInputFormat: Using read commited transaction isolation
16/07/09 16:03:50 INFO mapreduce.JobSubmitter: number of splits:1
16/07/09 16:03:51 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1468057265140_0001
16/07/09 16:03:52 INFO impl.YarnClientImpl: Submitted application application_1468057265140_0001
16/07/09 16:03:52 INFO mapreduce.Job: The url to track the job: http://Inceptez:8088/proxy/application_1468057265140_0001/
16/07/09 16:03:52 INFO mapreduce.Job: Running job: job_1468057265140_0001
16/07/09 16:04:05 INFO mapreduce.Job: Job job_1468057265140_0001 running in uber mode : false
16/07/09 16:04:05 INFO mapreduce.Job:  map 0% reduce 0%
16/07/09 16:04:18 INFO mapreduce.Job:  map 100% reduce 0%
16/07/09 16:04:19 INFO mapreduce.Job: Job job_1468057265140_0001 completed successfully
16/07/09 16:03:46 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hduser/compile/3cce09384c600d1a3806ca95cc2ca4ca/Employee.jar
16/07/09 16:04:19 INFO mapreduce.ImportJobBase: Retrieved 5 records.
[hduser@Inceptez ~]$ hdfs dfs -ls /user/hduser/sqphv01/employee1
Found 2 items
-rw-r--r--   1 hduser supergroup          0 2016-07-09 16:04 /user/hduser/sqphv01/employee1/_SUCCESS
-rw-r--r--   1 hduser supergroup        199 2016-07-09 16:04 /user/hduser/sqphv01/employee1/part-m-00000

hive> set hive.cli.print.header=true;
hive> set hive.cli.print.current.db=true;
hive (dkmdb01)> select * from employee1;           
OK
employee1.emp_id	employee1.emp_name	employee1.emp_sal	employee1.row_ts
1	Deepak	75000	2016-06-13 11:36:27
2	Sachin	1500000	2016-06-13 11:37:51
3	Messi	2500000	2016-06-13 11:38:04
4	Federer	2000000	2016-06-13 12:18:19
5	Ronaldo	2750000	2016-06-18 02:58:31
Time taken: 0.113 seconds, Fetched: 5 row(s)

mysql> insert into Employee values (6,'Rooney',2000000,current_timestamp);
mysql> select * from Employee;
+--------+----------+------------+---------------------+
| Emp_id | Emp_Name | Emp_sal    | Row_ts              |
+--------+----------+------------+---------------------+
|      1 | Deepak   |   75000.00 | 2016-06-13 17:06:27 |
|      2 | Sachin   | 1500000.00 | 2016-06-13 17:07:51 |
|      3 | Messi    | 2500000.00 | 2016-06-13 17:08:04 |
|      4 | Federer  | 2000000.00 | 2016-06-13 17:48:19 |
|      5 | Ronaldo  | 2750000.00 | 2016-06-18 08:28:31 |
|      6 | Rooney   | 2000000.00 | 2016-07-09 16:11:36 |
+--------+----------+------------+---------------------+
6 rows in set (0.00 sec)

[hduser@Inceptez ~]$ sqoop import --connect jdbc:mysql://localhost/DKMDB01 --direct --username root --password root --table Employee --m 1 --target-dir /user/hduser/sqphv01/employee1 --incremental append --check-column Emp_id --last-value 5;
Note: /tmp/sqoop-hduser/compile/2b41292747ebb96638dd84e6b220b46c/Employee.java uses or overrides a deprecated API.
16/07/09 16:14:35 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hduser/compile/2b41292747ebb96638dd84e6b220b46c/Employee.jar
16/07/09 16:14:36 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(`Emp_id`) FROM Employee
16/07/09 16:14:36 INFO tool.ImportTool: Incremental import based on column `Emp_id`
16/07/09 16:14:36 INFO tool.ImportTool: Lower bound value: 5
16/07/09 16:14:36 INFO tool.ImportTool: Upper bound value: 6
16/07/09 16:14:38 INFO mapreduce.JobSubmitter: number of splits:1
16/07/09 16:14:39 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1468057265140_0002
16/07/09 16:14:39 INFO impl.YarnClientImpl: Submitted application application_1468057265140_0002
16/07/09 16:14:39 INFO mapreduce.Job: The url to track the job: http://Inceptez:8088/proxy/application_1468057265140_0002/
16/07/09 16:14:39 INFO mapreduce.Job: Running job: job_1468057265140_0002
16/07/09 16:14:50 INFO mapreduce.Job: Job job_1468057265140_0002 running in uber mode : false
16/07/09 16:14:50 INFO mapreduce.Job:  map 0% reduce 0%
16/07/09 16:15:00 INFO mapreduce.Job:  map 100% reduce 0%
16/07/09 16:15:01 INFO mapreduce.Job: Job job_1468057265140_0002 completed successfully
16/07/09 16:15:01 INFO mapreduce.ImportJobBase: Retrieved 1 records.
16/07/09 16:15:01 INFO util.AppendUtils: Appending to directory employee1
16/07/09 16:15:01 INFO util.AppendUtils: Using found partition 1
16/07/09 16:15:01 INFO tool.ImportTool: Incremental import complete! To run another incremental import of all data following this import, supply the following arguments:
16/07/09 16:15:01 INFO tool.ImportTool:  --incremental append
16/07/09 16:15:01 INFO tool.ImportTool:   --check-column Emp_id
16/07/09 16:15:01 INFO tool.ImportTool:   --last-value 6
16/07/09 16:15:01 INFO tool.ImportTool: (Consider saving this with 'sqoop job --create')

[hduser@Inceptez ~]$ hdfs dfs -ls /user/hduser/sqphv01/employee1
Found 3 items
-rw-r--r--   1 hduser supergroup          0 2016-07-09 16:04 /user/hduser/sqphv01/employee1/_SUCCESS
-rw-r--r--   1 hduser supergroup        199 2016-07-09 16:04 /user/hduser/sqphv01/employee1/part-m-00000
-rw-r--r--   1 hduser supergroup         40 2016-07-09 16:14 /user/hduser/sqphv01/employee1/part-m-00001

hive (dkmdb01)> select * from employee1;
employee1.emp_id	employee1.emp_name	employee1.emp_sal	employee1.row_ts
1	Deepak	75000	2016-06-13 11:36:27
2	Sachin	1500000	2016-06-13 11:37:51
3	Messi	2500000	2016-06-13 11:38:04
4	Federer	2000000	2016-06-13 12:18:19
5	Ronaldo	2750000	2016-06-18 02:58:31
6	Rooney	2000000	2016-07-09 10:41:36
Time taken: 0.241 seconds, Fetched: 6 row(s)
====================================================================================================
hive (dkmdb01)> create table players1 (pl_id int, pl_name varchar(30),pl_country varchar(30), pl_age int)        
              > row format delimited                                                                     
              > fields terminated by ',';                                                                

hive (dkmdb01)> show tables;
tab_name
employee1
players1

hive (dkmdb01)> show create table players1;
createtab_stmt
CREATE TABLE `players1`(
  `pl_id` int, 
  `pl_name` varchar(30), 
  `pl_country` varchar(30), 
  `pl_age` int)
ROW FORMAT DELIMITED 
  FIELDS TERMINATED BY ',' 
STORED AS INPUTFORMAT 
  'org.apache.hadoop.mapred.TextInputFormat' 
OUTPUTFORMAT 
  'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
LOCATION
  'hdfs://localhost:54310/user/hive/warehouse/dkmdb01.db/players1'
TBLPROPERTIES (
  'transient_lastDdlTime'='1468087240')

hive (dkmdb01)> insert into players1 values (10,'Sachin','India',43);
FAILED: ParseException line 1:12 missing TABLE at 'players1' near '<EOF>'

hive (dkmdb01)> insert into table players1 values (10,'Sachin','India',43);
Query ID = hduser_20160709233636_856ac873-aba6-48f8-9424-99b918597808
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1468084578304_0001, Tracking URL = http://Inceptez:8088/proxy/application_1468084578304_0001/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1468084578304_0001
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2016-07-09 23:37:08,583 Stage-1 map = 0%,  reduce = 0%
2016-07-09 23:37:20,826 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.88 sec
MapReduce Total cumulative CPU time: 1 seconds 880 msec
Ended Job = job_1468084578304_0001
Stage-4 is selected by condition resolver.
Stage-3 is filtered out by condition resolver.
Stage-5 is filtered out by condition resolver.
Moving data to: hdfs://localhost:54310/tmp/hive/hduser/670b684e-b746-45e0-8ce8-d9f12da9cecb/hive_2016-07-09_23-36-49_272_4775220109184786936-1/-ext-10000
Loading data to table dkmdb01.players1
[Error 30017]: Skipping stats aggregation by error org.apache.hadoop.hive.ql.metadata.HiveException: [Error 30001]: StatsPublisher cannot be initialized. There was a error in the initialization of StatsPublisher, and retrying might help. If you dont want the query to fail because accurate statistics could not be collected, set hive.stats.reliable=false
Table dkmdb01.players1 stats: [numFiles=1, totalSize=19]
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1   Cumulative CPU: 1.88 sec   HDFS Read: 294 HDFS Write: 19 SUCCESS
Total MapReduce CPU Time Spent: 1 seconds 880 msec
_col0	_col1	_col2	_col3
Time taken: 35.02 seconds
hive (dkmdb01)> select * from players1;
players1.pl_id	players1.pl_name	players1.pl_country	players1.pl_age
10	Sachin	India	43
Time taken: 0.277 seconds, Fetched: 1 row(s)

[hduser@Inceptez hive_data_01]$ echo "10,'Messi','Argentina',29" > players1v2.txt
[hduser@Inceptez hive_data_01]$ echo "17,'Federer','Switzerland',34" >> players1v2.txt
[hduser@Inceptez hive_data_01]$ echo "10,'Rooney','England',30" >> players1v2.txt
[hduser@Inceptez hive_data_01]$ echo "7,'Ronaldo','Portugal',31" >> players1v2.txt
[hduser@Inceptez hive_data_01]$ echo "7,'Dhoni','India',35" >> players1v2.txt
[hduser@Inceptez hive_data_01]$ echo "12,'Yuvraj','India',34" >> players1v2.txt
[hduser@Inceptez hive_data_01]$ cat players1v2.txt 
10,'Messi','Argentina',29
17,'Federer','Switzerland',34
10,'Rooney','England',30
7,'Ronaldo','Portugal',31
7,'Dhoni','India',35
12,'Yuvraj','India',34

[hduser@Inceptez hive_data_01]$ hdfs dfs -put players1v2.txt data_01/
[hduser@Inceptez hive_data_01]$ hdfs dfs -ls -R data_01
-rw-r--r--   1 hduser supergroup        151 2016-07-09 23:56 data_01/players1v2.txt

hive (dkmdb01)> load data inpath 'user/hduser/data_01/players1v2.txt' into table players1;
FAILED: SemanticException Line 1:17 Invalid path ''user/hduser/data_01/players1v2.txt'': No files matching path hdfs://localhost:54310/user/hduser/user/hduser/data_01/players1v2.txt
hive (dkmdb01)> load data inpath 'data_01/players1v2.txt' into table players1;            
Loading data to table dkmdb01.players1
Table dkmdb01.players1 stats: [numFiles=2, totalSize=170]
OK
Time taken: 0.471 seconds
hive (dkmdb01)> select * from players1;                                                   
OK
players1.pl_id	players1.pl_name	players1.pl_country	players1.pl_age
10	Sachin	India	43
10	'Messi'	'Argentina'	29
17	'Federer'	'Switzerland'	34
10	'Rooney'	'England'	30
7	'Ronaldo'	'Portugal'	31
7	'Dhoni'	'India'	35
12	'Yuvraj'	'India'	34
Time taken: 0.28 seconds, Fetched: 7 row(s)

[hduser@Inceptez hive_data_01]$ vi players1v2.txt 
[hduser@Inceptez hive_data_01]$ cat players1v2.txt 
10,Messi,Argentina,29
17,Federer,Switzerland,34
10,Rooney,England,30
7,Ronaldo,Portugal,31
7,Dhoni,India,35
12,Yuvraj,India,34

hive (dkmdb01)> load data inpath 'data_01/players1v2.txt' overwrite into table players1;
Loading data to table dkmdb01.players1
Table dkmdb01.players1 stats: [numFiles=1, numRows=0, totalSize=127, rawDataSize=0]
OK
Time taken: 0.84 seconds
hive (dkmdb01)> select * from players1;                                                 
OK
players1.pl_id	players1.pl_name	players1.pl_country	players1.pl_age
10	Messi	Argentina	29
17	Federer	Switzerland	34
10	Rooney	England	30
7	Ronaldo	Portugal	31
7	Dhoni	India	35
12	Yuvraj	India	34
Time taken: 0.161 seconds, Fetched: 6 row(s)

[hduser@Inceptez hive_data_01]$ hdfs dfs -ls /user/hive/warehouse/dkmdb01.db/*
16/07/10 00:12:18 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Found 1 items
-rw-r--r--   1 hduser supergroup        127 2016-07-10 00:03 /user/hive/warehouse/dkmdb01.db/players1/players1v2.txt
[hduser@Inceptez hive_data_01]$ hdfs dfs -cat /user/hive/warehouse/dkmdb01.db/p*
16/07/10 00:12:59 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
cat: `/user/hive/warehouse/dkmdb01.db/players1': Is a directory
cat: `/user/hive/warehouse/dkmdb01.db/players2': Is a directory
[hduser@Inceptez hive_data_01]$ hdfs dfs -cat /user/hive/warehouse/dkmdb01.db/players1/players1v2.txt
16/07/10 00:13:26 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
10,Messi,Argentina,29
17,Federer,Switzerland,34
10,Rooney,England,30
7,Ronaldo,Portugal,31
7,Dhoni,India,35
12,Yuvraj,India,34

====================================================================================================
hive (dkmdb01)> create table players2 (pl_id int, pl_name varchar(30), pl_age int) partitioned by (pl_country varchar(30)) 
              > row format delimited fields terminated by ',';
OK
Time taken: 0.212 seconds
hive (dkmdb01)> show create table players2;                                                                               
OK
createtab_stmt
CREATE TABLE `players2`(
  `pl_id` int, 
  `pl_name` varchar(30), 
  `pl_age` int)
PARTITIONED BY ( 
  `pl_country` varchar(30))
ROW FORMAT DELIMITED 
  FIELDS TERMINATED BY ',' 
STORED AS INPUTFORMAT 
  'org.apache.hadoop.mapred.TextInputFormat' 
OUTPUTFORMAT 
  'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
LOCATION
  'hdfs://localhost:54310/user/hive/warehouse/dkmdb01.db/players2'
TBLPROPERTIES (
  'transient_lastDdlTime'='1468089560')
Time taken: 0.281 seconds, Fetched: 16 row(s)

hive (dkmdb01)> insert into table players2 partition(pl_country='India') select pl_id,pl_name,pl_country,pl_age from players1; 
FAILED: SemanticException [Error 10044]: Line 1:18 Cannot insert into target table because column number/types are different ''India'': Table insclause-0 has 3 columns, but query has 4 columns.

hive (dkmdb01)> insert into table players2 partition(pl_country='India') select pl_id,pl_name,pl_age,pl_country from players1;
FAILED: SemanticException [Error 10044]: Line 1:18 Cannot insert into target table because column number/types are different ''India'': Table insclause-0 has 3 columns, but query has 4 columns.

hive (dkmdb01)> insert into table players2 partition(pl_country='India') select pl_id,pl_name,pl_age from players1;           
Query ID = hduser_20160710002929_2bf220af-1981-44f2-8d62-766de3a2781f
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1468084578304_0002, Tracking URL = http://Inceptez:8088/proxy/application_1468084578304_0002/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1468084578304_0002
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2016-07-10 00:29:45,680 Stage-1 map = 0%,  reduce = 0%
2016-07-10 00:30:07,824 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.97 sec
MapReduce Total cumulative CPU time: 2 seconds 970 msec
Ended Job = job_1468084578304_0002
Stage-4 is selected by condition resolver.
Stage-3 is filtered out by condition resolver.
Stage-5 is filtered out by condition resolver.
Moving data to: hdfs://localhost:54310/tmp/hive/hduser/670b684e-b746-45e0-8ce8-d9f12da9cecb/hive_2016-07-10_00-29-21_047_7530057870069759280-1/-ext-10000
Loading data to table dkmdb01.players2 partition (pl_country=India)
[Error 30017]: Skipping stats aggregation by error org.apache.hadoop.hive.ql.metadata.HiveException: [Error 30001]: StatsPublisher cannot be initialized. There was a error in the initialization of StatsPublisher, and retrying might help. If you dont want the query to fail because accurate statistics could not be collected, set hive.stats.reliable=false
Partition dkmdb01.players2{pl_country=India} stats: [numFiles=1, numRows=-1, totalSize=76, rawDataSize=-1]
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1   Cumulative CPU: 2.97 sec   HDFS Read: 358 HDFS Write: 76 SUCCESS
Total MapReduce CPU Time Spent: 2 seconds 970 msec
OK
pl_id	pl_name	pl_age
Time taken: 50.84 seconds
hive (dkmdb01)> select * from players2;                                                                            
OK
players2.pl_id	players2.pl_name	players2.pl_age	players2.pl_country
10	Messi	29	India
17	Federer	34	India
10	Rooney	30	India
7	Ronaldo	31	India
7	Dhoni	35	India
12	Yuvraj	34	India
Time taken: 0.62 seconds, Fetched: 6 row(s)

[hduser@Inceptez hive_data_01]$ hdfs dfs -ls /user/hive/warehouse/dkmdb01.db
16/07/10 00:31:37 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Found 2 items
drwxr-xr-x   - hduser supergroup          0 2016-07-10 00:04 /user/hive/warehouse/dkmdb01.db/players1
drwxr-xr-x   - hduser supergroup          0 2016-07-10 00:30 /user/hive/warehouse/dkmdb01.db/players2
[hduser@Inceptez hive_data_01]$ hdfs dfs -ls -R /user/hive/warehouse/dkmdb01.db
16/07/10 00:32:06 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
drwxr-xr-x   - hduser supergroup          0 2016-07-10 00:04 /user/hive/warehouse/dkmdb01.db/players1
-rw-r--r--   1 hduser supergroup        127 2016-07-10 00:03 /user/hive/warehouse/dkmdb01.db/players1/players1v2.txt
drwxr-xr-x   - hduser supergroup          0 2016-07-10 00:30 /user/hive/warehouse/dkmdb01.db/players2
drwxr-xr-x   - hduser supergroup          0 2016-07-10 00:30 /user/hive/warehouse/dkmdb01.db/players2/pl_country=India
-rw-r--r--   1 hduser supergroup         76 2016-07-10 00:30 /user/hive/warehouse/dkmdb01.db/players2/pl_country=India/000000_0

[hduser@Inceptez hive_data_01]$ hadoop fs -cat /user/hive/warehouse/dkmdb01.db/players2/pl_country=India/000000_0
10,Messi,29
17,Federer,34
10,Rooney,30
7,Ronaldo,31
7,Dhoni,35
12,Yuvraj,34

hive (dkmdb01)> load data inpath '/user/hive/warehouse/dkmdb01.db/players1/players1v2.txt' into table players2 partition (pl_country='India');
Loading data to table dkmdb01.players2 partition (pl_country=India)
Partition dkmdb01.players2{pl_country=India} stats: [numFiles=2, numRows=0, totalSize=203, rawDataSize=0]
OK
Time taken: 1.378 seconds
hive (dkmdb01)> select * from players2;                                                                                                       
OK
players2.pl_id	players2.pl_name	players2.pl_age	players2.pl_country
10	Messi	29	India
17	Federer	34	India
10	Rooney	30	India
7	Ronaldo	31	India
7	Dhoni	35	India
12	Yuvraj	34	India
10	Messi	NULL	India
17	Federer	NULL	India
10	Rooney	NULL	India
7	Ronaldo	NULL	India
7	Dhoni	NULL	India
12	Yuvraj	NULL	India
Time taken: 0.211 seconds, Fetched: 12 row(s)

16/07/10 00:48:02 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
drwxr-xr-x   - hduser supergroup          0 2016-07-10 00:47 /user/hive/warehouse/dkmdb01.db/players1
-rw-r--r--   1 hduser supergroup        127 2016-07-10 00:47 /user/hive/warehouse/dkmdb01.db/players1/players1v2.txt
drwxr-xr-x   - hduser supergroup          0 2016-07-10 00:44 /user/hive/warehouse/dkmdb01.db/players2
drwxr-xr-x   - hduser supergroup          0 2016-07-10 00:44 /user/hive/warehouse/dkmdb01.db/players2/pl_country=India
-rw-r--r--   1 hduser supergroup          0 2016-07-10 00:44 /user/hive/warehouse/dkmdb01.db/players2/pl_country=India/000000_0
[hduser@Inceptez hive_data_01]$ hadoop fs -ls -R /user/hive/warehouse/dkmdb01.db

hive (dkmdb01)> insert overwrite table players2 partition(pl_country='India') select pl_id,pl_name,pl_age from players1 where pl_country = 'India';
Query ID = hduser_20160710004848_1557014d-de52-4ea1-b0df-61f4b33a20f7
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1468084578304_0004, Tracking URL = http://Inceptez:8088/proxy/application_1468084578304_0004/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1468084578304_0004
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2016-07-10 00:48:35,631 Stage-1 map = 0%,  reduce = 0%
2016-07-10 00:48:58,214 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 4.56 sec
MapReduce Total cumulative CPU time: 4 seconds 560 msec
Ended Job = job_1468084578304_0004
Stage-4 is selected by condition resolver.
Stage-3 is filtered out by condition resolver.
Stage-5 is filtered out by condition resolver.
Moving data to: hdfs://localhost:54310/tmp/hive/hduser/670b684e-b746-45e0-8ce8-d9f12da9cecb/hive_2016-07-10_00-48-11_292_1295302903156593783-1/-ext-10000
Loading data to table dkmdb01.players2 partition (pl_country=India)
[Error 30017]: Skipping stats aggregation by error org.apache.hadoop.hive.ql.metadata.HiveException: [Error 30001]: StatsPublisher cannot be initialized. There was a error in the initialization of StatsPublisher, and retrying might help. If you dont want the query to fail because accurate statistics could not be collected, set hive.stats.reliable=false
Partition dkmdb01.players2{pl_country=India} stats: [numFiles=1, numRows=0, totalSize=24, rawDataSize=0]
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1   Cumulative CPU: 4.56 sec   HDFS Read: 358 HDFS Write: 24 SUCCESS
Total MapReduce CPU Time Spent: 4 seconds 560 msec
OK
pl_id	pl_name	pl_age
Time taken: 50.206 seconds
hive (dkmdb01)> select * from players2;                                                                                                            
OK
players2.pl_id	players2.pl_name	players2.pl_age	players2.pl_country
7	Dhoni	35	India
12	Yuvraj	34	India
Time taken: 0.153 seconds, Fetched: 2 row(s)

16/07/10 00:49:57 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
drwxr-xr-x   - hduser supergroup          0 2016-07-10 00:47 /user/hive/warehouse/dkmdb01.db/players1
-rw-r--r--   1 hduser supergroup        127 2016-07-10 00:47 /user/hive/warehouse/dkmdb01.db/players1/players1v2.txt
drwxr-xr-x   - hduser supergroup          0 2016-07-10 00:49 /user/hive/warehouse/dkmdb01.db/players2
drwxr-xr-x   - hduser supergroup          0 2016-07-10 00:48 /user/hive/warehouse/dkmdb01.db/players2/pl_country=India
-rw-r--r--   1 hduser supergroup         24 2016-07-10 00:48 /user/hive/warehouse/dkmdb01.db/players2/pl_country=India/000000_0
====================================================================================================
hive (dkmdb01)> insert into table players1 values (10,'Sachin','India',42);           
Query ID = hduser_20160710005757_38773268-7d10-4764-90a3-e173cf7cb91f
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1468084578304_0006, Tracking URL = http://Inceptez:8088/proxy/application_1468084578304_0006/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1468084578304_0006
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2016-07-10 00:57:31,478 Stage-1 map = 0%,  reduce = 0%
2016-07-10 00:57:53,132 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.99 sec
MapReduce Total cumulative CPU time: 3 seconds 990 msec
Ended Job = job_1468084578304_0006
Stage-4 is selected by condition resolver.
Stage-3 is filtered out by condition resolver.
Stage-5 is filtered out by condition resolver.
Moving data to: hdfs://localhost:54310/tmp/hive/hduser/670b684e-b746-45e0-8ce8-d9f12da9cecb/hive_2016-07-10_00-57-09_973_8690214922335226132-1/-ext-10000
Loading data to table dkmdb01.players1
[Error 30017]: Skipping stats aggregation by error org.apache.hadoop.hive.ql.metadata.HiveException: [Error 30001]: StatsPublisher cannot be initialized. There was a error in the initialization of StatsPublisher, and retrying might help. If you dont want the query to fail because accurate statistics could not be collected, set hive.stats.reliable=false
Table dkmdb01.players1 stats: [numFiles=3, numRows=0, totalSize=168, rawDataSize=0]
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1   Cumulative CPU: 3.99 sec   HDFS Read: 294 HDFS Write: 19 SUCCESS
Total MapReduce CPU Time Spent: 3 seconds 990 msec
OK
_col0	_col1	_col2	_col3
Time taken: 44.949 seconds
hive (dkmdb01)> select * from players1;
OK
players1.pl_id	players1.pl_name	players1.pl_country	players1.pl_age
23	Beckham	England	42
10	Sachin	India	42
10	Messi	Argentina	29
17	Federer	Switzerland	34
10	Rooney	England	30
7	Ronaldo	Portugal	31
7	Dhoni	India	35
12	Yuvraj	India	34
Time taken: 0.136 seconds, Fetched: 8 row(s)

hive (dkmdb01)> delete from players1;
FAILED: SemanticException [Error 10297]: Attempt to do update or delete on table dkmdb01.players1 that does not use an AcidOutputFormat or is not bucketed

hive (dkmdb01)> create table players3 (pl_id int, pl_name varchar(30), pl_age int) partitioned by (pl_country varchar(30))
              > row format delimited fields terminated by ',';

hive (dkmdb01)> show create table players3;
OK
createtab_stmt
CREATE TABLE `players3`(
  `pl_id` int, 
  `pl_name` varchar(30), 
  `pl_age` int)
PARTITIONED BY ( 
  `pl_country` varchar(30))
ROW FORMAT DELIMITED 
  FIELDS TERMINATED BY ',' 
STORED AS INPUTFORMAT 
  'org.apache.hadoop.mapred.TextInputFormat' 
OUTPUTFORMAT 
  'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
LOCATION
  'hdfs://localhost:54310/user/hive/warehouse/dkmdb01.db/players3'
TBLPROPERTIES (
  'transient_lastDdlTime'='1468092616')
Time taken: 0.801 seconds, Fetched: 16 row(s)

[hduser@Inceptez hive_data_01]$ hadoop fs -ls -R /user/hive/warehouse/dkmdb01.db
drwxr-xr-x   - hduser supergroup          0 2016-07-10 00:57 /user/hive/warehouse/dkmdb01.db/players1
-rw-r--r--   1 hduser supergroup         22 2016-07-10 00:56 /user/hive/warehouse/dkmdb01.db/players1/000000_0
-rw-r--r--   1 hduser supergroup         19 2016-07-10 00:57 /user/hive/warehouse/dkmdb01.db/players1/000000_0_copy_1
-rw-r--r--   1 hduser supergroup        127 2016-07-10 00:47 /user/hive/warehouse/dkmdb01.db/players1/players1v2.txt
drwxr-xr-x   - hduser supergroup          0 2016-07-10 00:49 /user/hive/warehouse/dkmdb01.db/players2
drwxr-xr-x   - hduser supergroup          0 2016-07-10 00:48 /user/hive/warehouse/dkmdb01.db/players2/pl_country=India
-rw-r--r--   1 hduser supergroup         24 2016-07-10 00:48 /user/hive/warehouse/dkmdb01.db/players2/pl_country=India/000000_0
drwxr-xr-x   - hduser supergroup          0 2016-07-10 01:00 /user/hive/warehouse/dkmdb01.db/players3

hive (dkmdb01)> insert into table players3 partition(pl_country) select pl_id,pl_name,pl_age,pl_country from players1;
Query ID = hduser_20160710010505_ffa6d240-a3d2-423d-b1b9-b3ee526ebde9
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1468084578304_0007, Tracking URL = http://Inceptez:8088/proxy/application_1468084578304_0007/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1468084578304_0007
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2016-07-10 01:06:05,520 Stage-1 map = 0%,  reduce = 0%
2016-07-10 01:06:26,279 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.75 sec
MapReduce Total cumulative CPU time: 1 seconds 750 msec
Ended Job = job_1468084578304_0007
Stage-4 is selected by condition resolver.
Stage-3 is filtered out by condition resolver.
Stage-5 is filtered out by condition resolver.
Moving data to: hdfs://localhost:54310/tmp/hive/hduser/e039c0f0-7643-4717-949e-344e4aeff86f/hive_2016-07-10_01-05-53_095_6179721050356212473-1/-ext-10000
Loading data to table dkmdb01.players3 partition (pl_country=null)
	 Time taken for load dynamic partitions : 1010
	Loading partition {pl_country=India}
	Loading partition {pl_country=Portugal}
	Loading partition {pl_country=Argentina}
	Loading partition {pl_country=England}
	Loading partition {pl_country=Switzerland}
	 Time taken for adding to write entity : 16
[Error 30017]: Skipping stats aggregation by error org.apache.hadoop.hive.ql.metadata.HiveException: [Error 30001]: StatsPublisher cannot be initialized. There was a error in the initialization of StatsPublisher, and retrying might help. If you dont want the query to fail because accurate statistics could not be collected, set hive.stats.reliable=false
Partition dkmdb01.players3{pl_country=Argentina} stats: [numFiles=1, numRows=-1, totalSize=12, rawDataSize=-1]
Partition dkmdb01.players3{pl_country=England} stats: [numFiles=1, numRows=-1, totalSize=27, rawDataSize=-1]
Partition dkmdb01.players3{pl_country=India} stats: [numFiles=1, numRows=-1, totalSize=37, rawDataSize=-1]
Partition dkmdb01.players3{pl_country=Portugal} stats: [numFiles=1, numRows=-1, totalSize=13, rawDataSize=-1]
Partition dkmdb01.players3{pl_country=Switzerland} stats: [numFiles=1, numRows=-1, totalSize=14, rawDataSize=-1]
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1   Cumulative CPU: 1.75 sec   HDFS Read: 582 HDFS Write: 103 SUCCESS
Total MapReduce CPU Time Spent: 1 seconds 750 msec
OK
pl_id	pl_name	pl_age	pl_country
Time taken: 37.946 seconds

hive (dkmdb01)> select * from players3;
OK
players3.pl_id	players3.pl_name	players3.pl_age	players3.pl_country
10	Messi	29	Argentina
23	Beckham	42	England
10	Rooney	30	England
10	Sachin	42	India
7	Dhoni	35	India
12	Yuvraj	34	India
7	Ronaldo	31	Portugal
17	Federer	34	Switzerland
Time taken: 0.438 seconds, Fetched: 8 row(s)

[hduser@Inceptez hive_data_01]$ hadoop fs -ls -R /user/hive/warehouse/dkmdb01.db
16/07/10 01:07:36 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
drwxr-xr-x   - hduser supergroup          0 2016-07-10 00:57 /user/hive/warehouse/dkmdb01.db/players1
-rw-r--r--   1 hduser supergroup         22 2016-07-10 00:56 /user/hive/warehouse/dkmdb01.db/players1/000000_0
-rw-r--r--   1 hduser supergroup         19 2016-07-10 00:57 /user/hive/warehouse/dkmdb01.db/players1/000000_0_copy_1
-rw-r--r--   1 hduser supergroup        127 2016-07-10 00:47 /user/hive/warehouse/dkmdb01.db/players1/players1v2.txt
drwxr-xr-x   - hduser supergroup          0 2016-07-10 00:49 /user/hive/warehouse/dkmdb01.db/players2
drwxr-xr-x   - hduser supergroup          0 2016-07-10 00:48 /user/hive/warehouse/dkmdb01.db/players2/pl_country=India
-rw-r--r--   1 hduser supergroup         24 2016-07-10 00:48 /user/hive/warehouse/dkmdb01.db/players2/pl_country=India/000000_0
drwxr-xr-x   - hduser supergroup          0 2016-07-10 01:06 /user/hive/warehouse/dkmdb01.db/players3
drwxr-xr-x   - hduser supergroup          0 2016-07-10 01:06 /user/hive/warehouse/dkmdb01.db/players3/pl_country=Argentina
-rw-r--r--   1 hduser supergroup         12 2016-07-10 01:06 /user/hive/warehouse/dkmdb01.db/players3/pl_country=Argentina/000000_0
drwxr-xr-x   - hduser supergroup          0 2016-07-10 01:06 /user/hive/warehouse/dkmdb01.db/players3/pl_country=England
-rw-r--r--   1 hduser supergroup         27 2016-07-10 01:06 /user/hive/warehouse/dkmdb01.db/players3/pl_country=England/000000_0
drwxr-xr-x   - hduser supergroup          0 2016-07-10 01:06 /user/hive/warehouse/dkmdb01.db/players3/pl_country=India
-rw-r--r--   1 hduser supergroup         37 2016-07-10 01:06 /user/hive/warehouse/dkmdb01.db/players3/pl_country=India/000000_0
drwxr-xr-x   - hduser supergroup          0 2016-07-10 01:06 /user/hive/warehouse/dkmdb01.db/players3/pl_country=Portugal
-rw-r--r--   1 hduser supergroup         13 2016-07-10 01:06 /user/hive/warehouse/dkmdb01.db/players3/pl_country=Portugal/000000_0
drwxr-xr-x   - hduser supergroup          0 2016-07-10 01:06 /user/hive/warehouse/dkmdb01.db/players3/pl_country=Switzerland
-rw-r--r--   1 hduser supergroup         14 2016-07-10 01:06 /user/hive/warehouse/dkmdb01.db/players3/pl_country=Switzerland/000000_0
====================================================================================================
hive (dkmdb01)> create table salesorders (ord_dt date, region varchar(8), rep varchar(15), item varchar(15), unit int, unit_cost decimal (9,2),
              > total decimal(10,2))                                                                                                           
              > clustered by (item) into 5 buckets                                                                                             
              > row format delimited
              > fields terminated by ','
              > stored as textfile location '/user/hduser/hiveexternaldata/salesorders';
OK
Time taken: 8.217 seconds
hive (dkmdb01)> show create table salesorders;
OK
createtab_stmt
CREATE TABLE `salesorders`(
  `ord_dt` date, 
  `region` varchar(8), 
  `rep` varchar(15), 
  `item` varchar(15), 
  `unit` int, 
  `unit_cost` decimal(9,2), 
  `total` decimal(10,2))
CLUSTERED BY ( 
  item) 
INTO 5 BUCKETS
ROW FORMAT DELIMITED 
  FIELDS TERMINATED BY ',' 
STORED AS INPUTFORMAT 
  'org.apache.hadoop.mapred.TextInputFormat' 
OUTPUTFORMAT 
  'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
LOCATION
  'hdfs://localhost:54310/user/hduser/hiveexternaldata/salesorders'
TBLPROPERTIES (
  'transient_lastDdlTime'='1468175874')
Time taken: 1.14 seconds, Fetched: 21 row(s)

hive (dkmdb01)> select count(1) from salesorders;
Query ID = hduser_20160711000909_8f422b0d-94f1-46cc-97df-a4503795496a
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1468173964010_0001, Tracking URL = http://Inceptez:8088/proxy/application_1468173964010_0001/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1468173964010_0001
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2016-07-11 00:10:06,440 Stage-1 map = 0%,  reduce = 0%
2016-07-11 00:10:46,431 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.97 sec
2016-07-11 00:11:21,216 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 6.93 sec
MapReduce Total cumulative CPU time: 6 seconds 930 msec
Ended Job = job_1468173964010_0001
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 6.93 sec   HDFS Read: 303 HDFS Write: 2 SUCCESS
Total MapReduce CPU Time Spent: 6 seconds 930 msec
OK
_c0
0
Time taken: 132.817 seconds, Fetched: 1 row(s)
hive (dkmdb01)> select * from salesorders;       
OK
salesorders.ord_dt	salesorders.region	salesorders.rep	salesorders.item	salesorders.unit	salesorders.unit_cost	salesorders.total
Time taken: 0.562 seconds

[hduser@Inceptez hive_data_01]$ hadoop fs -ls -R hiveexternaldata
drwxr-xr-x   - hduser supergroup          0 2016-07-02 23:31 hiveexternaldata/players
-rw-r--r--   1 hduser supergroup        149 2016-07-02 23:27 hiveexternaldata/players/players.txt
-rw-r--r--   1 hduser supergroup         72 2016-07-02 23:27 hiveexternaldata/players/players1.txt
-rw-r--r--   1 hduser supergroup         55 2016-07-02 23:31 hiveexternaldata/players/players2.txt
drwxr-xr-x   - hduser supergroup          0 2016-07-11 00:07 hiveexternaldata/salesorders

[hduser@Inceptez hive_data_01]$ hdfs dfs -put salesorders.data hiveexternaldata/salesorders/
[hduser@Inceptez hive_data_01]$ hdfs dfs -ls -R hiveexternaldata/sales*
-rw-r--r--   1 hduser supergroup       2165 2016-07-11 00:16 hiveexternaldata/salesorders/salesorders.data

hive (dkmdb01)> select * from salesorders;        
OK
salesorders.ord_dt	salesorders.region	salesorders.rep	salesorders.item	salesorders.unit	salesorders.unit_cost	salesorders.total
2015-01-06	East	Jones	Pencil	95	NULL	NULL
2015-01-23	Central	Kivell	Binder	50	NULL	NULL
2015-02-09	Central	Jardine	Pencil	36	NULL	NULL
2015-02-26	Central	Gill	Pen	27	NULL	NULL
2015-03-15	West	Sorvino	Pencil	56	NULL	NULL
2015-04-01	East	Jones	Binder	60	NULL	NULL
2015-04-18	Central	Andrews	Pencil	75	NULL	NULL
2015-05-05	Central	Jardine	Pencil	90	NULL	NULL
2015-05-22	West	Thompson	Pencil	32	NULL	NULL
2015-06-08	East	Jones	Binder	60	NULL	NULL
2015-06-25	Central	Morgan	Pencil	90	NULL	NULL
2015-07-12	East	Howard	Binder	29	NULL	NULL
2015-07-29	East	Parent	Binder	81	NULL	1
2015-08-15	East	Jones	Pencil	35	NULL	NULL
2015-09-01	Central	Smith	Desk	2	NULL	NULL
2015-09-18	East	Jones	Pen Set	16	NULL	NULL
2015-10-05	Central	Morgan	Binder	28	NULL	NULL
2015-10-22	East	Jones	Pen	64	NULL	NULL
2015-11-08	East	Parent	Pen	15	NULL	NULL
2015-11-25	Central	Kivell	Pen Set	96	NULL	NULL
2015-12-12	Central	Smith	Pencil	67	NULL	NULL
2015-12-29	East	Parent	Pen Set	74	NULL	1
2016-01-15	Central	Gill	Binder	46	NULL	NULL
2016-02-01	Central	Smith	Binder	87	NULL	1
2016-02-18	East	Jones	Binder	4	NULL	NULL
2016-03-07	West	Sorvino	Binder	7	NULL	NULL
2016-03-24	Central	Jardine	Pen Set	50	NULL	NULL
2016-04-10	Central	Andrews	Pencil	66	NULL	NULL
2016-04-27	East	Howard	Pen	96	NULL	NULL
2016-05-14	Central	Gill	Pencil	53	NULL	NULL
2016-05-31	Central	Gill	Binder	80	NULL	NULL
2016-06-17	Central	Kivell	Desk	5	NULL	NULL
2016-07-04	East	Jones	Pen Set	62	NULL	NULL
2016-07-21	Central	Morgan	Pen Set	55	NULL	NULL
2016-08-07	Central	Kivell	Pen Set	42	NULL	1
2016-08-24	West	Sorvino	Desk	3	NULL	NULL
2016-09-10	Central	Gill	Pencil	7	NULL	NULL
2016-09-27	West	Sorvino	Pen	76	NULL	NULL
2016-10-14	West	Thompson	Binder	57	NULL	1
2016-10-31	Central	Andrews	Pencil	14	NULL	NULL
2016-11-17	Central	Jardine	Binder	11	NULL	NULL
2016-12-04	Central	Jardine	Binder	94	NULL	1
2016-12-21	Central	Andrews	Binder	28	NULL	NULL
Time taken: 0.214 seconds, Fetched: 43 row(s)

hive (dkmdb01)> select * from salesorders tablesample(bucket 1 out of 5); 
OK
salesorders.ord_dt	salesorders.region	salesorders.rep	salesorders.item	salesorders.unit	salesorders.unit_cost	salesorders.total
Time taken: 0.682 seconds
hive (dkmdb01)> select * from salesorders tablesample(bucket 2 out of 5);
OK
salesorders.ord_dt	salesorders.region	salesorders.rep	salesorders.item	salesorders.unit	salesorders.unit_cost	salesorders.total
2015-01-06	East	Jones	Pencil	95	NULL	NULL
2015-02-09	Central	Jardine	Pencil	36	NULL	NULL
2015-03-15	West	Sorvino	Pencil	56	NULL	NULL
2015-04-18	Central	Andrews	Pencil	75	NULL	NULL
2015-05-05	Central	Jardine	Pencil	90	NULL	NULL
2015-05-22	West	Thompson	Pencil	32	NULL	NULL
2015-06-25	Central	Morgan	Pencil	90	NULL	NULL
2015-08-15	East	Jones	Pencil	35	NULL	NULL
2015-12-12	Central	Smith	Pencil	67	NULL	NULL
2016-04-10	Central	Andrews	Pencil	66	NULL	NULL
2016-05-14	Central	Gill	Pencil	53	NULL	NULL
2016-09-10	Central	Gill	Pencil	7	NULL	NULL
2016-10-31	Central	Andrews	Pencil	14	NULL	NULL
Time taken: 0.363 seconds, Fetched: 13 row(s)
hive (dkmdb01)> select * from salesorders tablesample(bucket 3 out of 5);
OK
salesorders.ord_dt	salesorders.region	salesorders.rep	salesorders.item	salesorders.unit	salesorders.unit_cost	salesorders.total
2015-02-26	Central	Gill	Pen	27	NULL	NULL
2015-09-01	Central	Smith	Desk	2	NULL	NULL
2015-10-22	East	Jones	Pen	64	NULL	NULL
2015-11-08	East	Parent	Pen	15	NULL	NULL
2016-04-27	East	Howard	Pen	96	NULL	NULL
2016-06-17	Central	Kivell	Desk	5	NULL	NULL
2016-08-24	West	Sorvino	Desk	3	NULL	NULL
2016-09-27	West	Sorvino	Pen	76	NULL	NULL
Time taken: 0.182 seconds, Fetched: 8 row(s)
hive (dkmdb01)> select * from salesorders tablesample(bucket 4 out of 5);
OK
salesorders.ord_dt	salesorders.region	salesorders.rep	salesorders.item	salesorders.unit	salesorders.unit_cost	salesorders.total
2015-09-18	East	Jones	Pen Set	16	NULL	NULL
2015-11-25	Central	Kivell	Pen Set	96	NULL	NULL
2015-12-29	East	Parent	Pen Set	74	NULL	1
2016-03-24	Central	Jardine	Pen Set	50	NULL	NULL
2016-07-04	East	Jones	Pen Set	62	NULL	NULL
2016-07-21	Central	Morgan	Pen Set	55	NULL	NULL
2016-08-07	Central	Kivell	Pen Set	42	NULL	1
Time taken: 0.2 seconds, Fetched: 7 row(s)
hive (dkmdb01)> select * from salesorders tablesample(bucket 5 out of 5);
OK
salesorders.ord_dt	salesorders.region	salesorders.rep	salesorders.item	salesorders.unit	salesorders.unit_cost	salesorders.total
2015-01-23	Central	Kivell	Binder	50	NULL	NULL
2015-04-01	East	Jones	Binder	60	NULL	NULL
2015-06-08	East	Jones	Binder	60	NULL	NULL
2015-07-12	East	Howard	Binder	29	NULL	NULL
2015-07-29	East	Parent	Binder	81	NULL	1
2015-10-05	Central	Morgan	Binder	28	NULL	NULL
2016-01-15	Central	Gill	Binder	46	NULL	NULL
2016-02-01	Central	Smith	Binder	87	NULL	1
2016-02-18	East	Jones	Binder	4	NULL	NULL
2016-03-07	West	Sorvino	Binder	7	NULL	NULL
2016-05-31	Central	Gill	Binder	80	NULL	NULL
2016-10-14	West	Thompson	Binder	57	NULL	1
2016-11-17	Central	Jardine	Binder	11	NULL	NULL
2016-12-04	Central	Jardine	Binder	94	NULL	1
2016-12-21	Central	Andrews	Binder	28	NULL	NULL
Time taken: 0.17 seconds, Fetched: 15 row(s)

====================================================================================================
hive (dkmdb01)> create table salesorders2 (ord_dt date, region varchar(8), rep varchar(15), item varchar(15), unit int, unit_cost decimal (9,2),
              > total decimal(10,2))                                                                                                            
              > clustered by (item) into 3 buckets
              > row format delimited
              > fields terminated by ','
              > stored as textfile location '/user/hduser/hiveexternaldata/salesorders2';
OK
Time taken: 0.174 seconds

hive (dkmdb01)> insert into table salesorders2 select ord_dt,region,rep,item,unit,unit_cost,total from salesorders;                             
Query ID = hduser_20160711211818_d07d10f3-a9f6-4741-ac0b-0756ed098df3
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1468250619678_0003, Tracking URL = http://Inceptez:8088/proxy/application_1468250619678_0003/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1468250619678_0003
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2016-07-11 21:18:56,587 Stage-1 map = 0%,  reduce = 0%
2016-07-11 21:19:16,621 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.77 sec
MapReduce Total cumulative CPU time: 2 seconds 770 msec
Ended Job = job_1468250619678_0003
Loading data to table dkmdb01.salesorders2
[Error 30017]: Skipping stats aggregation by error org.apache.hadoop.hive.ql.metadata.HiveException: [Error 30001]: StatsPublisher cannot be initialized. There was a error in the initialization of StatsPublisher, and retrying might help. If you dont want the query to fail because accurate statistics could not be collected, set hive.stats.reliable=false
Table dkmdb01.salesorders2 stats: [numFiles=1, totalSize=1719]
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1   Cumulative CPU: 2.77 sec   HDFS Read: 2398 HDFS Write: 1719 SUCCESS
Total MapReduce CPU Time Spent: 2 seconds 770 msec
OK
ord_dt	region	rep	item	unit	unit_cost	total
Time taken: 44.907 seconds

hive (dkmdb01)> select * from salesorders2 tablesample(bucket 1 out of 5);
OK
salesorders2.ord_dt	salesorders2.region	salesorders2.rep	salesorders2.item	salesorders2.unit	salesorders2.unit_cost	salesorders2.total
Time taken: 0.322 seconds
hive (dkmdb01)> select * from salesorders2 tablesample(bucket 2 out of 5);
OK
salesorders2.ord_dt	salesorders2.region	salesorders2.rep	salesorders2.item	salesorders2.unit	salesorders2.unit_cost	salesorders2.total
2015-01-06	East	Jones	Pencil	95	NULL	NULL
2015-02-09	Central	Jardine	Pencil	36	NULL	NULL
2015-03-15	West	Sorvino	Pencil	56	NULL	NULL
2015-04-18	Central	Andrews	Pencil	75	NULL	NULL
2015-05-05	Central	Jardine	Pencil	90	NULL	NULL
2015-05-22	West	Thompson	Pencil	32	NULL	NULL
2015-06-25	Central	Morgan	Pencil	90	NULL	NULL
2015-08-15	East	Jones	Pencil	35	NULL	NULL
2015-12-12	Central	Smith	Pencil	67	NULL	NULL
2016-04-10	Central	Andrews	Pencil	66	NULL	NULL
2016-05-14	Central	Gill	Pencil	53	NULL	NULL
2016-09-10	Central	Gill	Pencil	7	NULL	NULL
2016-10-31	Central	Andrews	Pencil	14	NULL	NULL
Time taken: 0.169 seconds, Fetched: 13 row(s)
hive (dkmdb01)> select * from salesorders2 tablesample(bucket 3 out of 5);
OK
salesorders2.ord_dt	salesorders2.region	salesorders2.rep	salesorders2.item	salesorders2.unit	salesorders2.unit_cost	salesorders2.total
2015-02-26	Central	Gill	Pen	27	NULL	NULL
2015-09-01	Central	Smith	Desk	2	NULL	NULL
2015-10-22	East	Jones	Pen	64	NULL	NULL
2015-11-08	East	Parent	Pen	15	NULL	NULL
2016-04-27	East	Howard	Pen	96	NULL	NULL
2016-06-17	Central	Kivell	Desk	5	NULL	NULL
2016-08-24	West	Sorvino	Desk	3	NULL	NULL
2016-09-27	West	Sorvino	Pen	76	NULL	NULL
Time taken: 0.145 seconds, Fetched: 8 row(s)
hive (dkmdb01)> select * from salesorders2 tablesample(bucket 4 out of 5);
OK
salesorders2.ord_dt	salesorders2.region	salesorders2.rep	salesorders2.item	salesorders2.unit	salesorders2.unit_cost	salesorders2.total
2015-09-18	East	Jones	Pen Set	16	NULL	NULL
2015-11-25	Central	Kivell	Pen Set	96	NULL	NULL
2015-12-29	East	Parent	Pen Set	74	NULL	1
2016-03-24	Central	Jardine	Pen Set	50	NULL	NULL
2016-07-04	East	Jones	Pen Set	62	NULL	NULL
2016-07-21	Central	Morgan	Pen Set	55	NULL	NULL
2016-08-07	Central	Kivell	Pen Set	42	NULL	1
Time taken: 0.156 seconds, Fetched: 7 row(s)
hive (dkmdb01)> select * from salesorders2 tablesample(bucket 5 out of 5);
OK
salesorders2.ord_dt	salesorders2.region	salesorders2.rep	salesorders2.item	salesorders2.unit	salesorders2.unit_cost	salesorders2.total
2015-01-23	Central	Kivell	Binder	50	NULL	NULL
2015-04-01	East	Jones	Binder	60	NULL	NULL
2015-06-08	East	Jones	Binder	60	NULL	NULL
2015-07-12	East	Howard	Binder	29	NULL	NULL
2015-07-29	East	Parent	Binder	81	NULL	1
2015-10-05	Central	Morgan	Binder	28	NULL	NULL
2016-01-15	Central	Gill	Binder	46	NULL	NULL
2016-02-01	Central	Smith	Binder	87	NULL	1
2016-02-18	East	Jones	Binder	4	NULL	NULL
2016-03-07	West	Sorvino	Binder	7	NULL	NULL
2016-05-31	Central	Gill	Binder	80	NULL	NULL
2016-10-14	West	Thompson	Binder	57	NULL	1
2016-11-17	Central	Jardine	Binder	11	NULL	NULL
2016-12-04	Central	Jardine	Binder	94	NULL	1
2016-12-21	Central	Andrews	Binder	28	NULL	NULL
Time taken: 0.155 seconds, Fetched: 15 row(s)

====================================================================================================
hive (dkmdb01)> create table salesorders3 (ord_dt date, region varchar(8), rep varchar(15), item varchar(15), unit int, unit_cost decimal (9,2),
              > total decimal(10,2))
              > clustered by (item) into 3 buckets
              > row format delimited
              > fields terminated by ','
              > stored as textfile location '/user/hduser/hiveexternaldata/salesorders3';
OK
Time taken: 0.286 seconds
hive (dkmdb01)> set map.reduce.tasks=3;                                                                                                         
hive (dkmdb01)> load data local inpath '/home/hduser/hive_data_01/salesorders.data' into table salesorders3;                                    
Loading data to table dkmdb01.salesorders3
Table dkmdb01.salesorders3 stats: [numFiles=1, totalSize=2165]
OK
Time taken: 0.994 seconds

[hduser@Inceptez hive_data_01]$ hdfs dfs -ls /user/hduser/hiveexternaldata/salesorders3
Found 1 items
-rw-r--r--   1 hduser supergroup       2165 2016-07-11 21:41 /user/hduser/hiveexternaldata/salesorders3/salesorders.data
[hduser@Inceptez hive_data_01]$ hdfs dfs -ls -R /user/hduser/hiveexternaldata/salesorders3
-rw-r--r--   1 hduser supergroup       2165 2016-07-11 21:41 /user/hduser/hiveexternaldata/salesorders3/salesorders.data

hive (dkmdb01)> select * from salesorders3 tablesample(bucket 1 out of 3);
OK
salesorders3.ord_dt	salesorders3.region	salesorders3.rep	salesorders3.item	salesorders3.unit	salesorders3.unit_cost	salesorders3.total
Time taken: 0.158 seconds
hive (dkmdb01)> select * from salesorders3 tablesample(bucket 2 out of 3);
OK
salesorders3.ord_dt	salesorders3.region	salesorders3.rep	salesorders3.item	salesorders3.unit	salesorders3.unit_cost	salesorders3.total
2015-01-23	Central	Kivell	Binder	50	NULL	NULL
2015-02-26	Central	Gill	Pen	27	NULL	NULL
2015-04-01	East	Jones	Binder	60	NULL	NULL
2015-06-08	East	Jones	Binder	60	NULL	NULL
2015-07-12	East	Howard	Binder	29	NULL	NULL
2015-07-29	East	Parent	Binder	81	NULL	1
2015-10-05	Central	Morgan	Binder	28	NULL	NULL
2015-10-22	East	Jones	Pen	64	NULL	NULL
2015-11-08	East	Parent	Pen	15	NULL	NULL
2016-01-15	Central	Gill	Binder	46	NULL	NULL
2016-02-01	Central	Smith	Binder	87	NULL	1
2016-02-18	East	Jones	Binder	4	NULL	NULL
2016-03-07	West	Sorvino	Binder	7	NULL	NULL
2016-04-27	East	Howard	Pen	96	NULL	NULL
2016-05-31	Central	Gill	Binder	80	NULL	NULL
2016-09-27	West	Sorvino	Pen	76	NULL	NULL
2016-10-14	West	Thompson	Binder	57	NULL	1
2016-11-17	Central	Jardine	Binder	11	NULL	NULL
2016-12-04	Central	Jardine	Binder	94	NULL	1
2016-12-21	Central	Andrews	Binder	28	NULL	NULL
Time taken: 0.14 seconds, Fetched: 20 row(s)
hive (dkmdb01)> select * from salesorders3 tablesample(bucket 3 out of 3);
OK
salesorders3.ord_dt	salesorders3.region	salesorders3.rep	salesorders3.item	salesorders3.unit	salesorders3.unit_cost	salesorders3.total
2015-01-06	East	Jones	Pencil	95	NULL	NULL
2015-02-09	Central	Jardine	Pencil	36	NULL	NULL
2015-03-15	West	Sorvino	Pencil	56	NULL	NULL
2015-04-18	Central	Andrews	Pencil	75	NULL	NULL
2015-05-05	Central	Jardine	Pencil	90	NULL	NULL
2015-05-22	West	Thompson	Pencil	32	NULL	NULL
2015-06-25	Central	Morgan	Pencil	90	NULL	NULL
2015-08-15	East	Jones	Pencil	35	NULL	NULL
2015-09-01	Central	Smith	Desk	2	NULL	NULL
2015-09-18	East	Jones	Pen Set	16	NULL	NULL
2015-11-25	Central	Kivell	Pen Set	96	NULL	NULL
2015-12-12	Central	Smith	Pencil	67	NULL	NULL
2015-12-29	East	Parent	Pen Set	74	NULL	1
2016-03-24	Central	Jardine	Pen Set	50	NULL	NULL
2016-04-10	Central	Andrews	Pencil	66	NULL	NULL
2016-05-14	Central	Gill	Pencil	53	NULL	NULL
2016-06-17	Central	Kivell	Desk	5	NULL	NULL
2016-07-04	East	Jones	Pen Set	62	NULL	NULL
2016-07-21	Central	Morgan	Pen Set	55	NULL	NULL
2016-08-07	Central	Kivell	Pen Set	42	NULL	1
2016-08-24	West	Sorvino	Desk	3	NULL	NULL
2016-09-10	Central	Gill	Pencil	7	NULL	NULL
2016-10-31	Central	Andrews	Pencil	14	NULL	NULL
Time taken: 0.66 seconds, Fetched: 23 row(s)
====================================================================================================
hive (dkmdb01)> drop table salesorders3;
OK
Time taken: 0.605 seconds
hive (dkmdb01)> create table salesorders3 (ord_dt date, region varchar(8), rep varchar(15), item varchar(15), unit int, unit_cost decimal (9,2),
              > total decimal(10,2))
              > clustered by (item) into 3 buckets
              > row format delimited
              > fields terminated by ','
              > stored as textfile location '/user/hduser/hiveexternaldata/salesorders3';
OK
Time taken: 0.13 seconds
hive (dkmdb01)> set map.reduce.tasks=3;                                                  
hive (dkmdb01)> insert into table salesorders3 select ord_dt,region,rep,item,unit,unit_cost,total from salesorders;
Query ID = hduser_20160711214646_d05bb84f-c0f7-4bf6-8792-35583a8c2980
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1468250619678_0004, Tracking URL = http://Inceptez:8088/proxy/application_1468250619678_0004/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1468250619678_0004
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 3
2016-07-11 21:47:14,756 Stage-1 map = 0%,  reduce = 0%
2016-07-11 21:47:38,083 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.47 sec
2016-07-11 21:48:36,715 Stage-1 map = 100%,  reduce = 67%, Cumulative CPU 6.39 sec
2016-07-11 21:48:39,832 Stage-1 map = 100%,  reduce = 69%, Cumulative CPU 8.53 sec
2016-07-11 21:48:44,624 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 13.13 sec
MapReduce Total cumulative CPU time: 13 seconds 130 msec
Ended Job = job_1468250619678_0004
Loading data to table dkmdb01.salesorders3
[Error 30017]: Skipping stats aggregation by error org.apache.hadoop.hive.ql.metadata.HiveException: [Error 30001]: StatsPublisher cannot be initialized. There was a error in the initialization of StatsPublisher, and retrying might help. If you dont want the query to fail because accurate statistics could not be collected, set hive.stats.reliable=false
Table dkmdb01.salesorders3 stats: [numFiles=3, totalSize=1719]
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 3   Cumulative CPU: 15.51 sec   HDFS Read: 2399 HDFS Write: 1719 SUCCESS
Total MapReduce CPU Time Spent: 15 seconds 510 msec
OK
ord_dt	region	rep	item	unit	unit_cost	total
Time taken: 120.674 seconds
hive (dkmdb01)> select * from salesorders3 tablesample(bucket 1 out of 3);                                                                      
OK
salesorders3.ord_dt	salesorders3.region	salesorders3.rep	salesorders3.item	salesorders3.unit	salesorders3.unit_cost	salesorders3.total
Time taken: 0.196 seconds
hive (dkmdb01)> select * from salesorders3 tablesample(bucket 2 out of 3); 
OK
salesorders3.ord_dt	salesorders3.region	salesorders3.rep	salesorders3.item	salesorders3.unit	salesorders3.unit_cost	salesorders3.total
2016-09-27	West	Sorvino	Pen	76	NULL	NULL
2016-04-27	East	Howard	Pen	96	NULL	NULL
2015-11-08	East	Parent	Pen	15	NULL	NULL
2015-10-22	East	Jones	Pen	64	NULL	NULL
2015-02-26	Central	Gill	Pen	27	NULL	NULL
2016-12-04	Central	Jardine	Binder	94	NULL	1
2015-10-05	Central	Morgan	Binder	28	NULL	NULL
2015-01-23	Central	Kivell	Binder	50	NULL	NULL
2016-03-07	West	Sorvino	Binder	7	NULL	NULL
2015-07-29	East	Parent	Binder	81	NULL	1
2015-07-12	East	Howard	Binder	29	NULL	NULL
2016-05-31	Central	Gill	Binder	80	NULL	NULL
2015-06-08	East	Jones	Binder	60	NULL	NULL
2016-10-14	West	Thompson	Binder	57	NULL	1
2015-04-01	East	Jones	Binder	60	NULL	NULL
2016-01-15	Central	Gill	Binder	46	NULL	NULL
2016-02-01	Central	Smith	Binder	87	NULL	1
2016-02-18	East	Jones	Binder	4	NULL	NULL
2016-11-17	Central	Jardine	Binder	11	NULL	NULL
2016-12-21	Central	Andrews	Binder	28	NULL	NULL
Time taken: 0.147 seconds, Fetched: 20 row(s)
hive (dkmdb01)> select * from salesorders3 tablesample(bucket 3 out of 3); 
OK
salesorders3.ord_dt	salesorders3.region	salesorders3.rep	salesorders3.item	salesorders3.unit	salesorders3.unit_cost	salesorders3.total
2016-05-14	Central	Gill	Pencil	53	NULL	NULL
2015-02-09	Central	Jardine	Pencil	36	NULL	NULL
2016-04-10	Central	Andrews	Pencil	66	NULL	NULL
2016-09-10	Central	Gill	Pencil	7	NULL	NULL
2015-08-15	East	Jones	Pencil	35	NULL	NULL
2015-12-12	Central	Smith	Pencil	67	NULL	NULL
2015-01-06	East	Jones	Pencil	95	NULL	NULL
2015-05-05	Central	Jardine	Pencil	90	NULL	NULL
2015-03-15	West	Sorvino	Pencil	56	NULL	NULL
2016-10-31	Central	Andrews	Pencil	14	NULL	NULL
2015-04-18	Central	Andrews	Pencil	75	NULL	NULL
2015-05-22	West	Thompson	Pencil	32	NULL	NULL
2015-06-25	Central	Morgan	Pencil	90	NULL	NULL
2015-12-29	East	Parent	Pen Set	74	NULL	1
2016-08-07	Central	Kivell	Pen Set	42	NULL	1
2016-07-21	Central	Morgan	Pen Set	55	NULL	NULL
2016-07-04	East	Jones	Pen Set	62	NULL	NULL
2016-03-24	Central	Jardine	Pen Set	50	NULL	NULL
2015-11-25	Central	Kivell	Pen Set	96	NULL	NULL
2015-09-18	East	Jones	Pen Set	16	NULL	NULL
2015-09-01	Central	Smith	Desk	2	NULL	NULL
2016-06-17	Central	Kivell	Desk	5	NULL	NULL
2016-08-24	West	Sorvino	Desk	3	NULL	NULL
Time taken: 0.141 seconds, Fetched: 23 row(s)

[hduser@Inceptez hive_data_01]$ hdfs dfs -ls -R /user/hduser/hiveexternaldata/salesorders3
ls: `/user/hduser/hiveexternaldata/salesorders3': No such file or directory
[hduser@Inceptez hive_data_01]$ hdfs dfs -ls -R /user/hduser/hiveexternaldata/salesorders3
-rw-r--r--   1 hduser supergroup        534 2016-07-11 21:48 /user/hduser/hiveexternaldata/salesorders3/000000_0
-rw-r--r--   1 hduser supergroup        470 2016-07-11 21:48 /user/hduser/hiveexternaldata/salesorders3/000001_0
-rw-r--r--   1 hduser supergroup        715 2016-07-11 21:48 /user/hduser/hiveexternaldata/salesorders3/000002_0
[hduser@Inceptez hive_data_01]$ hdfs dfs -cat /user/hduser/hiveexternaldata/salesorders3/*0_*
2016-05-14,Central,Gill,Pencil,53,\N,\N
2015-02-09,Central,Jardine,Pencil,36,\N,\N
2016-04-10,Central,Andrews,Pencil,66,\N,\N
2016-09-10,Central,Gill,Pencil,7,\N,\N
2015-08-15,East,Jones,Pencil,35,\N,\N
2015-12-12,Central,Smith,Pencil,67,\N,\N
2015-01-06,East,Jones,Pencil,95,\N,\N
2015-05-05,Central,Jardine,Pencil,90,\N,\N
2015-03-15,West,Sorvino,Pencil,56,\N,\N
2016-10-31,Central,Andrews,Pencil,14,\N,\N
2015-04-18,Central,Andrews,Pencil,75,\N,\N
2015-05-22,West,Thompson,Pencil,32,\N,\N
2015-06-25,Central,Morgan,Pencil,90,\N,\N
[hduser@Inceptez hive_data_01]$ hdfs dfs -cat /user/hduser/hiveexternaldata/salesorders3/*1_*
2015-12-29,East,Parent,Pen Set,74,\N,1
2016-09-27,West,Sorvino,Pen,76,\N,\N
2016-08-07,Central,Kivell,Pen Set,42,\N,1
2016-07-21,Central,Morgan,Pen Set,55,\N,\N
2016-07-04,East,Jones,Pen Set,62,\N,\N
2016-04-27,East,Howard,Pen,96,\N,\N
2016-03-24,Central,Jardine,Pen Set,50,\N,\N
2015-11-25,Central,Kivell,Pen Set,96,\N,\N
2015-11-08,East,Parent,Pen,15,\N,\N
2015-10-22,East,Jones,Pen,64,\N,\N
2015-09-18,East,Jones,Pen Set,16,\N,\N
2015-02-26,Central,Gill,Pen,27,\N,\N
[hduser@Inceptez hive_data_01]$ hdfs dfs -cat /user/hduser/hiveexternaldata/salesorders3/*2_*
2016-12-04,Central,Jardine,Binder,94,\N,1
2015-10-05,Central,Morgan,Binder,28,\N,\N
2015-01-23,Central,Kivell,Binder,50,\N,\N
2015-09-01,Central,Smith,Desk,2,\N,\N
2016-03-07,West,Sorvino,Binder,7,\N,\N
2015-07-29,East,Parent,Binder,81,\N,1
2015-07-12,East,Howard,Binder,29,\N,\N
2016-05-31,Central,Gill,Binder,80,\N,\N
2015-06-08,East,Jones,Binder,60,\N,\N
2016-06-17,Central,Kivell,Desk,5,\N,\N
2016-08-24,West,Sorvino,Desk,3,\N,\N
2016-10-14,West,Thompson,Binder,57,\N,1
2015-04-01,East,Jones,Binder,60,\N,\N
2016-01-15,Central,Gill,Binder,46,\N,\N
2016-02-01,Central,Smith,Binder,87,\N,1
2016-02-18,East,Jones,Binder,4,\N,\N
2016-11-17,Central,Jardine,Binder,11,\N,\N
2016-12-21,Central,Andrews,Binder,28,\N,\N
====================================================================================================
