scala> val ce = sc.textFile("file:/home/hduser/mydata01/CanEmp.csv")
ce: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[3] at textFile at <console>:21

scala> val cenh = ce.filter(_.charAt(0) != 'i')
cenh: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[4] at filter at <console>:23

scala> cenh.toDebugString
16/09/24 22:05:15 INFO mapred.FileInputFormat: Total input paths to process : 1
res0: String =
(1) MapPartitionsRDD[4] at filter at <console>:23 []
 |  MapPartitionsRDD[3] at textFile at <console>:21 []
 |  file:/home/hduser/mydata01/CanEmp.csv HadoopRDD[2] at textFile at <console>:21 []

scala> cenh.partitions.size
res1: Int = 1

scala> cenh.first
res2: String = 1730,Isaac Mack,"P.O. Box 248, 7414 Dapibus Ave",Mundare,P1H 2A1,Canada,(666) 753-5715,11380.73,13

scala> cenh.take(3)
res3: Array[String] = Array(1730,Isaac Mack,"P.O. Box 248, 7414 Dapibus Ave",Mundare,P1H 2A1,Canada,(666) 753-5715,11380.73,13, 5257,Barrett Herrera,Ap #682-4918 Lorem St.,Daly,A3V 5R5,Canada,(232) 932-3742,117146.97,11, 1305,Louis Sharp,"P.O. Box 512, 4042 Facilisis Av.",Valleyview,E7X 2P8,Canada,(453) 117-5523,244760.14,23)

scala> cenh.cache()
res4: cenh.type = MapPartitionsRDD[4] at filter at <console>:23

>>> didn't showup in UI Storage

scala> val cenh_split = cenh.map(x => x.split(","))
cenh_split: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[5] at map at <console>:25

scala> cenh_split.toDebugString
res5: String =
(1) MapPartitionsRDD[5] at map at <console>:25 []
 |  MapPartitionsRDD[4] at filter at <console>:23 []
 |  MapPartitionsRDD[3] at textFile at <console>:21 []
 |  file:/home/hduser/mydata01/CanEmp.csv HadoopRDD[2] at textFile at <console>:21 []

scala> cenh_split.take(10)
16/09/24 22:11:47 INFO spark.SparkContext: Starting job: take at <console>:28
16/09/24 22:11:47 INFO scheduler.DAGScheduler: Got job 2 (take at <console>:28) with 1 output partitions
16/09/24 22:11:47 INFO scheduler.DAGScheduler: Final stage: ResultStage 2 (take at <console>:28)
16/09/24 22:11:47 INFO scheduler.DAGScheduler: Parents of final stage: List()
16/09/24 22:11:47 INFO scheduler.DAGScheduler: Missing parents: List()
16/09/24 22:11:47 INFO scheduler.DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[5] at map at <console>:25), which has no missing parents
16/09/24 22:11:47 INFO storage.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 3.5 KB, free 363.1 KB)
16/09/24 22:11:47 INFO storage.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 2006.0 B, free 365.0 KB)
16/09/24 22:11:47 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on localhost:43112 (size: 2006.0 B, free: 517.4 MB)
16/09/24 22:11:47 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1006
16/09/24 22:11:47 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at map at <console>:25)
16/09/24 22:11:47 INFO scheduler.TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
16/09/24 22:11:47 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 2139 bytes)
16/09/24 22:11:47 INFO executor.Executor: Running task 0.0 in stage 2.0 (TID 2)
16/09/24 22:11:47 INFO spark.CacheManager: Partition rdd_4_0 not found, computing it
16/09/24 22:11:47 INFO rdd.HadoopRDD: Input split: file:/home/hduser/mydata01/CanEmp.csv:0+9615
16/09/24 22:11:47 INFO storage.MemoryStore: Block rdd_4_0 stored as values in memory (estimated size 23.1 KB, free 388.1 KB)
16/09/24 22:11:47 INFO storage.BlockManagerInfo: Added rdd_4_0 in memory on localhost:43112 (size: 23.1 KB, free: 517.3 MB)
16/09/24 22:11:47 INFO executor.Executor: Finished task 0.0 in stage 2.0 (TID 2). 3879 bytes result sent to driver
16/09/24 22:11:47 INFO scheduler.DAGScheduler: ResultStage 2 (take at <console>:28) finished in 0.292 s
16/09/24 22:11:47 INFO scheduler.DAGScheduler: Job 2 finished: take at <console>:28, took 0.329277 s
16/09/24 22:11:47 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 294 ms on localhost (1/1)
16/09/24 22:11:47 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
res6: Array[Array[String]] = Array(Array(1730, Isaac Mack, "P.O. Box 248, " 7414 Dapibus Ave"", Mundare, P1H 2A1, Canada, (666) 753-5715, 11380.73, 13), Array(5257, Barrett Herrera, Ap #682-4918 Lorem St., Daly, A3V 5R5, Canada, (232) 932-3742, 117146.97, 11), Array(1305, Louis Sharp, "P.O. Box 512, " 4042 Facilisis Av."", Valleyview, E7X 2P8, Canada, (453) 117-5523, 244760.14, 23), Array(2393, Samson Ramirez, 2795 A St., Isle-aux-Coudres, E0X 4B3, Canada, (172) 805-2137, 86177.41, 13), Array(1138, Griffith Perkins, "9452 A, " Road"", Flin Flon, P5X 7J4, Canada, (136) 964-5830, 438969.41, 11), Array(1903, Lester Moses, 107-5311 Molestie St., Midlands, G7S 4T6, Canada, (864) 341-9449, 314634.13, 21), Array(5877, Odysseus Frost, 794-1399 Dui. Rd., Whitehorse, C7T 6S4, Canada, (733) 578-28...
scala> val cenh_split2 = cenh.map(x => (x.split(",")(1), x.split(",")(7)))
cenh_split2: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[6] at map at <console>:25

>>> showed up in UI Storage now

scala> val cenh_split2 = cenh.map(x => (x.split(",")(1), x.split(",")(7)))
cenh_split2: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[6] at map at <console>:25

scala> cenh_split2.toDebugString
res7: String =
(1) MapPartitionsRDD[6] at map at <console>:25 []
 |  MapPartitionsRDD[4] at filter at <console>:23 []
 |      CachedPartitions: 1; MemorySize: 23.1 KB; ExternalBlockStoreSize: 0.0 B; DiskSize: 0.0 B
 |  MapPartitionsRDD[3] at textFile at <console>:21 []
 |  file:/home/hduser/mydata01/CanEmp.csv HadoopRDD[2] at textFile at <console>:21 []

scala> cenh_split2.take(3)
16/09/24 22:16:11 INFO spark.SparkContext: Starting job: take at <console>:28
16/09/24 22:16:11 INFO scheduler.DAGScheduler: Got job 3 (take at <console>:28) with 1 output partitions
16/09/24 22:16:11 INFO scheduler.DAGScheduler: Final stage: ResultStage 3 (take at <console>:28)
16/09/24 22:16:11 INFO scheduler.DAGScheduler: Parents of final stage: List()
16/09/24 22:16:11 INFO scheduler.DAGScheduler: Missing parents: List()
16/09/24 22:16:11 INFO scheduler.DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[6] at map at <console>:25), which has no missing parents
16/09/24 22:16:11 INFO storage.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 3.4 KB, free 391.6 KB)
16/09/24 22:16:11 INFO storage.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 1994.0 B, free 393.5 KB)
16/09/24 22:16:11 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on localhost:43112 (size: 1994.0 B, free: 517.3 MB)
16/09/24 22:16:11 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1006
16/09/24 22:16:11 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[6] at map at <console>:25)
16/09/24 22:16:11 INFO scheduler.TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
16/09/24 22:16:11 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, localhost, partition 0,PROCESS_LOCAL, 2139 bytes)
16/09/24 22:16:11 INFO executor.Executor: Running task 0.0 in stage 3.0 (TID 3)
16/09/24 22:16:11 INFO storage.BlockManager: Found block rdd_4_0 locally
16/09/24 22:16:11 INFO executor.Executor: Finished task 0.0 in stage 3.0 (TID 3). 2263 bytes result sent to driver
16/09/24 22:16:11 INFO scheduler.DAGScheduler: ResultStage 3 (take at <console>:28) finished in 0.073 s
16/09/24 22:16:11 INFO scheduler.DAGScheduler: Job 3 finished: take at <console>:28, took 0.184120 s
16/09/24 22:16:11 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 79 ms on localhost (1/1)
16/09/24 22:16:11 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
res8: Array[(String, String)] = Array((Isaac Mack,(666) 753-5715), (Barrett Herrera,117146.97), (Louis Sharp,(453) 117-5523))

scala> cenh.uncache()
<console>:26: error: value uncache is not a member of org.apache.spark.rdd.RDD[String]
              cenh.uncache()
                   ^
scala> cenh.uncache
<console>:26: error: value uncache is not a member of org.apache.spark.rdd.RDD[String]
              cenh.uncache
                   ^
scala> cenh.unpersist()
16/09/24 22:29:03 INFO rdd.MapPartitionsRDD: Removing RDD 4 from persistence list
16/09/24 22:29:03 INFO storage.BlockManager: Removing RDD 4
res12: cenh.type = MapPartitionsRDD[4] at filter at <console>:23

>>> removed the RDD immediately from storage in spark UI