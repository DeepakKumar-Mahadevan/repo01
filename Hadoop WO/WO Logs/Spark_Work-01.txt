[hduser@Inceptez mydata01]$ vi spark_input.txt
[hduser@Inceptez mydata01]$ hdfs dfs -mkdir spark_data
16/07/30 12:04:02 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[hduser@Inceptez mydata01]$ hadoop fs -put spark_input.txt spark_data/
16/07/30 12:04:28 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[hduser@Inceptez mydata01]$ hdfs dfs -ls spark_data
16/07/30 12:05:10 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Found 1 items
-rw-r--r--   1 hduser supergroup        399 2016-07-30 12:04 spark_data/spark_input.txt
====================================================================================================
spark-shell

scala> val a =sc.textFile("input.txt").map(x=>x.toUpperCase())
16/07/30 12:15:59 INFO storage.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 86.5 KB, free 86.5 KB)
16/07/30 12:15:59 INFO storage.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 19.6 KB, free 106.0 KB)
16/07/30 12:15:59 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:39983 (size: 19.6 KB, free: 517.4 MB)
16/07/30 12:15:59 INFO spark.SparkContext: Created broadcast 0 from textFile at <console>:27
a: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at map at <console>:27

scala> a.cache()
res0: a.type = MapPartitionsRDD[2] at map at <console>:27

scala> a.collect()
org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://localhost:54310/user/hduser/input.txt
	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:285)
	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:228)
	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:313)
	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:199)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:237)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:237)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:237)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1929)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:927)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:926)
	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:30)
	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:35)
	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:37)
	at $iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:39)
	at $iwC$$iwC$$iwC$$iwC.<init>(<console>:41)
	at $iwC$$iwC$$iwC.<init>(<console>:43)
	at $iwC$$iwC.<init>(<console>:45)
	at $iwC.<init>(<console>:47)
	at <init>(<console>:49)
	at .<init>(<console>:53)
	at .<clinit>(<console>)
	at .<init>(<console>:7)
	at .<clinit>(<console>)
	at $print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)
	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:857)
	at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:902)
	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:814)
	at org.apache.spark.repl.SparkILoop.processLine$1(SparkILoop.scala:657)
	at org.apache.spark.repl.SparkILoop.innerLoop$1(SparkILoop.scala:665)
	at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$loop(SparkILoop.scala:670)
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply$mcZ$sp(SparkILoop.scala:997)
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945)
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945)
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
	at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$process(SparkILoop.scala:945)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1059)
	at org.apache.spark.repl.Main$.main(Main.scala:31)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

scala> val a =sc.textFile("spark_data/spark_input.txt").map(x=>x.toUpperCase())
16/07/30 12:18:46 INFO storage.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 228.9 KB, free 583.3 KB)
16/07/30 12:18:46 INFO storage.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 19.6 KB, free 602.9 KB)
16/07/30 12:18:46 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:39983 (size: 19.6 KB, free: 517.3 MB)
16/07/30 12:18:46 INFO spark.SparkContext: Created broadcast 2 from textFile at <console>:27
a: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[8] at map at <console>:27

scala> a.cache()
res4: a.type = MapPartitionsRDD[8] at map at <console>:27

scala> a.collect()
16/07/30 12:18:51 INFO mapred.FileInputFormat: Total input paths to process : 1
16/07/30 12:18:52 INFO spark.SparkContext: Starting job: collect at <console>:30
16/07/30 12:18:52 INFO scheduler.DAGScheduler: Got job 0 (collect at <console>:30) with 1 output partitions
16/07/30 12:18:52 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (collect at <console>:30)
16/07/30 12:18:52 INFO scheduler.DAGScheduler: Parents of final stage: List()
16/07/30 12:18:52 INFO scheduler.DAGScheduler: Missing parents: List()
16/07/30 12:18:52 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[8] at map at <console>:27), which has no missing parents
16/07/30 12:18:53 INFO storage.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 3.2 KB, free 606.1 KB)
16/07/30 12:18:53 INFO storage.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 1881.0 B, free 607.9 KB)
16/07/30 12:18:53 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:39983 (size: 1881.0 B, free: 517.3 MB)
16/07/30 12:18:53 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1006
16/07/30 12:18:53 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[8] at map at <console>:27)
16/07/30 12:18:53 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/07/30 12:18:53 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,ANY, 2163 bytes)
16/07/30 12:18:53 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
16/07/30 12:18:53 INFO spark.CacheManager: Partition rdd_8_0 not found, computing it
16/07/30 12:18:53 INFO rdd.HadoopRDD: Input split: hdfs://localhost:54310/user/hduser/spark_data/spark_input.txt:0+399
16/07/30 12:18:53 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
16/07/30 12:18:53 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
16/07/30 12:18:53 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
16/07/30 12:18:53 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
16/07/30 12:18:53 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id
16/07/30 12:18:54 INFO storage.MemoryStore: Block rdd_8_0 stored as values in memory (estimated size 1264.0 B, free 609.1 KB)
16/07/30 12:18:54 INFO storage.BlockManagerInfo: Added rdd_8_0 in memory on localhost:39983 (size: 1264.0 B, free: 517.3 MB)
16/07/30 12:18:55 INFO executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 3046 bytes result sent to driver
16/07/30 12:18:55 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1915 ms on localhost (1/1)
16/07/30 12:18:55 INFO scheduler.DAGScheduler: ResultStage 0 (collect at <console>:30) finished in 2.061 s
16/07/30 12:18:55 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/07/30 12:18:55 INFO scheduler.DAGScheduler: Job 0 finished: collect at <console>:30, took 2.640325 s
res5: Array[String] = Array(AND WHAT IS LOVE? IT IS A DOLL DRESSED UP, FOR IDLENESS TO COSSET, NURSE, AND DANDLE;, A THING OF SOFT MISNOMERS, SO DIVINE, THAT SILLY YOUTH DOTH THINK TO MAKE ITSELF, DIVINE BY LOVING, AND SO GOES ON, YAWNING AND DOTING A WHOLE SUMMER LONG,, TILL MISS'S COMB IS MADE A PERFECT TIARA,, AND COMMON WELLINGTONS TURN ROMEO BOOTS;, TILL CLEOPATRA LIVES AT NUMBER SEVEN,, AND ANTONY RESIDES IN BRUNSWICK SQUARE.)

scala> val b =a.filter(x=>x.startsWith("AND"))
b: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[9] at filter at <console>:29

scala> b.collect()
16/07/30 12:19:47 INFO spark.SparkContext: Starting job: collect at <console>:32
16/07/30 12:19:47 INFO scheduler.DAGScheduler: Got job 1 (collect at <console>:32) with 1 output partitions
16/07/30 12:19:47 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (collect at <console>:32)
16/07/30 12:19:47 INFO scheduler.DAGScheduler: Parents of final stage: List()
16/07/30 12:19:47 INFO scheduler.DAGScheduler: Missing parents: List()
16/07/30 12:19:47 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at filter at <console>:29), which has no missing parents
16/07/30 12:19:47 INFO storage.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 3.4 KB, free 612.6 KB)
16/07/30 12:19:47 INFO storage.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 1946.0 B, free 614.5 KB)
16/07/30 12:19:47 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on localhost:39983 (size: 1946.0 B, free: 517.3 MB)
16/07/30 12:19:47 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1006
16/07/30 12:19:47 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at filter at <console>:29)
16/07/30 12:19:47 INFO scheduler.TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
16/07/30 12:19:47 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2163 bytes)
16/07/30 12:19:47 INFO executor.Executor: Running task 0.0 in stage 1.0 (TID 1)
16/07/30 12:19:47 INFO storage.BlockManager: Found block rdd_8_0 locally
16/07/30 12:19:47 INFO executor.Executor: Finished task 0.0 in stage 1.0 (TID 1). 2173 bytes result sent to driver
16/07/30 12:19:47 INFO scheduler.DAGScheduler: ResultStage 1 (collect at <console>:32) finished in 0.010 s
16/07/30 12:19:47 INFO scheduler.DAGScheduler: Job 1 finished: collect at <console>:32, took 0.029250 s
res6: Array[String] = Array(AND WHAT IS LOVE? IT IS A DOLL DRESSED UP, AND COMMON WELLINGTONS TURN ROMEO BOOTS;, AND ANTONY RESIDES IN BRUNSWICK SQUARE.)

scala> b.count()
16/07/30 12:20:43 INFO spark.SparkContext: Starting job: count at <console>:32
16/07/30 12:20:43 INFO scheduler.DAGScheduler: Got job 2 (count at <console>:32) with 1 output partitions
16/07/30 12:20:43 INFO scheduler.DAGScheduler: Final stage: ResultStage 2 (count at <console>:32)
16/07/30 12:20:43 INFO scheduler.DAGScheduler: Parents of final stage: List()
16/07/30 12:20:43 INFO scheduler.DAGScheduler: Missing parents: List()
16/07/30 12:20:43 INFO scheduler.DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[9] at filter at <console>:29), which has no missing parents
16/07/30 12:20:43 INFO storage.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 3.3 KB, free 617.8 KB)
16/07/30 12:20:43 INFO storage.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 1914.0 B, free 619.6 KB)
16/07/30 12:20:43 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on localhost:39983 (size: 1914.0 B, free: 517.3 MB)
16/07/30 12:20:43 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1006
16/07/30 12:20:43 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at filter at <console>:29)
16/07/30 12:20:43 INFO scheduler.TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
16/07/30 12:20:43 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 2163 bytes)
16/07/30 12:20:43 INFO executor.Executor: Running task 0.0 in stage 2.0 (TID 2)
16/07/30 12:20:43 INFO storage.BlockManager: Found block rdd_8_0 locally
16/07/30 12:20:43 INFO executor.Executor: Finished task 0.0 in stage 2.0 (TID 2). 2082 bytes result sent to driver
16/07/30 12:20:43 INFO scheduler.DAGScheduler: ResultStage 2 (count at <console>:32) finished in 0.032 s
16/07/30 12:20:43 INFO scheduler.DAGScheduler: Job 2 finished: count at <console>:32, took 0.064383 s
res7: Long = 3
====================================================================================================
pyspark
>>> mydata = sc.textFile("input.txt")
16/07/30 12:22:43 INFO storage.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 153.9 KB, free 153.9 KB)
16/07/30 12:22:43 INFO storage.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.1 KB, free 168.1 KB)
16/07/30 12:22:43 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:50798 (size: 14.1 KB, free: 517.4 MB)
16/07/30 12:22:43 INFO spark.SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:-2
>>> mydata = sc.textFile("spark_data/spark_input.txt")
16/07/30 12:22:58 INFO storage.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 154.0 KB, free 322.0 KB)
16/07/30 12:22:58 INFO storage.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 14.1 KB, free 336.2 KB)
16/07/30 12:22:58 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:50798 (size: 14.1 KB, free: 517.4 MB)
16/07/30 12:22:58 INFO spark.SparkContext: Created broadcast 1 from textFile at NativeMethodAccessorImpl.java:-2
>>> myrdd = mydata.map(lambda s:s.upper())
>>> myrdd.cache()
PythonRDD[4] at RDD at PythonRDD.scala:43
>>> print(myrdd.collect())

16/07/30 12:23:31 INFO mapred.FileInputFormat: Total input paths to process : 1
16/07/30 12:23:31 INFO spark.SparkContext: Starting job: collect at <stdin>:1
16/07/30 12:23:32 INFO scheduler.DAGScheduler: Got job 0 (collect at <stdin>:1) with 1 output partitions
16/07/30 12:23:32 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (collect at <stdin>:1)
16/07/30 12:23:32 INFO scheduler.DAGScheduler: Parents of final stage: List()
16/07/30 12:23:32 INFO scheduler.DAGScheduler: Missing parents: List()
16/07/30 12:23:32 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (PythonRDD[4] at RDD at PythonRDD.scala:43), which has no missing parents
16/07/30 12:23:32 INFO storage.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 4.8 KB, free 341.0 KB)
16/07/30 12:23:32 INFO storage.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.0 KB, free 343.9 KB)
16/07/30 12:23:32 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:50798 (size: 3.0 KB, free: 517.4 MB)
16/07/30 12:23:32 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1006
16/07/30 12:23:32 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (PythonRDD[4] at RDD at PythonRDD.scala:43)
16/07/30 12:23:32 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/07/30 12:23:32 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,ANY, 2163 bytes)
16/07/30 12:23:32 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
16/07/30 12:23:32 INFO spark.CacheManager: Partition rdd_4_0 not found, computing it
16/07/30 12:23:32 INFO rdd.HadoopRDD: Input split: hdfs://localhost:54310/user/hduser/spark_data/spark_input.txt:0+399
16/07/30 12:23:32 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
16/07/30 12:23:32 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
16/07/30 12:23:32 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
16/07/30 12:23:32 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
16/07/30 12:23:32 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id
16/07/30 12:23:35 INFO python.PythonRunner: Times: total = 2835, boot = 2302, init = 510, finish = 23
16/07/30 12:23:35 INFO storage.MemoryStore: Block rdd_4_0 stored as bytes in memory (estimated size 518.0 B, free 344.4 KB)
16/07/30 12:23:35 INFO storage.BlockManagerInfo: Added rdd_4_0 in memory on localhost:50798 (size: 518.0 B, free: 517.4 MB)
16/07/30 12:23:36 INFO executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 3218 bytes result sent to driver
16/07/30 12:23:36 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3615 ms on localhost (1/1)
16/07/30 12:23:36 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/07/30 12:23:36 INFO scheduler.DAGScheduler: ResultStage 0 (collect at <stdin>:1) finished in 3.780 s
16/07/30 12:23:36 INFO scheduler.DAGScheduler: Job 0 finished: collect at <stdin>:1, took 4.284592 s
[u'AND WHAT IS LOVE? IT IS A DOLL DRESSED UP', u'FOR IDLENESS TO COSSET, NURSE, AND DANDLE;', u'A THING OF SOFT MISNOMERS, SO DIVINE', u'THAT SILLY YOUTH DOTH THINK TO MAKE ITSELF', u'DIVINE BY LOVING, AND SO GOES ON', u'YAWNING AND DOTING A WHOLE SUMMER LONG,', u"TILL MISS'S COMB IS MADE A PERFECT TIARA,", u'AND COMMON WELLINGTONS TURN ROMEO BOOTS;', u'TILL CLEOPATRA LIVES AT NUMBER SEVEN,', u'AND ANTONY RESIDES IN BRUNSWICK SQUARE.']
>>> myrdd2 = myrdd.filter(lambda s:s.startswith('AND'))
>>> myrdd2.count()
16/07/30 12:24:28 INFO spark.SparkContext: Starting job: count at <stdin>:1
16/07/30 12:24:28 INFO scheduler.DAGScheduler: Got job 1 (count at <stdin>:1) with 1 output partitions
16/07/30 12:24:28 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (count at <stdin>:1)
16/07/30 12:24:28 INFO scheduler.DAGScheduler: Parents of final stage: List()
16/07/30 12:24:28 INFO scheduler.DAGScheduler: Missing parents: List()
16/07/30 12:24:28 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (PythonRDD[5] at count at <stdin>:1), which has no missing parents
16/07/30 12:24:28 INFO storage.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 6.6 KB, free 351.0 KB)
16/07/30 12:24:28 INFO storage.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.0 KB, free 355.0 KB)
16/07/30 12:24:28 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:50798 (size: 4.0 KB, free: 517.4 MB)
16/07/30 12:24:28 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1006
16/07/30 12:24:28 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (PythonRDD[5] at count at <stdin>:1)
16/07/30 12:24:28 INFO scheduler.TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
16/07/30 12:24:28 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2163 bytes)
16/07/30 12:24:28 INFO executor.Executor: Running task 0.0 in stage 1.0 (TID 1)
16/07/30 12:24:28 INFO storage.BlockManager: Found block rdd_4_0 locally
16/07/30 12:24:28 INFO python.PythonRunner: Times: total = 43, boot = -52460, init = 52502, finish = 1
16/07/30 12:24:28 INFO executor.Executor: Finished task 0.0 in stage 1.0 (TID 1). 2124 bytes result sent to driver
16/07/30 12:24:28 INFO scheduler.DAGScheduler: ResultStage 1 (count at <stdin>:1) finished in 0.142 s
16/07/30 12:24:28 INFO scheduler.DAGScheduler: Job 1 finished: count at <stdin>:1, took 0.207119 s
16/07/30 12:24:28 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 143 ms on localhost (1/1)
16/07/30 12:24:28 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
3
====================================================================================================
[hduser@Inceptez mydata01]$ vi coursedetails.txt
[hduser@Inceptez mydata01]$ hdfs dfs -put coursedetails.txt spark_data/
16/07/30 12:32:31 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[hduser@Inceptez mydata01]$ hdfs dfs -ls spark_data
16/07/30 12:34:04 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Found 2 items
-rw-r--r--   1 hduser supergroup         81 2016-07-30 12:32 spark_data/coursedetails.txt
-rw-r--r--   1 hduser supergroup        399 2016-07-30 12:04 spark_data/spark_input.txt

scala> val counts = sc.textFile("spark_data/coursedetails.txt").flatMap(line=>line.split(" ")).map(word=>(word,1)).reduceByKey((v1,v2)=>v1+v2)
16/07/30 12:39:13 INFO storage.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 228.9 KB, free 1081.4 KB)
16/07/30 12:39:13 INFO storage.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 19.6 KB, free 1100.9 KB)
16/07/30 12:39:13 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:39983 (size: 19.6 KB, free: 517.3 MB)
16/07/30 12:39:13 INFO spark.SparkContext: Created broadcast 7 from textFile at <console>:27
16/07/30 12:39:13 INFO mapred.FileInputFormat: Total input paths to process : 1
counts: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[18] at reduceByKey at <console>:27

scala> counts.collect() 
16/07/30 12:40:16 INFO spark.SparkContext: Starting job: collect at <console>:30
16/07/30 12:40:16 INFO scheduler.DAGScheduler: Registering RDD 17 (map at <console>:27)
16/07/30 12:40:16 INFO scheduler.DAGScheduler: Got job 3 (collect at <console>:30) with 1 output partitions
16/07/30 12:40:16 INFO scheduler.DAGScheduler: Final stage: ResultStage 4 (collect at <console>:30)
16/07/30 12:40:16 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
16/07/30 12:40:16 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 3)
16/07/30 12:40:16 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[17] at map at <console>:27), which has no missing parents
16/07/30 12:40:16 INFO storage.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 4.1 KB, free 1105.1 KB)
16/07/30 12:40:16 INFO storage.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 2.3 KB, free 1107.4 KB)
16/07/30 12:40:16 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on localhost:39983 (size: 2.3 KB, free: 517.3 MB)
16/07/30 12:40:16 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1006
16/07/30 12:40:16 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[17] at map at <console>:27)
16/07/30 12:40:16 INFO scheduler.TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
16/07/30 12:40:16 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, localhost, partition 0,ANY, 2154 bytes)
16/07/30 12:40:16 INFO executor.Executor: Running task 0.0 in stage 3.0 (TID 3)
16/07/30 12:40:16 INFO rdd.HadoopRDD: Input split: hdfs://localhost:54310/user/hduser/spark_data/coursedetails.txt:0+81
16/07/30 12:40:17 INFO executor.Executor: Finished task 0.0 in stage 3.0 (TID 3). 2253 bytes result sent to driver
16/07/30 12:40:17 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 521 ms on localhost (1/1)
16/07/30 12:40:17 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
16/07/30 12:40:17 INFO scheduler.DAGScheduler: ShuffleMapStage 3 (map at <console>:27) finished in 0.535 s
16/07/30 12:40:17 INFO scheduler.DAGScheduler: looking for newly runnable stages
16/07/30 12:40:17 INFO scheduler.DAGScheduler: running: Set()
16/07/30 12:40:17 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 4)
16/07/30 12:40:17 INFO scheduler.DAGScheduler: failed: Set()
16/07/30 12:40:17 INFO scheduler.DAGScheduler: Submitting ResultStage 4 (ShuffledRDD[18] at reduceByKey at <console>:27), which has no missing parents
16/07/30 12:40:17 INFO storage.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 2.6 KB, free 1109.9 KB)
16/07/30 12:40:17 INFO storage.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 1589.0 B, free 1111.5 KB)
16/07/30 12:40:17 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on localhost:39983 (size: 1589.0 B, free: 517.3 MB)
16/07/30 12:40:17 INFO spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1006
16/07/30 12:40:17 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (ShuffledRDD[18] at reduceByKey at <console>:27)
16/07/30 12:40:17 INFO scheduler.TaskSchedulerImpl: Adding task set 4.0 with 1 tasks
16/07/30 12:40:17 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4, localhost, partition 0,NODE_LOCAL, 1894 bytes)
16/07/30 12:40:17 INFO executor.Executor: Running task 0.0 in stage 4.0 (TID 4)
16/07/30 12:40:17 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
16/07/30 12:40:17 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 25 ms
16/07/30 12:40:17 INFO executor.Executor: Finished task 0.0 in stage 4.0 (TID 4). 1411 bytes result sent to driver
16/07/30 12:40:17 INFO scheduler.DAGScheduler: ResultStage 4 (collect at <console>:30) finished in 0.440 s
16/07/30 12:40:17 INFO scheduler.DAGScheduler: Job 3 finished: collect at <console>:30, took 1.336844 s
16/07/30 12:40:17 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 444 ms on localhost (1/1)
16/07/30 12:40:17 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
res8: Array[(String, Int)] = Array((hadoop,3), (mainframe,2), (informatica,1), (bigdata,3), (sas,1))

>>> counts = sc.textFile('spark_data/coursedetails.txt').flatMap(lambda line: line.split()).map(lambda word:(word,1)).reduceByKey(lambda v1,v2: v1+v2)
16/07/30 12:43:27 INFO storage.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 228.9 KB, free 503.5 KB)
16/07/30 12:43:27 INFO storage.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 19.6 KB, free 523.1 KB)
16/07/30 12:43:27 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on localhost:50798 (size: 19.6 KB, free: 517.4 MB)
16/07/30 12:43:27 INFO spark.SparkContext: Created broadcast 5 from textFile at NativeMethodAccessorImpl.java:-2
16/07/30 12:43:27 INFO mapred.FileInputFormat: Total input paths to process : 1
>>> print(counts.collect())
16/07/30 12:43:36 INFO spark.SparkContext: Starting job: collect at <stdin>:1
16/07/30 12:43:36 INFO scheduler.DAGScheduler: Registering RDD 11 (reduceByKey at <stdin>:1)
16/07/30 12:43:36 INFO scheduler.DAGScheduler: Got job 2 (collect at <stdin>:1) with 1 output partitions
16/07/30 12:43:36 INFO scheduler.DAGScheduler: Final stage: ResultStage 3 (collect at <stdin>:1)
16/07/30 12:43:36 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
16/07/30 12:43:36 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 2)
16/07/30 12:43:36 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 2 (PairwiseRDD[11] at reduceByKey at <stdin>:1), which has no missing parents
16/07/30 12:43:36 INFO storage.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 8.0 KB, free 531.0 KB)
16/07/30 12:43:36 INFO storage.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 5.1 KB, free 536.1 KB)
16/07/30 12:43:36 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:50798 (size: 5.1 KB, free: 517.3 MB)
16/07/30 12:43:36 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1006
16/07/30 12:43:36 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (PairwiseRDD[11] at reduceByKey at <stdin>:1)
16/07/30 12:43:36 INFO scheduler.TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
16/07/30 12:43:36 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,ANY, 2154 bytes)
16/07/30 12:43:36 INFO executor.Executor: Running task 0.0 in stage 2.0 (TID 2)
16/07/30 12:43:37 INFO rdd.HadoopRDD: Input split: hdfs://localhost:54310/user/hduser/spark_data/coursedetails.txt:0+81
16/07/30 12:43:37 INFO python.PythonRunner: Times: total = 261, boot = 43, init = 101, finish = 117
16/07/30 12:43:37 INFO executor.Executor: Finished task 0.0 in stage 2.0 (TID 2). 2317 bytes result sent to driver
16/07/30 12:43:37 INFO scheduler.DAGScheduler: ShuffleMapStage 2 (reduceByKey at <stdin>:1) finished in 0.933 s
16/07/30 12:43:37 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 930 ms on localhost (1/1)
16/07/30 12:43:37 INFO scheduler.DAGScheduler: looking for newly runnable stages
16/07/30 12:43:37 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
16/07/30 12:43:37 INFO scheduler.DAGScheduler: running: Set()
16/07/30 12:43:37 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 3)
16/07/30 12:43:37 INFO scheduler.DAGScheduler: failed: Set()
16/07/30 12:43:37 INFO scheduler.DAGScheduler: Submitting ResultStage 3 (PythonRDD[14] at collect at <stdin>:1), which has no missing parents
16/07/30 12:43:37 INFO storage.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 4.9 KB, free 541.1 KB)
16/07/30 12:43:37 INFO storage.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 3.1 KB, free 544.2 KB)
16/07/30 12:43:37 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:50798 (size: 3.1 KB, free: 517.3 MB)
16/07/30 12:43:38 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1006
16/07/30 12:43:38 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (PythonRDD[14] at collect at <stdin>:1)
16/07/30 12:43:38 INFO scheduler.TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
16/07/30 12:43:38 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, localhost, partition 0,NODE_LOCAL, 1894 bytes)
16/07/30 12:43:38 INFO executor.Executor: Running task 0.0 in stage 3.0 (TID 3)
16/07/30 12:43:38 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
16/07/30 12:43:38 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 19 ms
16/07/30 12:43:38 INFO python.PythonRunner: Times: total = 25, boot = -861, init = 885, finish = 1
16/07/30 12:43:38 INFO executor.Executor: Finished task 0.0 in stage 3.0 (TID 3). 1365 bytes result sent to driver
16/07/30 12:43:38 INFO scheduler.DAGScheduler: ResultStage 3 (collect at <stdin>:1) finished in 0.333 s
16/07/30 12:43:38 INFO scheduler.DAGScheduler: Job 2 finished: collect at <stdin>:1, took 1.571809 s
16/07/30 12:43:38 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 334 ms on localhost (1/1)
16/07/30 12:43:38 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
[(u'sas', 1), (u'informatica', 1), (u'mainframe', 2), (u'bigdata', 3), (u'hadoop', 3)]
====================================================================================================
scala> val x = sc.parallelize(Array("b","a","c"))
x: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[0] at parallelize at <console>:27

scala> val y = x.map(x=>(x,1))
y: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[1] at map at <console>:29

scala> x.collect()
16/07/30 22:45:35 INFO spark.SparkContext: Starting job: collect at <console>:30
16/07/30 22:45:35 INFO scheduler.DAGScheduler: Got job 0 (collect at <console>:30) with 1 output partitions
16/07/30 22:45:35 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (collect at <console>:30)
16/07/30 22:45:35 INFO scheduler.DAGScheduler: Parents of final stage: List()
16/07/30 22:45:35 INFO scheduler.DAGScheduler: Missing parents: List()
16/07/30 22:45:35 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelize at <console>:27), which has no missing parents
16/07/30 22:45:36 INFO storage.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 1240.0 B, free 1240.0 B)
16/07/30 22:45:36 INFO storage.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 800.0 B, free 2040.0 B)
16/07/30 22:45:36 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:41369 (size: 800.0 B, free: 517.4 MB)
16/07/30 22:45:36 INFO spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1006
16/07/30 22:45:36 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelize at <console>:27)
16/07/30 22:45:36 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/07/30 22:45:36 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2092 bytes)
16/07/30 22:45:36 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
16/07/30 22:45:37 INFO executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 927 bytes result sent to driver
16/07/30 22:45:37 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 519 ms on localhost (1/1)
16/07/30 22:45:37 INFO scheduler.DAGScheduler: ResultStage 0 (collect at <console>:30) finished in 0.646 s
16/07/30 22:45:37 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/07/30 22:45:37 INFO scheduler.DAGScheduler: Job 0 finished: collect at <console>:30, took 1.840646 s
res0: Array[String] = Array(b, a, c)

scala> y.collect()
16/07/30 22:45:46 INFO spark.SparkContext: Starting job: collect at <console>:32
16/07/30 22:45:46 INFO scheduler.DAGScheduler: Got job 1 (collect at <console>:32) with 1 output partitions
16/07/30 22:45:46 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (collect at <console>:32)
16/07/30 22:45:46 INFO scheduler.DAGScheduler: Parents of final stage: List()
16/07/30 22:45:46 INFO scheduler.DAGScheduler: Missing parents: List()
16/07/30 22:45:46 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[1] at map at <console>:29), which has no missing parents
16/07/30 22:45:46 INFO storage.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 1928.0 B, free 3.9 KB)
16/07/30 22:45:46 INFO storage.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 1189.0 B, free 5.0 KB)
16/07/30 22:45:46 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:41369 (size: 1189.0 B, free: 517.4 MB)
16/07/30 22:45:46 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1006
16/07/30 22:45:46 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[1] at map at <console>:29)
16/07/30 22:45:46 INFO scheduler.TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
16/07/30 22:45:46 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2092 bytes)
16/07/30 22:45:46 INFO executor.Executor: Running task 0.0 in stage 1.0 (TID 1)
16/07/30 22:45:46 INFO executor.Executor: Finished task 0.0 in stage 1.0 (TID 1). 1087 bytes result sent to driver
16/07/30 22:45:46 INFO scheduler.DAGScheduler: ResultStage 1 (collect at <console>:32) finished in 0.009 s
16/07/30 22:45:46 INFO scheduler.DAGScheduler: Job 1 finished: collect at <console>:32, took 0.036332 s
16/07/30 22:45:46 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 16 ms on localhost (1/1)
16/07/30 22:45:46 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
res1: Array[(String, Int)] = Array((b,1), (a,1), (c,1))


>>> x = sc.parallelize(["b", "a", "c"])
>>> y = x.map(lambda z: (z, 1))
>>> print(x.collect())
16/07/30 22:47:07 INFO spark.SparkContext: Starting job: collect at <stdin>:1
16/07/30 22:47:07 INFO scheduler.DAGScheduler: Got job 0 (collect at <stdin>:1) with 1 output partitions
16/07/30 22:47:07 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (collect at <stdin>:1)
16/07/30 22:47:07 INFO scheduler.DAGScheduler: Parents of final stage: List()
16/07/30 22:47:07 INFO scheduler.DAGScheduler: Missing parents: List()
16/07/30 22:47:07 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:423), which has no missing parents
16/07/30 22:47:08 INFO storage.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 1224.0 B, free 1224.0 B)
16/07/30 22:47:08 INFO storage.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 777.0 B, free 2001.0 B)
16/07/30 22:47:09 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:45367 (size: 777.0 B, free: 517.4 MB)
16/07/30 22:47:09 INFO spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1006
16/07/30 22:47:09 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:423)
16/07/30 22:47:09 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/07/30 22:47:09 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2091 bytes)
16/07/30 22:47:09 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
16/07/30 22:47:09 INFO executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 939 bytes result sent to driver
16/07/30 22:47:09 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 185 ms on localhost (1/1)
16/07/30 22:47:09 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/07/30 22:47:09 INFO scheduler.DAGScheduler: ResultStage 0 (collect at <stdin>:1) finished in 0.385 s
16/07/30 22:47:09 INFO scheduler.DAGScheduler: Job 0 finished: collect at <stdin>:1, took 1.820871 s
['b', 'a', 'c']
>>> print(y.collect())
16/07/30 22:47:16 INFO spark.SparkContext: Starting job: collect at <stdin>:1
16/07/30 22:47:16 INFO scheduler.DAGScheduler: Got job 1 (collect at <stdin>:1) with 1 output partitions
16/07/30 22:47:16 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (collect at <stdin>:1)
16/07/30 22:47:16 INFO scheduler.DAGScheduler: Parents of final stage: List()
16/07/30 22:47:16 INFO scheduler.DAGScheduler: Missing parents: List()
16/07/30 22:47:16 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (PythonRDD[1] at collect at <stdin>:1), which has no missing parents
16/07/30 22:47:16 INFO storage.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 3.2 KB, free 5.2 KB)
16/07/30 22:47:16 INFO storage.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.1 KB, free 7.3 KB)
16/07/30 22:47:16 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:45367 (size: 2.1 KB, free: 517.4 MB)
16/07/30 22:47:16 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1006
16/07/30 22:47:16 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (PythonRDD[1] at collect at <stdin>:1)
16/07/30 22:47:16 INFO scheduler.TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
16/07/30 22:47:16 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2091 bytes)
16/07/30 22:47:16 INFO executor.Executor: Running task 0.0 in stage 1.0 (TID 1)
16/07/30 22:47:18 INFO python.PythonRunner: Times: total = 2124, boot = 2004, init = 120, finish = 0
16/07/30 22:47:18 INFO executor.Executor: Finished task 0.0 in stage 1.0 (TID 1). 1035 bytes result sent to driver
16/07/30 22:47:18 INFO scheduler.DAGScheduler: ResultStage 1 (collect at <stdin>:1) finished in 2.157 s
16/07/30 22:47:18 INFO scheduler.DAGScheduler: Job 1 finished: collect at <stdin>:1, took 2.186924 s
16/07/30 22:47:18 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2159 ms on localhost (1/1)
16/07/30 22:47:18 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
[('b', 1), ('a', 1), ('c', 1)]
====================================================================================================
scala> val x = sc.parallelize(Array(1,2,3,4,5,6,7,8,9))
x: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[2] at parallelize at <console>:27

scala> val y = x.filter(a=>a%2==1)
y: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[3] at filter at <console>:29

scala> x.collect()
16/07/30 22:51:38 INFO spark.SparkContext: Starting job: collect at <console>:30
16/07/30 22:51:38 INFO scheduler.DAGScheduler: Got job 2 (collect at <console>:30) with 1 output partitions
16/07/30 22:51:38 INFO scheduler.DAGScheduler: Final stage: ResultStage 2 (collect at <console>:30)
16/07/30 22:51:38 INFO scheduler.DAGScheduler: Parents of final stage: List()
16/07/30 22:51:38 INFO scheduler.DAGScheduler: Missing parents: List()
16/07/30 22:51:38 INFO scheduler.DAGScheduler: Submitting ResultStage 2 (ParallelCollectionRDD[2] at parallelize at <console>:27), which has no missing parents
16/07/30 22:51:38 INFO storage.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 1272.0 B, free 1272.0 B)
16/07/30 22:51:38 INFO storage.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 843.0 B, free 2.1 KB)
16/07/30 22:51:38 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:41369 (size: 843.0 B, free: 517.4 MB)
16/07/30 22:51:38 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1006
16/07/30 22:51:38 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (ParallelCollectionRDD[2] at parallelize at <console>:27)
16/07/30 22:51:38 INFO scheduler.TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
16/07/30 22:51:38 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 2054 bytes)
16/07/30 22:51:38 INFO executor.Executor: Running task 0.0 in stage 2.0 (TID 2)
16/07/30 22:51:38 INFO executor.Executor: Finished task 0.0 in stage 2.0 (TID 2). 934 bytes result sent to driver
16/07/30 22:51:38 INFO scheduler.DAGScheduler: ResultStage 2 (collect at <console>:30) finished in 0.019 s
16/07/30 22:51:38 INFO scheduler.DAGScheduler: Job 2 finished: collect at <console>:30, took 0.071077 s
res2: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9)

scala> y.collect()
16/07/30 22:51:49 INFO spark.SparkContext: Starting job: collect at <console>:32
16/07/30 22:51:49 INFO scheduler.DAGScheduler: Got job 3 (collect at <console>:32) with 1 output partitions
16/07/30 22:51:49 INFO scheduler.DAGScheduler: Final stage: ResultStage 3 (collect at <console>:32)
16/07/30 22:51:49 INFO scheduler.DAGScheduler: Parents of final stage: List()
16/07/30 22:51:49 INFO scheduler.DAGScheduler: Missing parents: List()
16/07/30 22:51:49 INFO scheduler.DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[3] at filter at <console>:29), which has no missing parents
16/07/30 22:51:49 INFO storage.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 1952.0 B, free 4.0 KB)
16/07/30 22:51:49 INFO storage.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 1209.0 B, free 5.2 KB)
16/07/30 22:51:49 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:41369 (size: 1209.0 B, free: 517.4 MB)
16/07/30 22:51:49 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1006
16/07/30 22:51:49 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[3] at filter at <console>:29)
16/07/30 22:51:49 INFO scheduler.TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
16/07/30 22:51:49 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, localhost, partition 0,PROCESS_LOCAL, 2054 bytes)
16/07/30 22:51:49 INFO executor.Executor: Running task 0.0 in stage 3.0 (TID 3)
16/07/30 22:51:49 INFO executor.Executor: Finished task 0.0 in stage 3.0 (TID 3). 918 bytes result sent to driver
16/07/30 22:51:49 INFO scheduler.DAGScheduler: ResultStage 3 (collect at <console>:32) finished in 0.003 s
16/07/30 22:51:49 INFO scheduler.DAGScheduler: Job 3 finished: collect at <console>:32, took 0.015926 s
res3: Array[Int] = Array(1, 3, 5, 7, 9)


>>> x = sc.parallelize([1,2,3,4,5,6,7,8,9,10])
>>> y = x.filter(lambda x: x%2 == 1)
>>> print(x.collect())
16/07/30 22:53:46 INFO spark.SparkContext: Starting job: collect at <stdin>:1
16/07/30 22:53:46 INFO scheduler.DAGScheduler: Got job 2 (collect at <stdin>:1) with 1 output partitions
16/07/30 22:53:46 INFO scheduler.DAGScheduler: Final stage: ResultStage 2 (collect at <stdin>:1)
16/07/30 22:53:46 INFO scheduler.DAGScheduler: Parents of final stage: List()
16/07/30 22:53:46 INFO scheduler.DAGScheduler: Missing parents: List()
16/07/30 22:53:46 INFO scheduler.DAGScheduler: Submitting ResultStage 2 (ParallelCollectionRDD[2] at parallelize at PythonRDD.scala:423), which has no missing parents
16/07/30 22:53:46 INFO storage.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 1224.0 B, free 8.5 KB)
16/07/30 22:53:46 INFO storage.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 777.0 B, free 9.3 KB)
16/07/30 22:53:46 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:45367 (size: 777.0 B, free: 517.4 MB)
16/07/30 22:53:46 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1006
16/07/30 22:53:46 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (ParallelCollectionRDD[2] at parallelize at PythonRDD.scala:423)
16/07/30 22:53:46 INFO scheduler.TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
16/07/30 22:53:46 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 2102 bytes)
16/07/30 22:53:46 INFO executor.Executor: Running task 0.0 in stage 2.0 (TID 2)
16/07/30 22:53:46 INFO executor.Executor: Finished task 0.0 in stage 2.0 (TID 2). 950 bytes result sent to driver
16/07/30 22:53:46 INFO scheduler.DAGScheduler: ResultStage 2 (collect at <stdin>:1) finished in 0.025 s
16/07/30 22:53:46 INFO scheduler.DAGScheduler: Job 2 finished: collect at <stdin>:1, took 0.045257 s
16/07/30 22:53:46 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 24 ms on localhost (1/1)
16/07/30 22:53:46 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
>>> print(y.collect())
16/07/30 22:53:51 INFO spark.SparkContext: Starting job: collect at <stdin>:1
16/07/30 22:53:51 INFO scheduler.DAGScheduler: Got job 3 (collect at <stdin>:1) with 1 output partitions
16/07/30 22:53:51 INFO scheduler.DAGScheduler: Final stage: ResultStage 3 (collect at <stdin>:1)
16/07/30 22:53:51 INFO scheduler.DAGScheduler: Parents of final stage: List()
16/07/30 22:53:51 INFO scheduler.DAGScheduler: Missing parents: List()
16/07/30 22:53:51 INFO scheduler.DAGScheduler: Submitting ResultStage 3 (PythonRDD[3] at collect at <stdin>:1), which has no missing parents
16/07/30 22:53:51 INFO storage.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 3.4 KB, free 12.6 KB)
16/07/30 22:53:51 INFO storage.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.2 KB, free 14.9 KB)
16/07/30 22:53:51 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:45367 (size: 2.2 KB, free: 517.4 MB)
16/07/30 22:53:51 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1006
16/07/30 22:53:51 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (PythonRDD[3] at collect at <stdin>:1)
16/07/30 22:53:51 INFO scheduler.TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
16/07/30 22:53:51 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, localhost, partition 0,PROCESS_LOCAL, 2102 bytes)
16/07/30 22:53:51 INFO executor.Executor: Running task 0.0 in stage 3.0 (TID 3)
16/07/30 22:53:51 INFO python.PythonRunner: Times: total = 6, boot = 3, init = 3, finish = 0
16/07/30 22:53:51 INFO executor.Executor: Finished task 0.0 in stage 3.0 (TID 3). 1039 bytes result sent to driver
16/07/30 22:53:51 INFO scheduler.DAGScheduler: ResultStage 3 (collect at <stdin>:1) finished in 0.023 s
16/07/30 22:53:51 INFO scheduler.DAGScheduler: Job 3 finished: collect at <stdin>:1, took 0.038936 s
16/07/30 22:53:51 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 24 ms on localhost (1/1)
16/07/30 22:53:51 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
[1, 3, 5, 7, 9]
====================================================================================================
scala> val x = sc.parallelize(Array(("a", 1),("b", 2)))
x: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[4] at parallelize at <console>:27

scala> val y = sc.parallelize(Array(("a", 3), ("a", 4), ("b", 5)))
y: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[5] at parallelize at <console>:27

scala> val z = x.join(y)
z: org.apache.spark.rdd.RDD[(String, (Int, Int))] = MapPartitionsRDD[8] at join at <console>:31

scala> z.collect()
16/07/30 22:57:43 INFO spark.SparkContext: Starting job: collect at <console>:34
16/07/30 22:57:43 INFO scheduler.DAGScheduler: Registering RDD 4 (parallelize at <console>:27)
16/07/30 22:57:43 INFO scheduler.DAGScheduler: Registering RDD 5 (parallelize at <console>:27)
16/07/30 22:57:43 INFO scheduler.DAGScheduler: Got job 4 (collect at <console>:34) with 1 output partitions
16/07/30 22:57:43 INFO scheduler.DAGScheduler: Final stage: ResultStage 6 (collect at <console>:34)
16/07/30 22:57:43 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 5, ShuffleMapStage 4)
16/07/30 22:57:43 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 5, ShuffleMapStage 4)
16/07/30 22:57:43 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 4 (ParallelCollectionRDD[4] at parallelize at <console>:27), which has no missing parents
16/07/30 22:57:43 INFO storage.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 1808.0 B, free 6.9 KB)
16/07/30 22:57:43 INFO storage.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 1152.0 B, free 8.0 KB)
16/07/30 22:57:43 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on localhost:41369 (size: 1152.0 B, free: 517.4 MB)
16/07/30 22:57:43 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1006
16/07/30 22:57:43 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 4 (ParallelCollectionRDD[4] at parallelize at <console>:27)
16/07/30 22:57:43 INFO scheduler.TaskSchedulerImpl: Adding task set 4.0 with 1 tasks
16/07/30 22:57:43 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4, localhost, partition 0,PROCESS_LOCAL, 2188 bytes)
16/07/30 22:57:43 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 5 (ParallelCollectionRDD[5] at parallelize at <console>:27), which has no missing parents
16/07/30 22:57:43 INFO storage.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 1808.0 B, free 9.8 KB)
16/07/30 22:57:43 INFO executor.Executor: Running task 0.0 in stage 4.0 (TID 4)
16/07/30 22:57:43 INFO storage.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 1149.0 B, free 10.9 KB)
16/07/30 22:57:43 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on localhost:41369 (size: 1149.0 B, free: 517.4 MB)
16/07/30 22:57:43 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1006
16/07/30 22:57:43 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 5 (ParallelCollectionRDD[5] at parallelize at <console>:27)
16/07/30 22:57:43 INFO scheduler.TaskSchedulerImpl: Adding task set 5.0 with 1 tasks
16/07/30 22:57:43 INFO executor.Executor: Finished task 0.0 in stage 4.0 (TID 4). 1158 bytes result sent to driver
16/07/30 22:57:43 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5, localhost, partition 0,PROCESS_LOCAL, 2209 bytes)
16/07/30 22:57:43 INFO executor.Executor: Running task 0.0 in stage 5.0 (TID 5)
16/07/30 22:57:43 INFO scheduler.DAGScheduler: ShuffleMapStage 4 (parallelize at <console>:27) finished in 0.273 s
16/07/30 22:57:43 INFO scheduler.DAGScheduler: looking for newly runnable stages
16/07/30 22:57:43 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 275 ms on localhost (1/1)
16/07/30 22:57:43 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
16/07/30 22:57:43 INFO executor.Executor: Finished task 0.0 in stage 5.0 (TID 5). 1158 bytes result sent to driver
16/07/30 22:57:43 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 49 ms on localhost (1/1)
16/07/30 22:57:43 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
16/07/30 22:57:43 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 5)
16/07/30 22:57:43 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 6)
16/07/30 22:57:43 INFO scheduler.DAGScheduler: failed: Set()
16/07/30 22:57:43 INFO scheduler.DAGScheduler: ShuffleMapStage 5 (parallelize at <console>:27) finished in 0.313 s
16/07/30 22:57:43 INFO scheduler.DAGScheduler: looking for newly runnable stages
16/07/30 22:57:43 INFO scheduler.DAGScheduler: running: Set()
16/07/30 22:57:43 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 6)
16/07/30 22:57:43 INFO scheduler.DAGScheduler: failed: Set()
16/07/30 22:57:43 INFO scheduler.DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[8] at join at <console>:31), which has no missing parents
16/07/30 22:57:43 INFO storage.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 3.1 KB, free 14.0 KB)
16/07/30 22:57:43 INFO storage.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 1765.0 B, free 15.8 KB)
16/07/30 22:57:43 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:41369 (size: 1765.0 B, free: 517.4 MB)
16/07/30 22:57:43 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1006
16/07/30 22:57:43 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[8] at join at <console>:31)
16/07/30 22:57:43 INFO scheduler.TaskSchedulerImpl: Adding task set 6.0 with 1 tasks
16/07/30 22:57:43 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6, localhost, partition 0,PROCESS_LOCAL, 1967 bytes)
16/07/30 22:57:43 INFO executor.Executor: Running task 0.0 in stage 6.0 (TID 6)
16/07/30 22:57:43 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
16/07/30 22:57:43 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 36 ms
16/07/30 22:57:43 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
16/07/30 22:57:43 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
16/07/30 22:57:43 INFO executor.Executor: Finished task 0.0 in stage 6.0 (TID 6). 1394 bytes result sent to driver
16/07/30 22:57:43 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 228 ms on localhost (1/1)
16/07/30 22:57:43 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
16/07/30 22:57:43 INFO scheduler.DAGScheduler: ResultStage 6 (collect at <console>:34) finished in 0.234 s
16/07/30 22:57:43 INFO scheduler.DAGScheduler: Job 4 finished: collect at <console>:34, took 0.671136 s
res4: Array[(String, (Int, Int))] = Array((a,(1,3)), (a,(1,4)), (b,(2,5)))

>>> x = sc.parallelize([("a", 1), ("b", 2)])
>>> y = sc.parallelize([("a", 3), ("a", 4), ("b", 5)])
>>> z = x.join(y)
>>> print(z.collect())
16/07/30 22:58:56 INFO spark.SparkContext: Starting job: collect at <stdin>:1
16/07/30 22:58:56 INFO scheduler.DAGScheduler: Registering RDD 10 (join at <stdin>:1)
16/07/30 22:58:56 INFO scheduler.DAGScheduler: Got job 4 (collect at <stdin>:1) with 2 output partitions
16/07/30 22:58:56 INFO scheduler.DAGScheduler: Final stage: ResultStage 5 (collect at <stdin>:1)
16/07/30 22:58:56 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)
16/07/30 22:58:56 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 4)
16/07/30 22:58:56 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 4 (PairwiseRDD[10] at join at <stdin>:1), which has no missing parents
16/07/30 22:58:56 INFO storage.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 8.5 KB, free 23.4 KB)
16/07/30 22:58:56 INFO storage.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 4.9 KB, free 28.3 KB)
16/07/30 22:58:56 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on localhost:45367 (size: 4.9 KB, free: 517.4 MB)
16/07/30 22:58:56 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1006
16/07/30 22:58:56 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 4 (PairwiseRDD[10] at join at <stdin>:1)
16/07/30 22:58:56 INFO scheduler.TaskSchedulerImpl: Adding task set 4.0 with 2 tasks
16/07/30 22:58:56 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4, localhost, partition 0,PROCESS_LOCAL, 2196 bytes)
16/07/30 22:58:56 INFO executor.Executor: Running task 0.0 in stage 4.0 (TID 4)
16/07/30 22:58:56 INFO python.PythonRunner: Times: total = 6, boot = 3, init = 3, finish = 0
16/07/30 22:58:56 INFO python.PythonRunner: Times: total = 90, boot = 7, init = 7, finish = 76
16/07/30 22:58:56 INFO executor.Executor: Finished task 0.0 in stage 4.0 (TID 4). 1223 bytes result sent to driver
16/07/30 22:58:56 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 4.0 (TID 5, localhost, partition 1,PROCESS_LOCAL, 2204 bytes)
16/07/30 22:58:56 INFO executor.Executor: Running task 1.0 in stage 4.0 (TID 5)
16/07/30 22:58:56 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 270 ms on localhost (1/2)
16/07/30 22:58:56 INFO python.PythonRunner: Times: total = 56, boot = -204, init = 260, finish = 0
16/07/30 22:58:56 INFO python.PythonRunner: Times: total = 11, boot = -164, init = 174, finish = 1
16/07/30 22:58:56 INFO executor.Executor: Finished task 1.0 in stage 4.0 (TID 5). 1223 bytes result sent to driver
16/07/30 22:58:56 INFO scheduler.DAGScheduler: ShuffleMapStage 4 (join at <stdin>:1) finished in 0.382 s
16/07/30 22:58:56 INFO scheduler.DAGScheduler: looking for newly runnable stages
16/07/30 22:58:56 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 4.0 (TID 5) in 128 ms on localhost (2/2)
16/07/30 22:58:56 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
16/07/30 22:58:56 INFO scheduler.DAGScheduler: running: Set()
16/07/30 22:58:56 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 5)
16/07/30 22:58:56 INFO scheduler.DAGScheduler: failed: Set()
16/07/30 22:58:56 INFO scheduler.DAGScheduler: Submitting ResultStage 5 (PythonRDD[13] at collect at <stdin>:1), which has no missing parents
16/07/30 22:58:56 INFO storage.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 6.7 KB, free 35.0 KB)
16/07/30 22:58:56 INFO storage.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.2 KB, free 39.3 KB)
16/07/30 22:58:56 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on localhost:45367 (size: 4.2 KB, free: 517.4 MB)
16/07/30 22:58:56 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1006
16/07/30 22:58:56 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 5 (PythonRDD[13] at collect at <stdin>:1)
16/07/30 22:58:56 INFO scheduler.TaskSchedulerImpl: Adding task set 5.0 with 2 tasks
16/07/30 22:58:57 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 6, localhost, partition 0,NODE_LOCAL, 1894 bytes)
16/07/30 22:58:57 INFO executor.Executor: Running task 0.0 in stage 5.0 (TID 6)
16/07/30 22:58:57 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
16/07/30 22:58:57 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
16/07/30 22:58:57 INFO python.PythonRunner: Times: total = 186, boot = -285, init = 324, finish = 147
16/07/30 22:58:57 INFO executor.Executor: Finished task 0.0 in stage 5.0 (TID 6). 1284 bytes result sent to driver
16/07/30 22:58:57 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 5.0 (TID 7, localhost, partition 1,NODE_LOCAL, 1894 bytes)
16/07/30 22:58:57 INFO executor.Executor: Running task 1.0 in stage 5.0 (TID 7)
16/07/30 22:58:57 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 6) in 280 ms on localhost (1/2)
16/07/30 22:58:57 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
16/07/30 22:58:57 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
16/07/30 22:58:57 INFO python.PythonRunner: Times: total = 47, boot = -534, init = 580, finish = 1
16/07/30 22:58:57 INFO executor.Executor: Finished task 1.0 in stage 5.0 (TID 7). 1256 bytes result sent to driver
16/07/30 22:58:57 INFO scheduler.DAGScheduler: ResultStage 5 (collect at <stdin>:1) finished in 0.365 s
16/07/30 22:58:57 INFO scheduler.DAGScheduler: Job 4 finished: collect at <stdin>:1, took 1.029255 s
16/07/30 22:58:57 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 5.0 (TID 7) in 103 ms on localhost (2/2)
16/07/30 22:58:57 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
[('a', (1, 3)), ('a', (1, 4)), ('b', (2, 5))]
====================================================================================================
====================================================================================================
