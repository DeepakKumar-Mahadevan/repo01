mysql> create table record_hist (rec_num int(4) not null primary key, record_text1 varchar(10) default ' ');
Query OK, 0 rows affected (0.07 sec)

mysql> desc record_hist;
+--------------+-------------+------+-----+---------+-------+
| Field        | Type        | Null | Key | Default | Extra |
+--------------+-------------+------+-----+---------+-------+
| rec_num      | int(4)      | NO   | PRI | NULL    |       |
| record_text1 | varchar(10) | YES  |     |         |       |
+--------------+-------------+------+-----+---------+-------+
2 rows in set (0.00 sec)

mysql> insert into record_hist (1,'rec1');
ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '1,'rec1')' at line 1
mysql> insert into record_hist values (1,'rec1');
Query OK, 1 row affected (0.00 sec)

mysql> insert into record_hist values (2,'rec2');
Query OK, 1 row affected (0.00 sec)

mysql> insert into record_hist values (3,null);
Query OK, 1 row affected (0.00 sec)

mysql> select * from record_hist;
+---------+--------------+
| rec_num | record_text1 |
+---------+--------------+
|       1 | rec1         |
|       2 | rec2         |
|       3 | NULL         |
+---------+--------------+
3 rows in set (0.00 sec)

mysql> alter table record_hist change record_text1 rec_text1 varchar(10);
Query OK, 0 rows affected (0.01 sec)
Records: 0  Duplicates: 0  Warnings: 0

mysql> select * from record_hist;
+---------+-----------+
| rec_num | rec_text1 |
+---------+-----------+
|       1 | rec1      |
|       2 | rec2      |
|       3 | NULL      |
+---------+-----------+
3 rows in set (0.00 sec)
====================================================================================================
[hduser@Inceptez ~]$ sqoop import --connect jdbc:mysql://localhost/DKMDB01 --username root --password root --direct --table record_hist --m 1 --target-dir sqoopdb/DKMDB01/record_hist;

16/06/28 21:26:31 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
16/06/28 21:26:32 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `record_hist` AS t LIMIT 1
16/06/28 21:26:32 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `record_hist` AS t LIMIT 1
16/06/28 21:26:41 INFO mapreduce.JobSubmitter: number of splits:1
16/06/28 21:26:42 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1467128205694_0001
16/06/28 21:26:43 INFO impl.YarnClientImpl: Submitted application application_1467128205694_0001
16/06/28 21:26:43 INFO mapreduce.Job: The url to track the job: http://Inceptez:8088/proxy/application_1467128205694_0001/
16/06/28 21:26:43 INFO mapreduce.Job: Running job: job_1467128205694_0001
16/06/28 21:26:56 INFO mapreduce.Job: Job job_1467128205694_0001 running in uber mode : false
16/06/28 21:26:56 INFO mapreduce.Job:  map 0% reduce 0%
16/06/28 21:27:05 INFO mapreduce.Job:  map 100% reduce 0%
16/06/28 21:27:06 INFO mapreduce.Job: Job job_1467128205694_0001 completed successfully
16/06/28 21:27:07 INFO mapreduce.ImportJobBase: Transferred 21 bytes in 28.2198 seconds (0.7442 bytes/sec)
16/06/28 21:27:07 INFO mapreduce.ImportJobBase: Retrieved 3 records.

[hduser@Inceptez ~]$ hadoop fs -ls -R sqoopdb/DKMDB01/
drwxr-xr-x   - hduser supergroup          0 2016-06-28 21:27 sqoopdb/DKMDB01/record_hist
-rw-r--r--   1 hduser supergroup          0 2016-06-28 21:27 sqoopdb/DKMDB01/record_hist/_SUCCESS
-rw-r--r--   1 hduser supergroup         21 2016-06-28 21:27 sqoopdb/DKMDB01/record_hist/part-m-00000

[hduser@Inceptez ~]$ hadoop fs -cat sqoopdb/DKMDB01/record_hist/p*
1,rec1
2,rec2
3,NULL

====================================================================================================
mysql> alter table record_hist add rec_text2 varchar(7) default ' ';
Query OK, 3 rows affected (0.09 sec)
Records: 3  Duplicates: 0  Warnings: 0

mysql> select * from record_hist;
+---------+-----------+-----------+
| rec_num | rec_text1 | rec_text2 |
+---------+-----------+-----------+
|       1 | rec1      |           |
|       2 | rec2      |           |
|       3 | NULL      |           |
+---------+-----------+-----------+
3 rows in set (0.00 sec)

mysql> update record_hist set rec_text2 = 'r1t2' where rec_num = 1;
Query OK, 1 row affected (0.00 sec)
Rows matched: 1  Changed: 1  Warnings: 0

mysql> update record_hist set rec_text2 = null where rec_num = 2;
Query OK, 1 row affected (0.00 sec)
Rows matched: 1  Changed: 1  Warnings: 0

mysql> update record_hist set rec_text2 = 'r3t2' where rec_num = 3;
Query OK, 1 row affected (0.00 sec)
Rows matched: 1  Changed: 1  Warnings: 0

mysql> insert into record_hist values (4,'rec4','r4t2');
Query OK, 1 row affected (0.00 sec)

mysql> select * from record_hist;
+---------+-----------+-----------+
| rec_num | rec_text1 | rec_text2 |
+---------+-----------+-----------+
|       1 | rec1      | r1t2      |
|       2 | rec2      | NULL      |
|       3 | NULL      | r3t2      |
|       4 | rec4      | r4t2      |
+---------+-----------+-----------+
4 rows in set (0.00 sec)
====================================================================================================
[hduser@Inceptez ~]$ sqoop import --connect jdbc:mysql://localhost/DKMDB01 --username root --password root --direct --table record_hist --m 1 --target-dir sqoopdb/DKMDB01/record_hist --incremental append --check-column rec_num --last-value 3;

16/06/28 21:42:53 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `record_hist` AS t LIMIT 1
16/06/28 21:42:53 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `record_hist` AS t LIMIT 1
..
16/06/28 21:42:57 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(`rec_num`) FROM record_hist
16/06/28 21:42:57 INFO tool.ImportTool: Incremental import based on column `rec_num`
16/06/28 21:42:57 INFO tool.ImportTool: Lower bound value: 3
16/06/28 21:42:57 INFO tool.ImportTool: Upper bound value: 4
..
16/06/28 21:42:58 INFO mapreduce.JobSubmitter: number of splits:1
16/06/28 21:42:59 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1467128205694_0002
16/06/28 21:42:59 INFO impl.YarnClientImpl: Submitted application application_1467128205694_0002
16/06/28 21:42:59 INFO mapreduce.Job: The url to track the job: http://Inceptez:8088/proxy/application_1467128205694_0002/
16/06/28 21:42:59 INFO mapreduce.Job: Running job: job_1467128205694_0002
16/06/28 21:43:08 INFO mapreduce.Job: Job job_1467128205694_0002 running in uber mode : false
16/06/28 21:43:08 INFO mapreduce.Job:  map 0% reduce 0%
16/06/28 21:43:17 INFO mapreduce.Job:  map 100% reduce 0%
16/06/28 21:43:17 INFO mapreduce.Job: Job job_1467128205694_0002 completed successfully
16/06/28 21:43:17 INFO mapreduce.Job: Counters: 30
16/06/28 21:43:17 INFO mapreduce.ImportJobBase: Transferred 12 bytes in 20.0593 seconds (0.5982 bytes/sec)
16/06/28 21:43:17 INFO mapreduce.ImportJobBase: Retrieved 1 records.
16/06/28 21:43:17 INFO util.AppendUtils: Appending to directory record_hist
16/06/28 21:43:17 INFO util.AppendUtils: Using found partition 1
16/06/28 21:43:17 INFO tool.ImportTool: Incremental import complete! To run another incremental import of all data following this import, supply the following arguments:
16/06/28 21:43:17 INFO tool.ImportTool:  --incremental append
16/06/28 21:43:17 INFO tool.ImportTool:   --check-column rec_num
16/06/28 21:43:17 INFO tool.ImportTool:   --last-value 4
16/06/28 21:43:17 INFO tool.ImportTool: (Consider saving this with 'sqoop job --create')
[hduser@Inceptez ~]$ hadoop fs -ls sqoopdb/DKMDB01/record_hist/
Found 3 items
-rw-r--r--   1 hduser supergroup          0 2016-06-28 21:27 sqoopdb/DKMDB01/record_hist/_SUCCESS
-rw-r--r--   1 hduser supergroup         21 2016-06-28 21:27 sqoopdb/DKMDB01/record_hist/part-m-00000
-rw-r--r--   1 hduser supergroup         12 2016-06-28 21:43 sqoopdb/DKMDB01/record_hist/part-m-00001
[hduser@Inceptez ~]$ hadoop fs -cat sqoopdb/DKMDB01/record_hist/p*0
1,rec1
2,rec2
3,NULL
[hduser@Inceptez ~]$ hadoop fs -cat sqoopdb/DKMDB01/record_hist/p*1
4,rec4,r4t2
====================================================================================================
mysql> create table rec_hist_clone1 like record_hist;
Query OK, 0 rows affected (0.01 sec)

mysql> desc rec_hist_clone1;
+-----------+-------------+------+-----+---------+-------+
| Field     | Type        | Null | Key | Default | Extra |
+-----------+-------------+------+-----+---------+-------+
| rec_num   | int(4)      | NO   | PRI | NULL    |       |
| rec_text1 | varchar(10) | YES  |     | NULL    |       |
| rec_text2 | varchar(7)  | YES  |     |         |       |
+-----------+-------------+------+-----+---------+-------+
3 rows in set (0.00 sec)
====================================================================================================
[hduser@Inceptez ~]$ sqoop export --connect jdbc:mysql://localhost/DKMDB01 --username root --password root --direct --table rec_hist_clone1 --export-dir sqoopdb/DKMDB01/record_hist;

16/06/28 21:53:00 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `rec_hist_clone1` AS t LIMIT 1
16/06/28 21:53:00 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `rec_hist_clone1` AS t LIMIT 1
16/06/28 21:53:06 INFO input.FileInputFormat: Total input paths to process : 2
16/06/28 21:53:06 INFO input.FileInputFormat: Total input paths to process : 2
16/06/28 21:53:06 INFO mapreduce.JobSubmitter: number of splits:3
16/06/28 21:53:06 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1467128205694_0003
16/06/28 21:53:07 INFO impl.YarnClientImpl: Submitted application application_1467128205694_0003
16/06/28 21:53:07 INFO mapreduce.Job: The url to track the job: http://Inceptez:8088/proxy/application_1467128205694_0003/
16/06/28 21:53:07 INFO mapreduce.Job: Running job: job_1467128205694_0003
16/06/28 21:53:15 INFO mapreduce.Job: Job job_1467128205694_0003 running in uber mode : false
16/06/28 21:53:15 INFO mapreduce.Job:  map 0% reduce 0%
16/06/28 21:53:35 INFO mapreduce.Job:  map 100% reduce 0%
16/06/28 21:53:35 INFO mapreduce.Job: Job job_1467128205694_0003 completed successfully
16/06/28 21:53:35 INFO mapreduce.Job: Counters: 30
16/06/28 21:53:35 INFO mapreduce.ExportJobBase: Transferred 726 bytes in 30.7424 seconds (23.6156 bytes/sec)
16/06/28 21:53:35 INFO mapreduce.ExportJobBase: Exported 4 records.

mysql> select * from rec_hist_clone1;
+---------+-----------+-----------+
| rec_num | rec_text1 | rec_text2 |
+---------+-----------+-----------+
|       4 | rec4      | r4t2      |
|       3 | NULL      |           |
|       1 | rec1      |           |
|       2 | rec2      |           |
+---------+-----------+-----------+
4 rows in set (0.00 sec)
====================================================================================================
mysql> update record_hist set rec_text1 = 'rec3' where rec_num = 3;
Query OK, 1 row affected (0.00 sec)
Rows matched: 1  Changed: 1  Warnings: 0

mysql> update record_hist set rec_text2 = 'r2t2' where rec_num = 2;
Query OK, 1 row affected (0.00 sec)
Rows matched: 1  Changed: 1  Warnings: 0

mysql> select * from record_hist;
+---------+-----------+-----------+
| rec_num | rec_text1 | rec_text2 |
+---------+-----------+-----------+
|       1 | rec1      | r1t2      |
|       2 | rec2      | r2t2      |
|       3 | rec3      | r3t2      |
|       4 | rec4      | r4t2      |
+---------+-----------+-----------+
4 rows in set (0.00 sec)

[hduser@Inceptez ~]$ sqoop import --connect jdbc:mysql://localhost/DKMDB01 --username root --password root --direct --table record_hist --m 1 --target-dir sqoopdb/DKMDB01/record_hist_new;

16/06/28 22:08:59 INFO mapreduce.JobSubmitter: number of splits:1
16/06/28 22:08:59 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1467128205694_0007
16/06/28 22:09:00 INFO impl.YarnClientImpl: Submitted application application_1467128205694_0007
16/06/28 22:09:00 INFO mapreduce.Job: The url to track the job: http://Inceptez:8088/proxy/application_1467128205694_0007/
16/06/28 22:09:00 INFO mapreduce.Job: Running job: job_1467128205694_0007
16/06/28 22:09:09 INFO mapreduce.Job: Job job_1467128205694_0007 running in uber mode : false
16/06/28 22:09:09 INFO mapreduce.Job:  map 0% reduce 0%
16/06/28 22:09:16 INFO mapreduce.Job:  map 100% reduce 0%
16/06/28 22:09:16 INFO mapreduce.Job: Job job_1467128205694_0007 completed successfully
16/06/28 22:09:16 INFO mapreduce.Job: Counters: 30
16/06/28 22:09:16 INFO mapreduce.ImportJobBase: Transferred 48 bytes in 19.1491 seconds (2.5066 bytes/sec)
16/06/28 22:09:16 INFO mapreduce.ImportJobBase: Retrieved 4 records.

[hduser@Inceptez ~]$ hadoop fs -ls sqoopdb/DKMDB01/record_hist_new
-rw-r--r--   1 hduser supergroup          0 2016-06-28 22:09 sqoopdb/DKMDB01/record_hist_new/_SUCCESS
-rw-r--r--   1 hduser supergroup         48 2016-06-28 22:09 sqoopdb/DKMDB01/record_hist_new/part-m-00000
[hduser@Inceptez ~]$ hadoop fs -cat sqoopdb/DKMDB01/record_hist_new/p*
1,rec1,r1t2
2,rec2,r2t2
3,rec3,r3t2
4,rec4,r4t2
====================================================================================================
[hduser@Inceptez ~]$ sqoop import --connect jdbc:mysql://localhost/DKMDB01 --username root --password root --direct --table record_hist --m 1 --target-dir sqoopdb/DKMDB01/record_hist;
Warning: /usr/local/sqoop/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.
16/06/28 22:47:06 INFO sqoop.Sqoop: Running Sqoop version: 1.4.5
16/06/28 22:47:06 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
16/06/28 22:47:07 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
16/06/28 22:47:07 INFO tool.CodeGenTool: Beginning code generation
16/06/28 22:47:08 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `record_hist` AS t LIMIT 1
16/06/28 22:47:08 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `record_hist` AS t LIMIT 1
16/06/28 22:47:08 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop
Note: /tmp/sqoop-hduser/compile/aab1e776b4f56d1af2dd993f3a9e1522/record_hist.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
16/06/28 22:47:14 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hduser/compile/aab1e776b4f56d1af2dd993f3a9e1522/record_hist.jar
16/06/28 22:47:14 INFO manager.DirectMySQLManager: Beginning mysqldump fast path import
16/06/28 22:47:14 INFO mapreduce.ImportJobBase: Beginning import of record_hist
16/06/28 22:47:15 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/06/28 22:47:15 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
16/06/28 22:47:18 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
16/06/28 22:47:18 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
16/06/28 22:47:22 INFO db.DBInputFormat: Using read commited transaction isolation
16/06/28 22:47:22 INFO mapreduce.JobSubmitter: number of splits:1
16/06/28 22:47:23 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1467128205694_0010
16/06/28 22:47:24 INFO impl.YarnClientImpl: Submitted application application_1467128205694_0010
16/06/28 22:47:24 INFO mapreduce.Job: The url to track the job: http://Inceptez:8088/proxy/application_1467128205694_0010/
16/06/28 22:47:24 INFO mapreduce.Job: Running job: job_1467128205694_0010
16/06/28 22:47:45 INFO mapreduce.Job: Job job_1467128205694_0010 running in uber mode : false
16/06/28 22:47:45 INFO mapreduce.Job:  map 0% reduce 0%
16/06/28 22:48:03 INFO mapreduce.Job:  map 100% reduce 0%
16/06/28 22:48:04 INFO mapreduce.Job: Job job_1467128205694_0010 completed successfully
16/06/28 22:48:05 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=114941
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=21
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=14781
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=14781
		Total vcore-seconds taken by all map tasks=14781
		Total megabyte-seconds taken by all map tasks=15135744
	Map-Reduce Framework
		Map input records=1
		Map output records=3
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=233
		CPU time spent (ms)=2380
		Physical memory (bytes) snapshot=106160128
		Virtual memory (bytes) snapshot=976515072
		Total committed heap usage (bytes)=23855104
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=21
16/06/28 22:48:05 INFO mapreduce.ImportJobBase: Transferred 21 bytes in 46.7485 seconds (0.4492 bytes/sec)
16/06/28 22:48:05 INFO mapreduce.ImportJobBase: Retrieved 3 records.
[hduser@Inceptez ~]$ hadoop fs -ls sqoopdb/DKMDB01/record_hist
16/06/28 22:48:59 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Found 2 items
-rw-r--r--   1 hduser supergroup          0 2016-06-28 22:48 sqoopdb/DKMDB01/record_hist/_SUCCESS
-rw-r--r--   1 hduser supergroup         21 2016-06-28 22:48 sqoopdb/DKMDB01/record_hist/part-m-00000
[hduser@Inceptez ~]$ hadoop fs -cat sqoopdb/DKMDB01/record_hist/p*
16/06/28 22:49:15 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
1,rec1
2,rec2
3,NULL

[hduser@Inceptez ~]$ sqoop import --connect jdbc:mysql://localhost/DKMDB01 --username root --password root --direct --table record_hist --m 1 --target-dir sqoopdb/DKMDB01/record_hist_new;
Warning: /usr/local/sqoop/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.
16/06/28 22:51:34 INFO sqoop.Sqoop: Running Sqoop version: 1.4.5
16/06/28 22:51:34 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
16/06/28 22:51:35 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
16/06/28 22:51:35 INFO tool.CodeGenTool: Beginning code generation
16/06/28 22:51:35 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `record_hist` AS t LIMIT 1
16/06/28 22:51:36 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `record_hist` AS t LIMIT 1
16/06/28 22:51:36 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop
Note: /tmp/sqoop-hduser/compile/f7c51467655575474a7472bf82d5b200/record_hist.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
16/06/28 22:51:41 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hduser/compile/f7c51467655575474a7472bf82d5b200/record_hist.jar
16/06/28 22:51:41 INFO manager.DirectMySQLManager: Beginning mysqldump fast path import
16/06/28 22:51:41 INFO mapreduce.ImportJobBase: Beginning import of record_hist
16/06/28 22:51:42 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/06/28 22:51:42 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
16/06/28 22:51:44 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
16/06/28 22:51:45 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
16/06/28 22:51:48 INFO db.DBInputFormat: Using read commited transaction isolation
16/06/28 22:51:48 INFO mapreduce.JobSubmitter: number of splits:1
16/06/28 22:51:48 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1467128205694_0011
16/06/28 22:51:49 INFO impl.YarnClientImpl: Submitted application application_1467128205694_0011
16/06/28 22:51:49 INFO mapreduce.Job: The url to track the job: http://Inceptez:8088/proxy/application_1467128205694_0011/
16/06/28 22:51:49 INFO mapreduce.Job: Running job: job_1467128205694_0011
16/06/28 22:52:12 INFO mapreduce.Job: Job job_1467128205694_0011 running in uber mode : false
16/06/28 22:52:12 INFO mapreduce.Job:  map 0% reduce 0%
16/06/28 22:52:31 INFO mapreduce.Job:  map 100% reduce 0%
16/06/28 22:52:31 INFO mapreduce.Job: Job job_1467128205694_0011 completed successfully
16/06/28 22:52:31 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=114957
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=48
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=14520
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=14520
		Total vcore-seconds taken by all map tasks=14520
		Total megabyte-seconds taken by all map tasks=14868480
	Map-Reduce Framework
		Map input records=1
		Map output records=4
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=241
		CPU time spent (ms)=2380
		Physical memory (bytes) snapshot=105107456
		Virtual memory (bytes) snapshot=976515072
		Total committed heap usage (bytes)=23855104
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=48
16/06/28 22:52:31 INFO mapreduce.ImportJobBase: Transferred 48 bytes in 46.8707 seconds (1.0241 bytes/sec)
16/06/28 22:52:31 INFO mapreduce.ImportJobBase: Retrieved 4 records.

[hduser@Inceptez ~]$ hadoop fs -ls sqoopdb/DKMDB01/record_hist_new
16/06/28 23:01:11 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Found 2 items
-rw-r--r--   1 hduser supergroup          0 2016-06-28 22:52 sqoopdb/DKMDB01/record_hist_new/_SUCCESS
-rw-r--r--   1 hduser supergroup         48 2016-06-28 22:52 sqoopdb/DKMDB01/record_hist_new/part-m-00000
[hduser@Inceptez ~]$ hadoop fs -cat sqoopdb/DKMDB01/record_hist_new/p*
16/06/28 23:01:28 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
1,rec1,r1t2
2,rec2,NULL
3,NULL,r3t2
4,rec4,r4t2

====================================================================================================
[hduser@Inceptez ~]$ sqoop merge --new-data sqoopdb/DKMDB01/record_hist_new  --onto sqoopdb/DKMDB01/record_hist --target-dir sqoopdb/DKMDB01/record_hist_merged --merge-key rec_num --class-name record_hist --jar-file /tmp/sqoop-hduser/compile/f7c51467655575474a7472bf82d5b200/record_hist.jar;
Warning: /usr/local/sqoop/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.
16/06/28 22:58:22 INFO sqoop.Sqoop: Running Sqoop version: 1.4.5
16/06/28 22:58:24 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/06/28 22:58:24 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
16/06/28 22:58:27 INFO Configuration.deprecation: mapred.output.key.class is deprecated. Instead, use mapreduce.job.output.key.class
16/06/28 22:58:27 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
16/06/28 22:58:30 INFO input.FileInputFormat: Total input paths to process : 2
16/06/28 22:58:30 INFO mapreduce.JobSubmitter: number of splits:2
16/06/28 22:58:31 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1467128205694_0012
16/06/28 22:58:32 INFO impl.YarnClientImpl: Submitted application application_1467128205694_0012
16/06/28 22:58:32 INFO mapreduce.Job: The url to track the job: http://Inceptez:8088/proxy/application_1467128205694_0012/
16/06/28 22:58:32 INFO mapreduce.Job: Running job: job_1467128205694_0012
16/06/28 22:58:54 INFO mapreduce.Job: Job job_1467128205694_0012 running in uber mode : false
16/06/28 22:58:54 INFO mapreduce.Job:  map 0% reduce 0%
16/06/28 22:59:24 INFO mapreduce.Job:  map 50% reduce 0%
16/06/28 22:59:24 INFO mapreduce.Job: Task Id : attempt_1467128205694_0012_m_000001_0, Status : FAILED
Error: java.lang.RuntimeException: Can't parse input data: 'rec1'
	at record_hist.__loadFromFields(record_hist.java:292)
	at record_hist.parse(record_hist.java:230)
	at org.apache.sqoop.mapreduce.MergeTextMapper.map(MergeTextMapper.java:53)
	at org.apache.sqoop.mapreduce.MergeTextMapper.map(MergeTextMapper.java:34)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:784)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)
Caused by: java.util.NoSuchElementException
	at java.util.ArrayList$Itr.next(ArrayList.java:834)
	at record_hist.__loadFromFields(record_hist.java:287)
	... 11 more

16/06/28 23:00:00 INFO mapreduce.Job:  map 100% reduce 0%
16/06/28 23:00:00 INFO mapreduce.Job: Task Id : attempt_1467128205694_0012_m_000001_1, Status : FAILED
Error: java.lang.RuntimeException: Can't parse input data: 'rec1'
	at record_hist.__loadFromFields(record_hist.java:292)
	at record_hist.parse(record_hist.java:230)
	at org.apache.sqoop.mapreduce.MergeTextMapper.map(MergeTextMapper.java:53)
	at org.apache.sqoop.mapreduce.MergeTextMapper.map(MergeTextMapper.java:34)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:784)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)
Caused by: java.util.NoSuchElementException
	at java.util.ArrayList$Itr.next(ArrayList.java:834)
	at record_hist.__loadFromFields(record_hist.java:287)
	... 11 more

16/06/28 23:00:01 INFO mapreduce.Job:  map 50% reduce 0%
16/06/28 23:00:06 INFO mapreduce.Job: Task Id : attempt_1467128205694_0012_m_000001_2, Status : FAILED
Error: java.lang.RuntimeException: Can't parse input data: 'rec1'
	at record_hist.__loadFromFields(record_hist.java:292)
	at record_hist.parse(record_hist.java:230)
	at org.apache.sqoop.mapreduce.MergeTextMapper.map(MergeTextMapper.java:53)
	at org.apache.sqoop.mapreduce.MergeTextMapper.map(MergeTextMapper.java:34)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:784)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)
Caused by: java.util.NoSuchElementException
	at java.util.ArrayList$Itr.next(ArrayList.java:834)
	at record_hist.__loadFromFields(record_hist.java:287)
	... 11 more

16/06/28 23:00:07 INFO mapreduce.Job:  map 50% reduce 17%
16/06/28 23:00:24 INFO mapreduce.Job:  map 100% reduce 100%
16/06/28 23:00:25 INFO mapreduce.Job: Job job_1467128205694_0012 failed with state FAILED due to: Task failed task_1467128205694_0012_m_000001
Job failed as tasks failed. failedMaps:1 failedReduces:0

16/06/28 23:00:25 INFO mapreduce.Job: Counters: 40
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=113947
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=192
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=3
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Job Counters 
		Failed map tasks=4
		Killed map tasks=1
		Killed reduce tasks=1
		Launched map tasks=6
		Launched reduce tasks=1
		Other local map tasks=4
		Data-local map tasks=2
		Total time spent by all maps in occupied slots (ms)=133505
		Total time spent by all reduces in occupied slots (ms)=56285
		Total time spent by all map tasks (ms)=133505
		Total time spent by all reduce tasks (ms)=56285
		Total vcore-seconds taken by all map tasks=133505
		Total vcore-seconds taken by all reduce tasks=56285
		Total megabyte-seconds taken by all map tasks=136709120
		Total megabyte-seconds taken by all reduce tasks=57635840
	Map-Reduce Framework
		Map input records=4
		Map output records=4
		Map output bytes=128
		Map output materialized bytes=142
		Input split bytes=144
		Combine input records=0
		Spilled Records=4
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=267
		CPU time spent (ms)=1120
		Physical memory (bytes) snapshot=199041024
		Virtual memory (bytes) snapshot=972271616
		Total committed heap usage (bytes)=128716800
	File Input Format Counters 
		Bytes Read=48
16/06/28 23:00:25 ERROR tool.MergeTool: MapReduce job failed!

====================================================================================================
mysql> insert into record_hist values (5,'rec5','r5t2');
Query OK, 1 row affected (0.00 sec)

mysql> update record_hist set rec_text2 = 'r2t2' where rec_num = 2;
Query OK, 1 row affected (0.00 sec)
Rows matched: 1  Changed: 1  Warnings: 0

mysql> update record_hist set rec_text1 = 'rec3' where rec_num = 3;
Query OK, 1 row affected (0.01 sec)
Rows matched: 1  Changed: 1  Warnings: 0

mysql> select * from record_hist;
+---------+-----------+-----------+
| rec_num | rec_text1 | rec_text2 |
+---------+-----------+-----------+
|       1 | rec1      | r1t2      |
|       2 | rec2      | r2t2      |
|       3 | rec3      | r3t2      |
|       4 | rec4      | r4t2      |
|       5 | rec5      | r5t2      |
+---------+-----------+-----------+
5 rows in set (0.00 sec)
====================================================================================================
[hduser@Inceptez ~]$ sqoop import --connect jdbc:mysql://localhost/DKMDB01 --username root --password root --direct --table record_hist --m 1 --target-dir sqoopdb/DKMDB01/record_hist_new2;
Warning: /usr/local/sqoop/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.
16/06/28 23:04:07 INFO sqoop.Sqoop: Running Sqoop version: 1.4.5
16/06/28 23:04:07 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
16/06/28 23:04:08 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
16/06/28 23:04:08 INFO tool.CodeGenTool: Beginning code generation
16/06/28 23:04:09 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `record_hist` AS t LIMIT 1
16/06/28 23:04:09 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `record_hist` AS t LIMIT 1
16/06/28 23:04:09 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop
Note: /tmp/sqoop-hduser/compile/eda2e538d186201daa20b6f281f22adc/record_hist.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
16/06/28 23:04:14 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hduser/compile/eda2e538d186201daa20b6f281f22adc/record_hist.jar
16/06/28 23:04:14 INFO manager.DirectMySQLManager: Beginning mysqldump fast path import
16/06/28 23:04:14 INFO mapreduce.ImportJobBase: Beginning import of record_hist
16/06/28 23:04:15 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/06/28 23:04:15 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
16/06/28 23:04:17 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
16/06/28 23:04:18 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
16/06/28 23:04:21 INFO db.DBInputFormat: Using read commited transaction isolation
16/06/28 23:04:21 INFO mapreduce.JobSubmitter: number of splits:1
16/06/28 23:04:22 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1467128205694_0013
16/06/28 23:04:23 INFO impl.YarnClientImpl: Submitted application application_1467128205694_0013
16/06/28 23:04:23 INFO mapreduce.Job: The url to track the job: http://Inceptez:8088/proxy/application_1467128205694_0013/
16/06/28 23:04:23 INFO mapreduce.Job: Running job: job_1467128205694_0013
16/06/28 23:04:44 INFO mapreduce.Job: Job job_1467128205694_0013 running in uber mode : false
16/06/28 23:04:44 INFO mapreduce.Job:  map 0% reduce 0%
16/06/28 23:05:03 INFO mapreduce.Job:  map 100% reduce 0%
16/06/28 23:05:04 INFO mapreduce.Job: Job job_1467128205694_0013 completed successfully
16/06/28 23:05:04 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=114958
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=60
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=15678
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=15678
		Total vcore-seconds taken by all map tasks=15678
		Total megabyte-seconds taken by all map tasks=16054272
	Map-Reduce Framework
		Map input records=1
		Map output records=5
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=278
		CPU time spent (ms)=2430
		Physical memory (bytes) snapshot=99848192
		Virtual memory (bytes) snapshot=976515072
		Total committed heap usage (bytes)=23855104
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=60
16/06/28 23:05:04 INFO mapreduce.ImportJobBase: Transferred 60 bytes in 46.822 seconds (1.2814 bytes/sec)
16/06/28 23:05:04 INFO mapreduce.ImportJobBase: Retrieved 5 records.
====================================================================================================
[hduser@Inceptez ~]$ sqoop merge --new-data sqoopdb/DKMDB01/record_hist_new2  --onto sqoopdb/DKMDB01/record_hist_new --target-dir sqoopdb/DKMDB01/record_hist_merged --merge-key rec_num --class-name record_hist --jar-file /tmp/sqoop-hduser/compile/eda2e538d186201daa20b6f281f22adc/record_hist.jar;
Warning: /usr/local/sqoop/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /usr/local/sqoop/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /usr/local/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /usr/local/sqoop/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.
16/06/28 23:08:27 INFO sqoop.Sqoop: Running Sqoop version: 1.4.5
16/06/28 23:08:29 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/06/28 23:08:29 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
16/06/28 23:08:31 INFO Configuration.deprecation: mapred.output.key.class is deprecated. Instead, use mapreduce.job.output.key.class
16/06/28 23:08:32 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
16/06/28 23:08:34 INFO input.FileInputFormat: Total input paths to process : 2
16/06/28 23:08:34 INFO mapreduce.JobSubmitter: number of splits:2
16/06/28 23:08:35 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1467128205694_0014
16/06/28 23:08:36 INFO impl.YarnClientImpl: Submitted application application_1467128205694_0014
16/06/28 23:08:36 INFO mapreduce.Job: The url to track the job: http://Inceptez:8088/proxy/application_1467128205694_0014/
16/06/28 23:08:36 INFO mapreduce.Job: Running job: job_1467128205694_0014
16/06/28 23:08:55 INFO mapreduce.Job: Job job_1467128205694_0014 running in uber mode : false
16/06/28 23:08:55 INFO mapreduce.Job:  map 0% reduce 0%
16/06/28 23:09:24 INFO mapreduce.Job:  map 100% reduce 0%
16/06/28 23:09:42 INFO mapreduce.Job:  map 100% reduce 100%
16/06/28 23:09:42 INFO mapreduce.Job: Job job_1467128205694_0014 completed successfully
16/06/28 23:09:43 INFO mapreduce.Job: Counters: 49
	File System Counters
		FILE: Number of bytes read=312
		FILE: Number of bytes written=342020
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=397
		HDFS: Number of bytes written=60
		HDFS: Number of read operations=9
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=2
		Launched reduce tasks=1
		Data-local map tasks=2
		Total time spent by all maps in occupied slots (ms)=52671
		Total time spent by all reduces in occupied slots (ms)=14470
		Total time spent by all map tasks (ms)=52671
		Total time spent by all reduce tasks (ms)=14470
		Total vcore-seconds taken by all map tasks=52671
		Total vcore-seconds taken by all reduce tasks=14470
		Total megabyte-seconds taken by all map tasks=53935104
		Total megabyte-seconds taken by all reduce tasks=14817280
	Map-Reduce Framework
		Map input records=9
		Map output records=9
		Map output bytes=288
		Map output materialized bytes=318
		Input split bytes=289
		Combine input records=0
		Combine output records=0
		Reduce input groups=5
		Reduce shuffle bytes=318
		Reduce input records=9
		Reduce output records=5
		Spilled Records=18
		Shuffled Maps =2
		Failed Shuffles=0
		Merged Map outputs=2
		GC time elapsed (ms)=796
		CPU time spent (ms)=4650
		Physical memory (bytes) snapshot=505556992
		Virtual memory (bytes) snapshot=2923491328
		Total committed heap usage (bytes)=281223168
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=108
	File Output Format Counters 
		Bytes Written=60
[hduser@Inceptez ~]$ hadoop fs -ls sqoopdb/DKMDB01/*
16/06/28 23:10:13 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Found 2 items
-rw-r--r--   1 hduser supergroup          0 2016-06-28 22:48 sqoopdb/DKMDB01/record_hist/_SUCCESS
-rw-r--r--   1 hduser supergroup         21 2016-06-28 22:48 sqoopdb/DKMDB01/record_hist/part-m-00000
Found 2 items
-rw-r--r--   1 hduser supergroup          0 2016-06-28 23:09 sqoopdb/DKMDB01/record_hist_merged/_SUCCESS
-rw-r--r--   1 hduser supergroup         60 2016-06-28 23:09 sqoopdb/DKMDB01/record_hist_merged/part-r-00000
Found 2 items
-rw-r--r--   1 hduser supergroup          0 2016-06-28 22:52 sqoopdb/DKMDB01/record_hist_new/_SUCCESS
-rw-r--r--   1 hduser supergroup         48 2016-06-28 22:52 sqoopdb/DKMDB01/record_hist_new/part-m-00000
Found 2 items
-rw-r--r--   1 hduser supergroup          0 2016-06-28 23:05 sqoopdb/DKMDB01/record_hist_new2/_SUCCESS
-rw-r--r--   1 hduser supergroup         60 2016-06-28 23:05 sqoopdb/DKMDB01/record_hist_new2/part-m-00000
[hduser@Inceptez ~]$ hadoop fs -cat sqoopdb/DKMDB01/record_hist_new2/p*
16/06/28 23:11:16 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
1,rec1,r1t2
2,rec2,r2t2
3,rec3,r3t2
4,rec4,r4t2
5,rec5,r5t2
[hduser@Inceptez ~]$ hadoop fs -cat sqoopdb/DKMDB01/record_hist_merged/p*
16/06/28 23:11:35 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
1,rec1,r1t2
2,rec2,r2t2
3,rec3,r3t2
4,rec4,r4t2
5,rec5,r5t2
====================================================================================================
