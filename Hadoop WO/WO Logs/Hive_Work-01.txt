hive> create database retail;
OK
Time taken: 0.568 seconds
hive> show databases
    > ;
OK
default
retail
Time taken: 0.183 seconds, Fetched: 2 row(s)
hive> use retail;
OK
Time taken: 0.021 seconds
hive> set hive.cli.print.current.db=true;
hive (retail)> set hive.cli.print.current.db=false;
hive> set hive.cli.print.current.db=false;

hive (retail)> create table txnrecords(txnno INT, txndate STRING, custno INT, amount DOUBLE,
             > category STRING, product STRING, city STRING, state STRING, spendby STRING)
             > row format delimited
             > fields terminated by ','
             > lines terminated by '\n'
             > stored as textfile;
OK
Time taken: 0.527 seconds
hive (retail)> show tables in retail;
OK
txnrecords
Time taken: 0.055 seconds, Fetched: 1 row(s)

hive (retail)> !pwd;
/home/hduser

[hduser@Inceptez ~]$ hdfs dfs -ls -R /user/hive
16/07/02 10:55:29 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
drwxrwxr-x   - hduser supergroup          0 2016-07-02 10:47 /user/hive/warehouse
drwxr-xr-x   - hduser supergroup          0 2016-07-02 10:52 /user/hive/warehouse/retail.db
drwxr-xr-x   - hduser supergroup          0 2016-07-02 10:52 /user/hive/warehouse/retail.db/txnrecords

hive (retail)> dfs -ls;
Found 11 items
drwxr-xr-x   - hduser supergroup          0 2016-06-28 22:00 _sqoop
drwxr-xr-x   - hduser supergroup          0 2016-06-18 13:29 imp_del
drwxr-xr-x   - hduser supergroup          0 2016-06-18 22:40 import_all
drwxr-xr-x   - hduser supergroup          0 2016-06-19 00:08 inc_imp_lm
drwxr-xr-x   - hduser supergroup          0 2016-06-19 00:17 inc_imp_lm1
drwxr-xr-x   - hduser supergroup          0 2016-06-19 00:22 inc_imp_lm2
drwxr-xr-x   - hduser supergroup          0 2016-06-21 21:20 inc_imp_lm3_new
-rw-r--r--   1 hduser supergroup          4 2016-06-18 21:42 mysql_pwd.txt
drwxr-xr-x   - hduser supergroup          0 2016-06-18 10:25 practice
drwxr-xr-x   - hduser supergroup          0 2016-06-28 22:44 sqoopdb
drwxr-xr-x   - hduser supergroup          0 2016-06-05 12:20 testing
hive (retail)> dfs -ls /user/hive;
Found 1 items
drwxrwxr-x   - hduser supergroup          0 2016-07-02 10:47 /user/hive/warehouse
hive (retail)> dfs -ls -R /user/hive;
drwxrwxr-x   - hduser supergroup          0 2016-07-02 10:47 /user/hive/warehouse
drwxr-xr-x   - hduser supergroup          0 2016-07-02 10:52 /user/hive/warehouse/retail.db
drwxr-xr-x   - hduser supergroup          0 2016-07-02 10:52 /user/hive/warehouse/retail.db/txnrecords

hive (retail)> !ls hive;
data
helloword.java
hive-site_rms_transction.xml
HiveUdf.jar

hive (retail)> LOAD DATA LOCAL INPATH '/home/hduser/hive/data/txns' OVERWRITE INTO TABLE txnrecords;
Loading data to table retail.txnrecords
Table retail.txnrecords stats: [numFiles=1, numRows=0, totalSize=8472073, rawDataSize=0]
OK
Time taken: 3.119 seconds
hive (retail)> select count(*) from txnrecords;
Query ID = hduser_20160702112020_2c296705-e1cf-480d-ba5b-d06c11b1ebc4
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1467433224759_0001, Tracking URL = http://Inceptez:8088/proxy/application_1467433224759_0001/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1467433224759_0001
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2016-07-02 11:20:50,645 Stage-1 map = 0%,  reduce = 0%
2016-07-02 11:21:02,929 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.69 sec
2016-07-02 11:21:13,867 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.18 sec
MapReduce Total cumulative CPU time: 3 seconds 180 msec
Ended Job = job_1467433224759_0001
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 3.18 sec   HDFS Read: 8472295 HDFS Write: 6 SUCCESS
Total MapReduce CPU Time Spent: 3 seconds 180 msec
OK
95904
Time taken: 49.966 seconds, Fetched: 1 row(s)
hive (retail)> LOAD DATA LOCAL INPATH '/home/hduser/hive/data/txns' OVERWRITE INTO TABLE txnrecords;
Loading data to table retail.txnrecords
Table retail.txnrecords stats: [numFiles=1, numRows=0, totalSize=8472073, rawDataSize=0]
OK
Time taken: 0.656 seconds
hive (retail)> select * from txnrecords where txnno = 00095903;
OK
95903	02-07-2016	4005514	52.82	Jumping	Pogo Sticks	Scottsdale	Arizona	credit
Time taken: 0.527 seconds, Fetched: 1 row(s)

hive (retail)> set hive.cli.print.header=true;   
hive (retail)> select * from txnrecords where txnno = 00095903;
OK
txnrecords.txnno	txnrecords.txndate	txnrecords.custno	txnrecords.amount	txnrecords.category	txnrecords.product	txnrecords.city	txnrecords.state	txnrecords.spendby
95903	02-07-2016	4005514	52.82	Jumping	Pogo Sticks	Scottsdale	Arizona	credit
Time taken: 0.087 seconds, Fetched: 1 row(s)

hive (retail)> show create table txnrecords;
OK
createtab_stmt
CREATE TABLE `txnrecords`(
  `txnno` int, 
  `txndate` string, 
  `custno` int, 
  `amount` double, 
  `category` string, 
  `product` string, 
  `city` string, 
  `state` string, 
  `spendby` string)
ROW FORMAT DELIMITED 
  FIELDS TERMINATED BY ',' 
  LINES TERMINATED BY '\n' 
STORED AS INPUTFORMAT 
  'org.apache.hadoop.mapred.TextInputFormat' 
OUTPUTFORMAT 
  'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
LOCATION
  'hdfs://localhost:54310/user/hive/warehouse/retail.db/txnrecords'
TBLPROPERTIES (
  'COLUMN_STATS_ACCURATE'='true', 
  'numFiles'='1', 
  'numRows'='0', 
  'rawDataSize'='0', 
  'totalSize'='8472073', 
  'transient_lastDdlTime'='1467439247')
Time taken: 0.267 seconds, Fetched: 26 row(s)

hive (retail)> select count(1) from txnrecords;                
Query ID = hduser_20160702113737_aa9ebf7f-a226-483c-bcad-d9b9729199fc
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1467433224759_0002, Tracking URL = http://Inceptez:8088/proxy/application_1467433224759_0002/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1467433224759_0002
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2016-07-02 11:37:45,238 Stage-1 map = 0%,  reduce = 0%
2016-07-02 11:37:53,987 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.63 sec
2016-07-02 11:38:04,795 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.11 sec
MapReduce Total cumulative CPU time: 3 seconds 110 msec
Ended Job = job_1467433224759_0002
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 3.11 sec   HDFS Read: 8472295 HDFS Write: 6 SUCCESS
Total MapReduce CPU Time Spent: 3 seconds 110 msec
OK
_c0
95904
Time taken: 31.339 seconds, Fetched: 1 row(s)

hive (retail)> create external table externaltxnrecords(txnno INT, txndate STRING, custno INT, amount DOUBLE,
             > category STRING, product STRING, city STRING, state STRING, spendby STRING)
             > row format delimited
             > fields terminated by ','
             > stored as textfile location '/user/hduser/hiveexternaldata';  ==> This folder neet not be available @ the tims of table creation.
OK
Time taken: 0.194 seconds

hive (retail)> explain select * from txnrecords limit 10;  
OK
Explain
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: 10
      Processor Tree:
        TableScan
          alias: txnrecords
          Statistics: Num rows: 13753 Data size: 8472073 Basic stats: COMPLETE Column stats: NONE
          Select Operator
            expressions: txnno (type: int), txndate (type: string), custno (type: int), amount (type: double), category (type: string), product (type: string), city (type: string), state (type: string), spendby (type: string)
            outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
            Statistics: Num rows: 13753 Data size: 8472073 Basic stats: COMPLETE Column stats: NONE
            Limit
              Number of rows: 10
              Statistics: Num rows: 10 Data size: 6160 Basic stats: COMPLETE Column stats: NONE
              ListSink

Time taken: 0.446 seconds, Fetched: 20 row(s)

Insert into table managedtxnrecsbycat partition (category='Games') select txnno,txndate,custno,amount,product,city,state,spendby from txnrecords where category='Games';

hive (retail)> Insert into table managedtxnrecsbycat partition (category='Games') select txnno,txndate,custno,amount,product,city,state,spendby from txnrecords where category='Games';
Query ID = hduser_20160702120202_a1c91f65-f3b3-4c3a-8cef-36c49c6ac4b1
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1467433224759_0003, Tracking URL = http://Inceptez:8088/proxy/application_1467433224759_0003/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1467433224759_0003
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2016-07-02 12:03:00,763 Stage-1 map = 0%,  reduce = 0%
2016-07-02 12:03:24,133 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 7.58 sec
MapReduce Total cumulative CPU time: 7 seconds 580 msec
Ended Job = job_1467433224759_0003
Stage-4 is selected by condition resolver.
Stage-3 is filtered out by condition resolver.
Stage-5 is filtered out by condition resolver.
Moving data to: hdfs://localhost:54310/tmp/hive/hduser/6a39f2c2-860a-4caf-9b9c-c26a9cdabfcc/hive_2016-07-02_12-02-38_111_6133676808606257492-1/-ext-10000
Loading data to table retail.managedtxnrecsbycat partition (category=Games)
Partition retail.managedtxnrecsbycat{category=Games} stats: [numFiles=1, numRows=6946, totalSize=490503, rawDataSize=483557]
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1   Cumulative CPU: 7.58 sec   HDFS Read: 8472295 HDFS Write: 490606 SUCCESS
Total MapReduce CPU Time Spent: 7 seconds 580 msec
OK
txnno	txndate	custno	amount	product	city	state	spendby
Time taken: 49.148 seconds

show partitions managedtxnrecsByCat;
OK
partition
category=Games
Time taken: 0.191 seconds, Fetched: 1 row(s)

[hduser@Inceptez data]$ hdfs dfs -ls -R /user/hive
16/07/02 12:04:43 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
drwxrwxr-x   - hduser supergroup          0 2016-07-02 10:47 /user/hive/warehouse
drwxr-xr-x   - hduser supergroup          0 2016-07-02 11:54 /user/hive/warehouse/retail.db
drwxr-xr-x   - hduser supergroup          0 2016-07-02 12:03 /user/hive/warehouse/retail.db/managedtxnrecsbycat
drwxr-xr-x   - hduser supergroup          0 2016-07-02 12:03 /user/hive/warehouse/retail.db/managedtxnrecsbycat/category=Games
-rw-r--r--   1 hduser supergroup     490503 2016-07-02 12:03 /user/hive/warehouse/retail.db/managedtxnrecsbycat/category=Games/000000_0
drwxr-xr-x   - hduser supergroup          0 2016-07-02 11:30 /user/hive/warehouse/retail.db/txnrecords
-rw-r--r--   1 hduser supergroup    8472073 2016-07-02 11:30 /user/hive/warehouse/retail.db/txnrecords/txns

hive> dfs -mkdir -p /user/hduser/hiveexternaldata;

hive> select count(*) from externaltxnrecords;    
Query ID = hduser_20160702121717_b673a8bf-3db0-4618-9a40-bb4b5fb28d7a
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1467433224759_0005, Tracking URL = http://Inceptez:8088/proxy/application_1467433224759_0005/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1467433224759_0005
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2016-07-02 12:18:05,107 Stage-1 map = 0%,  reduce = 0%
2016-07-02 12:18:23,558 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.21 sec
2016-07-02 12:18:44,456 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 5.46 sec
MapReduce Total cumulative CPU time: 5 seconds 460 msec
Ended Job = job_1467433224759_0005
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 5.46 sec   HDFS Read: 303 HDFS Write: 2 SUCCESS
Total MapReduce CPU Time Spent: 5 seconds 460 msec
OK
0
Time taken: 67.873 seconds, Fetched: 1 row(s)


[hduser@Inceptez data]$ hdfs dfs -ls -R /user/hduser/hiveexternaldata
16/07/02 12:21:04 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[hduser@Inceptez data]$ hdfs dfs -cp txns_new /user/hduser/hiveexternaldata
16/07/02 12:22:04 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[hduser@Inceptez data]$ hdfs dfs -ls -R /user/hduser/hiveexternaldata
16/07/02 12:22:16 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
-rw-r--r--   1 hduser supergroup    8472073 2016-07-02 12:22 /user/hduser/hiveexternaldata/txns_new


hive> select count(*) from externaltxnrecords;
Query ID = hduser_20160702122222_5169c939-82df-45b4-89ec-68819552ccbd
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1467433224759_0006, Tracking URL = http://Inceptez:8088/proxy/application_1467433224759_0006/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1467433224759_0006
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2016-07-02 12:22:55,082 Stage-1 map = 0%,  reduce = 0%
2016-07-02 12:23:12,959 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.31 sec
2016-07-02 12:23:30,854 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 6.24 sec
MapReduce Total cumulative CPU time: 6 seconds 240 msec
Ended Job = job_1467433224759_0006
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 6.24 sec   HDFS Read: 8472287 HDFS Write: 6 SUCCESS
Total MapReduce CPU Time Spent: 6 seconds 240 msec
OK
95904
Time taken: 57.337 seconds, Fetched: 1 row(s)
====================================================================================================
hive> create external table players (pl_id int, pl_name string, pl_team string, pl_salary string, pl_country string);
OK
Time taken: 0.978 seconds
hive> desc players;
OK
pl_id               	int                 	                    
pl_name             	string              	                    
pl_team             	string              	                    
pl_salary           	string              	                    
pl_country          	string              	                    
Time taken: 1.386 seconds, Fetched: 5 row(s)
hive> show create table players;
OK
CREATE EXTERNAL TABLE `players`(
  `pl_id` int, 
  `pl_name` string, 
  `pl_team` string, 
  `pl_salary` string, 
  `pl_country` string)
ROW FORMAT SERDE 
  'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe' 
STORED AS INPUTFORMAT 
  'org.apache.hadoop.mapred.TextInputFormat' 
OUTPUTFORMAT 
  'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
LOCATION
  'hdfs://localhost:54310/user/hive/warehouse/players'
TBLPROPERTIES (
  'transient_lastDdlTime'='1467477662')
Time taken: 0.431 seconds, Fetched: 16 row(s)

hive> alter table players set location '/user/hduser/hiveexternaldata/players';
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. {0}  is not absolute or has no scheme information.  Please specify a complete absolute uri with scheme information. /user/hduser/hiveexternaldata/players
hive> alter table players set location 'hdfs://localhost:54310/user/hduser/hiveexternaldata/players'; 
OK
Time taken: 0.743 seconds
hive> show create table players;                                                                     
OK
CREATE EXTERNAL TABLE `players`(
  `pl_id` int, 
  `pl_name` string, 
  `pl_team` string, 
  `pl_salary` string, 
  `pl_country` string)
ROW FORMAT DELIMITED 
  FIELDS TERMINATED BY ',' 
  LINES TERMINATED BY '\n' 
STORED AS INPUTFORMAT 
  'org.apache.hadoop.mapred.TextInputFormat' 
OUTPUTFORMAT 
  'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
LOCATION
  'hdfs://localhost:54310/user/hduser/hiveexternaldata/players'
TBLPROPERTIES (
  'COLUMN_STATS_ACCURATE'='false', 
  'last_modified_by'='hduser', 
  'last_modified_time'='1467479060', 
  'numFiles'='0', 
  'numRows'='-1', 
  'rawDataSize'='-1', 
  'totalSize'='0', 
  'transient_lastDdlTime'='1467479060')
Time taken: 0.201 seconds, Fetched: 24 row(s)

[hduser@Inceptez ~]$ mkdir hive_data_01
[hduser@Inceptez ~]$ cd hive_data_01/
[hduser@Inceptez hive_data_01]$ echo 10,'Sachin','India',2500000,'India' > players.txt
[hduser@Inceptez hive_data_01]$ echo 17,'Federer','Switzerland',6000000,'Switzerland' >> players.txt
[hduser@Inceptez hive_data_01]$ echo 10,'Messi','Barcelona',3000000,'Argentina' >> players.txt
[hduser@Inceptez hive_data_01]$ echo 7,'Ronaldo','Real Madrid',3250000,'Portugal' >> players.txt
[hduser@Inceptez hive_data_01]$ cat players.txt 
10,Sachin,India,2500000,India
17,Federer,Switzerland,6000000,Switzerland
10,Messi,Barcelona,3000000,Argentina
7,Ronaldo,Real Madrid,3250000,Portugal

[hduser@Inceptez data]$ hdfs dfs -put ~/hive_data_01/players.txt /user/hduser/hiveexternaldata/players

hive> select * from players;
OK
10	Sachin	India	2500000	India
17	Federer	Switzerland	6000000	Switzerland
10	Messi	Barcelona	3000000	Argentina
7	Ronaldo	Real Madrid	3250000	Portugal
Time taken: 0.742 seconds, Fetched: 4 row(s)

[hduser@Inceptez hive_data_01]$ hdfs dfs -rm /user/hduser/hiveexternaldata/players
16/07/02 23:24:04 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted /user/hduser/hiveexternaldata/players

hive> select * from players;
OK
Time taken: 0.133 seconds

[hduser@Inceptez hive_data_01]$ hdfs dfs -mkdir /user/hduser/hiveexternaldata/players
[hduser@Inceptez hive_data_01]$ hdfs dfs -put ~/hive_data_01/players* /user/hduser/hiveexternaldata/players
[hduser@Inceptez hive_data_01]$ hdfs dfs -ls -R /user/hduser/hiveexternaldata/players
-rw-r--r--   1 hduser supergroup        149 2016-07-02 23:27 /user/hduser/hiveexternaldata/players/players.txt
-rw-r--r--   1 hduser supergroup         72 2016-07-02 23:27 /user/hduser/hiveexternaldata/players/players1.txt

hive> select * from players;                                                                         
OK
10	Sachin	India	2500000	India
17	Federer	Switzerland	6000000	Switzerland
10	Messi	Barcelona	3000000	Argentina
7	Ronaldo	Real Madrid	3250000	Portugal
7	Dhoni	India	2750000	India
10	Rooney	Manchester United	2750000	England
Time taken: 0.059 seconds, Fetched: 6 row(s)


[hduser@Inceptez hive_data_01]$ echo 10,'Deepak','Manchester United',1010000,'India' >> players2.txt
[hduser@Inceptez hive_data_01]$ echo dummy record >> players2.txt
[hduser@Inceptez hive_data_01]$ cat players2.txt 
10,Deepak,Manchester United,1010000,India
dummy record

hive> select * from players;    
OK
10	Sachin	India	2500000	India
17	Federer	Switzerland	6000000	Switzerland
10	Messi	Barcelona	3000000	Argentina
7	Ronaldo	Real Madrid	3250000	Portugal
7	Dhoni	India	2750000	India
10	Rooney	Manchester United	2750000	England
10	Deepak	Manchester United	1010000	India
NULL	NULL	NULL	NULL	NULL
Time taken: 0.068 seconds, Fetched: 8 row(s)

hive> desc formatted players;
OK
# col_name            	data_type           	comment             
	 	 
pl_id               	int                 	                    
pl_name             	string              	                    
pl_team             	string              	                    
pl_salary           	string              	                    
pl_country          	string              	                    
	 	 
# Detailed Table Information	 	 
Database:           	default             	 
Owner:              	hduser              	 
CreateTime:         	Sat Jul 02 22:19:28 IST 2016	 
LastAccessTime:     	UNKNOWN             	 
Protect Mode:       	None                	 
Retention:          	0                   	 
Location:           	hdfs://localhost:54310/user/hduser/hiveexternaldata/players	 
Table Type:         	EXTERNAL_TABLE      	 
Table Parameters:	 	 
	COLUMN_STATS_ACCURATE	false               
	EXTERNAL            	TRUE                
	last_modified_by    	hduser              
	last_modified_time  	1467479060          
	numFiles            	0                   
	numRows             	-1                  
	rawDataSize         	-1                  
	totalSize           	0                   
	transient_lastDdlTime	1467479060          
	 	 
# Storage Information	 	 
SerDe Library:      	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe	 
InputFormat:        	org.apache.hadoop.mapred.TextInputFormat	 
OutputFormat:       	org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat	 
Compressed:         	No                  	 
Num Buckets:        	-1                  	 
Bucket Columns:     	[]                  	 
Sort Columns:       	[]                  	 
Storage Desc Params:	 	 
	field.delim         	,                   
	line.delim          	\n                  
	serialization.format	,                   
Time taken: 0.349 seconds, Fetched: 40 row(s)
====================================================================================================
[hduser@Inceptez hive]$ cd /home/hduser/install
[hduser@Inceptez install]$ ls
all_docs                            hadoop-2.6.0.tar.gz              hive-site_rms_transction.xml  sqoop-1.4.5.bin__hadoop-2.0.4-alpha.tar.gz
apache-flume-1.4.0-bin.tar.gz       hbase-0.98.4-hadoop2-bin.tar.gz  hivexmlserde-1.0.5.3.jar      tail_exec.conf
apache-hive-0.14.0-bin.tar.gz       hdfs.conf                        mysql-connector-java.jar      twitter.conf
flume-sources-1.0-SNAPSHOT.jar      helloword.java                   netcat_flume.conf             zookeeper-3.4.6.tar.gz
flume-sources-1.0-SNAPSHOT_old.jar  hive-exec-0.14.0.jar             pig-0.15.0.tar.gz
[hduser@Inceptez install]$ cp hive-exec-0.14.0.jar hivexmlserde-1.0.5.3.jar mysql-connector-java.jar /usr/local/hive/lib/
[hduser@Inceptez install]$ hive --service metastore
Starting Hive Metastore Server
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hive/lib/hive-jdbc-0.14.0-standalone.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]

====================================================================================================
hive> set hive.support.concurrency=true;
hive> set hive.enforce.bucketing=true;
hive> set hive.exec.dynamic.partition.mode=nonstrict;
hive> set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;
hive> set hive.compactor.initiator.on=true;
hive> set hive.compactor.worker.threads=1 ;
hive> create table HiveTest
    > (EmployeeID Int,FirstName String,Designation String,
    > Salary Int,Department String)
    > clustered by (department) into 3 buckets
    > stored as orc TBLPROPERTIES ('transactional'='true');
OK
Time taken: 1.28 seconds
hive> select * from HiveTest;
OK
Time taken: 0.545 seconds
hive> desc HiveTest;
OK
employeeid          	int                 	                    
firstname           	string              	                    
designation         	string              	                    
salary              	int                 	                    
department          	string              	                    
Time taken: 0.151 seconds, Fetched: 5 row(s)
hive> set hive.cli.print.current.db=true;
hive (default)> show tables in default;
OK
hivetest
Time taken: 0.276 seconds, Fetched: 1 row(s)
hive (default)> show databases;
OK
default
Time taken: 0.112 seconds, Fetched: 1 row(s)
hive (default)> quit;
[hduser@Inceptez ~]$ hive

Logging initialized using configuration in jar:file:/usr/local/hive/lib/hive-common-0.14.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hive/lib/hive-jdbc-0.14.0-standalone.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
hive> show databases;
OK
default
Time taken: 0.476 seconds, Fetched: 1 row(s)

mysql> select * from TBLS;
+--------+-------------+-------+------------------+--------+-----------+-------+----------+---------------+--------------------+--------------------+----------------+
| TBL_ID | CREATE_TIME | DB_ID | LAST_ACCESS_TIME | OWNER  | RETENTION | SD_ID | TBL_NAME | TBL_TYPE      | VIEW_EXPANDED_TEXT | VIEW_ORIGINAL_TEXT | LINK_TARGET_ID |
+--------+-------------+-------+------------------+--------+-----------+-------+----------+---------------+--------------------+--------------------+----------------+
|      1 |  1467522307 |     1 |                0 | hduser |         0 |     1 | hivetest | MANAGED_TABLE | NULL               | NULL               |           NULL |
+--------+-------------+-------+------------------+--------+-----------+-------+----------+---------------+--------------------+--------------------+----------------+
1 row in set (0.00 sec)																																																																																																							

mysql> select * from COLUMNS_V2;
+-------+---------+-------------+-----------+-------------+
| CD_ID | COMMENT | COLUMN_NAME | TYPE_NAME | INTEGER_IDX |
+-------+---------+-------------+-----------+-------------+
|     1 | NULL    | department  | string    |           4 |
|     1 | NULL    | designation | string    |           2 |
|     1 | NULL    | employeeid  | int       |           0 |
|     1 | NULL    | firstname   | string    |           1 |
|     1 | NULL    | salary      | int       |           3 |
+-------+---------+-------------+-----------+-------------+
5 rows in set (0.05 sec)

====================================================================================================
hive (default)> create table array(id int,name string,sal bigint,desig array<string>,city string)
row format delimited
fields terminated by ','
collection items terminated by '$';
OK
Time taken: 0.494 seconds
hive (default)> show tables;
OK
array
hivetest
Time taken: 0.18 seconds, Fetched: 2 row(s)
hive (default)> show create table array;
OK
CREATE TABLE `array`(
  `id` int, 
  `name` string, 
  `sal` bigint, 
  `desig` array<string>, 
  `city` string)
ROW FORMAT DELIMITED 
  FIELDS TERMINATED BY ',' 
  COLLECTION ITEMS TERMINATED BY '$' 
STORED AS INPUTFORMAT 
  'org.apache.hadoop.mapred.TextInputFormat' 
OUTPUTFORMAT 
  'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
LOCATION
  'hdfs://localhost:54310/user/hive/warehouse/array'
TBLPROPERTIES (
  'transient_lastDdlTime'='1467735040')
Time taken: 0.265 seconds, Fetched: 17 row(s)

[hduser@Inceptez hive_data_01]$ vi array_ip.data
[hduser@Inceptez hive_data_01]$ hdfs dfs -mkdir data_01
[hduser@Inceptez hive_data_01]$ hdfs dfs -put array_ip.data data_01/

[hduser@Inceptez hive_data_01]$ hdfs dfs -ls -R data_01*
-rw-r--r--   1 hduser supergroup        160 2016-07-05 21:52 data_01/array_ip.data

hive (default)> load data inpath '/user/hduser/data_01/array_ip.data' into table array; 
Loading data to table default.array
Table default.array stats: [numFiles=1, totalSize=160]
OK
Time taken: 0.682 seconds

[hduser@Inceptez hive_data_01]$ hdfs dfs -ls -R /*/*/data_01
-rw-r--r--   1 hduser supergroup        160 2016-07-05 21:52 /user/hduser/data_01/array_ip.data
[hduser@Inceptez hive_data_01]$ hdfs dfs -ls -R /*/*/data*

hive (default)> set hive.cli.print.header=true;
hive (default)> select * from array;           
OK
array.id	array.name	array.sal	array.desig	array.city
NULL	40000	NULL	["hyd"]	NULL
2	Bala	30000	["SWE","Analyst","SAnalyst","PM"]	hyd
3	Chandra	50000	["SWE","Analyst","PL","PM"]	Che
4	Gokul	30000	["SWE","Analyst","SAnalyst",""]	hyd
Time taken: 0.139 seconds, Fetched: 4 row(s)

hive (default)> !cat /home/hduser/hive_data_01/array_ip.data;
rvindh,40000,SWE$Analyst$SAnalyst$PL,hyd
2,Bala,30000,SWE$Analyst$SAnalyst$PM,hyd
3,Chandra,50000,SWE$Analyst$PL$PM,Che
4,Gokul,30000,SWE$Analyst$SAnalyst$,hyd

[hduser@Inceptez hive_data_01]$ vi array_ip.data 
[hduser@Inceptez hive_data_01]$ hdfs dfs -put array_ip.data data_01/

[hduser@Inceptez hive_data_01]$ hdfs dfs -ls -R /user/hive/warehouse/*
-rw-r--r--   1 hduser supergroup        160 2016-07-05 21:52 /user/hive/warehouse/array/array_ip.data

hive> load data inpath '/user/hduser/data_01/array_ip.data' into table array;
Loading data to table default.array
Table default.array stats: [numFiles=2, totalSize=323]
OK
Time taken: 0.818 seconds

[hduser@Inceptez hive_data_01]$ hdfs dfs -ls -R /user/hive/warehouse/*
-rw-r--r--   1 hduser supergroup        160 2016-07-05 21:52 /user/hive/warehouse/array/array_ip.data
-rw-r--r--   1 hduser supergroup        163 2016-07-05 22:12 /user/hive/warehouse/array/array_ip_copy_1.data

hive> select * from array;                                                   
OK
NULL	40000	NULL	["hyd"]	NULL
2	Bala	30000	["SWE","Analyst","SAnalyst","PM"]	hyd
3	Chandra	50000	["SWE","Analyst","PL","PM"]	Che
4	Gokul	30000	["SWE","Analyst","SAnalyst",""]	hyd
1	Arvindh	40000	["SWE","Analyst","SAnalyst","PL"]	hyd
2	Bala	30000	["SWE","Analyst","SAnalyst","PM"]	hyd
3	Chandra	50000	["SWE","Analyst","PL","PM"]	Che
4	Gokul	30000	["SWE","Analyst","SAnalyst",""]	hyd
Time taken: 0.418 seconds, Fetched: 8 row(s)

[hduser@Inceptez hive_data_01]$ hdfs dfs -rm /user/hive/warehouse/array/*
16/07/05 22:16:30 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted /user/hive/warehouse/array/array_ip.data
16/07/05 22:16:30 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted /user/hive/warehouse/array/array_ip_copy_1.data

hive> select * from array;
OK
Time taken: 0.076 seconds

hive> dfs -put /home/hduser/hive_data_01/array_ip.data data_01/;
hive> load data inpath '/user/hduser/data_01/array_ip.data' overwrite into table array;
Loading data to table default.array
Table default.array stats: [numFiles=1, numRows=0, totalSize=163, rawDataSize=0]
OK
Time taken: 0.725 seconds

hive> set hive.cli.print.header=true;
hive> select * from array;           
OK
array.id	array.name	array.sal	array.desig	array.city
1	Arvindh	40000	["SWE","Analyst","SAnalyst","PL"]	hyd
2	Bala	30000	["SWE","Analyst","SAnalyst","PM"]	hyd
3	Chandra	50000	["SWE","Analyst","PL","PM"]	Che
4	Gokul	30000	["SWE","Analyst","SAnalyst",""]	hyd
Time taken: 0.264 seconds, Fetched: 4 row(s)

hive> select desig[0],desig[1],desig[2],desig[3] from array;
OK
_c0	_c1	_c2	_c3
SWE	Analyst	SAnalyst	PL
SWE	Analyst	SAnalyst	PM
SWE	Analyst	PL	PM
SWE	Analyst	SAnalyst	
Time taken: 0.476 seconds, Fetched: 4 row(s)

hive> select desig[0],desig[1],desig[2],desig[3],desig[4] from array;
OK
_c0	_c1	_c2	_c3	_c4
SWE	Analyst	SAnalyst	PL	NULL
SWE	Analyst	SAnalyst	PM	NULL
SWE	Analyst	PL	PM	NULL
SWE	Analyst	SAnalyst		NULL
Time taken: 0.163 seconds, Fetched: 4 row(s)

[hduser@Inceptez ~]$ su
Password: 
[root@Inceptez hduser]# mysql -p
Enter password: 
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 43

mysql> select * from metastore.TBLS;
+--------+-------------+-------+------------------+--------+-----------+-------+----------+---------------+--------------------+--------------------+----------------+
| TBL_ID | CREATE_TIME | DB_ID | LAST_ACCESS_TIME | OWNER  | RETENTION | SD_ID | TBL_NAME | TBL_TYPE      | VIEW_EXPANDED_TEXT | VIEW_ORIGINAL_TEXT | LINK_TARGET_ID |
+--------+-------------+-------+------------------+--------+-----------+-------+----------+---------------+--------------------+--------------------+----------------+
|      1 |  1467522307 |     1 |                0 | hduser |         0 |     1 | hivetest | MANAGED_TABLE | NULL               | NULL               |           NULL |
|      6 |  1467735040 |     1 |                0 | hduser |         0 |     6 | array    | MANAGED_TABLE | NULL               | NULL               |           NULL |
+--------+-------------+-------+------------------+--------+-----------+-------+----------+---------------+--------------------+--------------------+----------------+
2 rows in set (0.00 sec)

mysql> select * from metastore.COLUMNS_V2;
+-------+---------+-------------+---------------+-------------+
| CD_ID | COMMENT | COLUMN_NAME | TYPE_NAME     | INTEGER_IDX |
+-------+---------+-------------+---------------+-------------+
|     1 | NULL    | department  | string        |           4 |
|     1 | NULL    | designation | string        |           2 |
|     1 | NULL    | employeeid  | int           |           0 |
|     1 | NULL    | firstname   | string        |           1 |
|     1 | NULL    | salary      | int           |           3 |
|     6 | NULL    | city        | string        |           4 |
|     6 | NULL    | desig       | array<string> |           3 |
|     6 | NULL    | id          | int           |           0 |
|     6 | NULL    | name        | string        |           1 |
|     6 | NULL    | sal         | bigint        |           2 |
+-------+---------+-------------+---------------+-------------+
10 rows in set (0.00 sec)

====================================================================================================
hive> create table Struct(id int,name string,sal bigint,addr struct<city:string,state:string,pin:bigint>)
    > row format delimited
    > fields terminated by ','
    > collection items terminated by '$';

hive> load data local inpath '/home/hduser/hive_data_01/struct_ip.data' overwrite into table struct;
Loading data to table default.struct
Table default.struct stats: [numFiles=1, numRows=0, totalSize=74, rawDataSize=0]
OK
Time taken: 0.822 seconds
hive> select * from struct;                                                                         
OK
1	Arvindh	40000	{"city":"Hyderabad","state":"AP","pin":800042}
2	Bala	30000	{"city":"Chennai","state":"TamilNadu","pin":600042}
Time taken: 0.165 seconds, Fetched: 2 row(s)
hive> select addr.city, addr.pin from struct;
OK
Hyderabad	800042
Chennai	600042
Time taken: 0.273 seconds, Fetched: 2 row(s)
====================================================================================================
hive> create table map1 (id int,name string,sal bigint,Mark map<string,int>,city string)
    > row format delimited
    > fields terminated by ','
    > collection items terminated by '$'
    > map keys terminated by '#';
OK
Time taken: 0.201 seconds
hive> load data local inpath '/home/hduser/hive_data_01/map_ip.data' overwrite into table map1;
Loading data to table default.map1
Table default.map1 stats: [numFiles=1, numRows=0, totalSize=67, rawDataSize=0]
OK
Time taken: 0.764 seconds
hive> select * from map1;                                                                      
OK
1	Arvindh	40000	{"mat":100,"Eng":99}	hyd
2	Bala	30000	{"Sci":100,"Eng":99}	chn
Time taken: 0.19 seconds, Fetched: 2 row(s)
hive> select mark from map1;
OK
{"mat":100,"Eng":99}
{"Sci":100,"Eng":99}
Time taken: 0.204 seconds, Fetched: 2 row(s)
hive> select mark('mat') from map1;
FAILED: SemanticException [Error 10011]: Line 1:7 Invalid function 'mark'
hive> select Mark('mat') from map1;
FAILED: SemanticException [Error 10011]: Line 1:7 Invalid function 'Mark'
hive> select Mark['mat'] from map1;
OK
100
NULL
Time taken: 0.284 seconds, Fetched: 2 row(s)
hive> select Mark['eng'] from map1;
OK
NULL
NULL
Time taken: 0.187 seconds, Fetched: 2 row(s)
hive> select Mark['Eng'] from map1;
OK
99
99
Time taken: 0.19 seconds, Fetched: 2 row(s)
====================================================================================================
create table union1 (
emp_id int, 
emp_name string, 
emp_info uniontype<age int, address struct<city:varchar,state:string,pin:bigint>,contact array<int>,score map(string,int)>
emp_gender char(1))
row format delimited
fields terminated by ','
collection items terminated by '$'
map keys terminated by '#';

hive> create table union1 (
    > emp_id int, 
    > emp_name string, 
    > emp_info uniontype<age int, address struct<city:varchar,state:string,pin:bigint>,contact array<int>,score map(string,int)>
    > emp_gender char(1))
    > row format delimited
    > fields terminated by ','
    > collection items terminated by '$'
    > map keys terminated by '#';
FAILED: ParseException line 4:19 cannot recognize input near 'age' 'int' ',' in column type

create table union1 (
emp_id int, 
emp_name string, 
emp_info uniontype<int, struct<city:varchar(30),state:string,pin:bigint>,array<int>,map<string,int>>,
emp_gender char(1))
row format delimited
fields terminated by ','
collection items terminated by '$'
map keys terminated by '#';

hive> create table union1 (
    > emp_id int, 
    > emp_name string, 
    > emp_info uniontype<int, struct<city:varchar(30),state:string,pin:bigint>,array<int>,map<string,int>>,
    > emp_gender char(1))
    > row format delimited
    > fields terminated by ','
    > collection items terminated by '$'
    > map keys terminated by '#';
OK
Time taken: 0.124 seconds

17,Deepak,27$Chennai$TN$600023$0283$6512$Math#95$Phy#98$Che#90,M
10,Sachin,43$Mumbai$MH$400011$200$245$Math#65$Phy#78$Che#60,M

hive> select * from union1;
OK
Failed with exception java.io.IOException:org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.IndexOutOfBoundsException: Index: 27, Size: 4
Time taken: 0.078 seconds
hive> select emp_id from union1;
OK
17
10
Time taken: 0.08 seconds, Fetched: 2 row(s)
hive> select emp_id,emp_name from union1;
OK
17	Deepak
10	Sachin
Time taken: 0.105 seconds, Fetched: 2 row(s)
hive> select emp_id,emp_name,emp_gender from union1;
OK
17	Deepak	M
10	Sachin	M
Time taken: 0.109 seconds, Fetched: 2 row(s)
hive> select emp_id,emp_name,emp_gender,emp_info from union1;
OK
Failed with exception java.io.IOException:org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.IndexOutOfBoundsException: Index: 27, Size: 4
Time taken: 0.105 seconds


17,Deepak,27,M
10,Sachin,Mumbai$MH$400011,M

hive> select emp_info from union1;                                                                  
OK
Failed with exception java.io.IOException:org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NumberFormatException: Mumbai
Time taken: 0.161 seconds
hive> select emp_info from union1 where emp_id = 10;
OK
Failed with exception java.io.IOException:org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NumberFormatException: Mumbai
Time taken: 0.463 seconds
hive> select emp_info from union1 where emp_id = 17;
OK
{0:null}
Time taken: 0.148 seconds, Fetched: 1 row(s)

17,Deepak,27$Chennai$TN$600023,M
10,Sachin,43$Mumbai$MH$400011,M
====================================================================================================
