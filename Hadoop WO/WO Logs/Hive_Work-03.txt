hive (dkmdb01)> select * from players3 tablesample(bucket 1 out of 2);    
FAILED: SemanticException [Error 10060]: Sampling expression needed for non-bucketed table players3

hive (dkmdb01)> select * from employee1 tablesample(bucket 1 out of 2);
FAILED: SemanticException [Error 10060]: Sampling expression needed for non-bucketed table employee1

hive (dkmdb01)> select * from salesorders3 tablesample(bucket 1 out of 2);
OK
salesorders3.ord_dt	salesorders3.region	salesorders3.rep	salesorders3.item	salesorders3.unit	salesorders3.unit_cost	salesorders3.total
2016-05-14	Central	Gill	Pencil	53	NULL	NULL
2015-02-09	Central	Jardine	Pencil	36	NULL	NULL
2016-04-10	Central	Andrews	Pencil	66	NULL	NULL
2016-09-10	Central	Gill	Pencil	7	NULL	NULL
2015-08-15	East	Jones	Pencil	35	NULL	NULL
2015-12-12	Central	Smith	Pencil	67	NULL	NULL
2015-01-06	East	Jones	Pencil	95	NULL	NULL
2015-05-05	Central	Jardine	Pencil	90	NULL	NULL
2015-03-15	West	Sorvino	Pencil	56	NULL	NULL
2016-10-31	Central	Andrews	Pencil	14	NULL	NULL
2015-04-18	Central	Andrews	Pencil	75	NULL	NULL
2015-05-22	West	Thompson	Pencil	32	NULL	NULL
2015-06-25	Central	Morgan	Pencil	90	NULL	NULL
2015-12-29	East	Parent	Pen Set	74	NULL	1
2016-09-27	West	Sorvino	Pen	76	NULL	NULL
2016-08-07	Central	Kivell	Pen Set	42	NULL	1
2016-07-21	Central	Morgan	Pen Set	55	NULL	NULL
2016-07-04	East	Jones	Pen Set	62	NULL	NULL
2016-04-27	East	Howard	Pen	96	NULL	NULL
2016-03-24	Central	Jardine	Pen Set	50	NULL	NULL
2015-11-25	Central	Kivell	Pen Set	96	NULL	NULL
2015-11-08	East	Parent	Pen	15	NULL	NULL
2015-10-22	East	Jones	Pen	64	NULL	NULL
2015-09-18	East	Jones	Pen Set	16	NULL	NULL
2015-02-26	Central	Gill	Pen	27	NULL	NULL
2015-09-01	Central	Smith	Desk	2	NULL	NULL
2016-06-17	Central	Kivell	Desk	5	NULL	NULL
2016-08-24	West	Sorvino	Desk	3	NULL	NULL
Time taken: 0.105 seconds, Fetched: 28 row(s)
====================================================================================================
hive (dkmdb01)> insert overwrite local directory '/home/hduser/hive2local_data' row format delimited select * from employee1;                         
Query ID = hduser_20160711222121_cee12cd2-802d-4d54-b04e-0a6f0a802604
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1468250619678_0005, Tracking URL = http://Inceptez:8088/proxy/application_1468250619678_0005/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1468250619678_0005
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2016-07-11 22:22:00,959 Stage-1 map = 0%,  reduce = 0%
2016-07-11 22:22:10,568 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.28 sec
MapReduce Total cumulative CPU time: 1 seconds 280 msec
Ended Job = job_1468250619678_0005
Copying data to local directory /home/hduser/hive2local_data
Copying data to local directory /home/hduser/hive2local_data
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1   Cumulative CPU: 1.28 sec   HDFS Read: 540 HDFS Write: 221 SUCCESS
Total MapReduce CPU Time Spent: 1 seconds 280 msec
OK
employee1.emp_id	employee1.emp_name	employee1.emp_sal	employee1.row_ts
Time taken: 21.072 seconds

[hduser@Inceptez hive2local_data]$ cd /home/hduser/hive2local_data
[hduser@Inceptez hive2local_data]$ ls
000000_0
[hduser@Inceptez hive2local_data]$ cat 000000_0 
1Deepak750002016-06-13 11:36:27
2Sachin15000002016-06-13 11:37:51
3Messi25000002016-06-13 11:38:04
4Federer20000002016-06-13 12:18:19
5Ronaldo27500002016-06-18 02:58:31
6Rooney20000002016-07-09 10:41:36

hive (dkmdb01)> insert overwrite local directory '/home/hduser/hive2local_data/employee1.data1' row format delimited select * from employee1;
Query ID = hduser_20160711222525_94652adc-0a00-46ab-94f6-36dec5dbd3e2
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1468250619678_0006, Tracking URL = http://Inceptez:8088/proxy/application_1468250619678_0006/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1468250619678_0006
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2016-07-11 22:25:25,583 Stage-1 map = 0%,  reduce = 0%
2016-07-11 22:25:34,333 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.29 sec
MapReduce Total cumulative CPU time: 1 seconds 290 msec
Ended Job = job_1468250619678_0006
Copying data to local directory /home/hduser/hive2local_data/employee1.data1
Copying data to local directory /home/hduser/hive2local_data/employee1.data1
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1   Cumulative CPU: 1.29 sec   HDFS Read: 540 HDFS Write: 221 SUCCESS
Total MapReduce CPU Time Spent: 1 seconds 290 msec
OK
employee1.emp_id	employee1.emp_name	employee1.emp_sal	employee1.row_ts
Time taken: 19.319 seconds

[hduser@Inceptez hive2local_data]$ ls
000000_0  employee1.data1
[hduser@Inceptez hive2local_data]$ cd employee1.data1/
[hduser@Inceptez employee1.data1]$ ls
000000_0

hive (dkmdb01)> insert overwrite local directory '/home/hduser/hive2local_data' row format delimited fields terminated by '|' select * from employee1;
Query ID = hduser_20160711222727_a9f892dd-22b0-4ef0-9e98-52f302b4fa59
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1468250619678_0007, Tracking URL = http://Inceptez:8088/proxy/application_1468250619678_0007/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1468250619678_0007
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2016-07-11 22:27:34,959 Stage-1 map = 0%,  reduce = 0%
2016-07-11 22:27:42,592 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.26 sec
MapReduce Total cumulative CPU time: 1 seconds 260 msec
Ended Job = job_1468250619678_0007
Copying data to local directory /home/hduser/hive2local_data
Copying data to local directory /home/hduser/hive2local_data
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1   Cumulative CPU: 1.26 sec   HDFS Read: 540 HDFS Write: 221 SUCCESS
Total MapReduce CPU Time Spent: 1 seconds 260 msec
OK
employee1.emp_id	employee1.emp_name	employee1.emp_sal	employee1.row_ts
Time taken: 20.608 seconds

[hduser@Inceptez hive2local_data]$ ls
000000_0
[hduser@Inceptez hive2local_data]$ ls -l
total 4
-rw-r--r-- 1 hduser hduser 221 Jul 11 22:27 000000_0
[hduser@Inceptez hive2local_data]$ date
Mon Jul 11 22:28:33 IST 2016
[hduser@Inceptez hive2local_data]$ cat 000000_0 
1|Deepak|75000|2016-06-13 11:36:27
2|Sachin|1500000|2016-06-13 11:37:51
3|Messi|2500000|2016-06-13 11:38:04
4|Federer|2000000|2016-06-13 12:18:19
5|Ronaldo|2750000|2016-06-18 02:58:31
6|Rooney|2000000|2016-07-09 10:41:36
====================================================================================================
[hduser@Inceptez hive_data_01]$ vi serde1.xml
[hduser@Inceptez hive_data_01]$ hdfs dfs -put serde1.xml /user/hduser/data_01
[hduser@Inceptez hive_data_01]$ hadoop fs -ls -R /user/hduser/data_01
-rw-r--r--   1 hduser supergroup        657 2016-07-12 20:14 /user/hduser/data_01/serde1.xml

hive (default)> create external table xml_Emp1(empid string, empname string,income BIGINT,age int)
              > row format serde 'com.ibm.spss.hive.serde2.xml.XmlSerDe'
              > WITH SERDEPROPERTIES (
              > "column.xpath.empid"="/record/empid/text()",
              > "column.xpath.empname"="/record/empname/text()",
              > "column.xpath.income"="/record/income/text()",
              > "column.xpath.age"="/record/age/text()"
              > )
              > STORED AS
              > INPUTFORMAT 'com.ibm.spss.hive.serde2.xml.XmlInputFormat'
              > OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat'
              > location '/user/hduser/data_01'
              > TBLPROPERTIES ("xmlinput.start"="<record>","xmlinput.end"="</record>");
OK
Time taken: 5.477 seconds

hive (default)> show create table xml_emp1;
OK
createtab_stmt
CREATE EXTERNAL TABLE `xml_emp1`(
  `empid` string COMMENT 'from deserializer', 
  `empname` string COMMENT 'from deserializer', 
  `income` bigint COMMENT 'from deserializer', 
  `age` int COMMENT 'from deserializer')
ROW FORMAT DELIMITED 
STORED AS INPUTFORMAT 
  'com.ibm.spss.hive.serde2.xml.XmlInputFormat' 
OUTPUTFORMAT 
  'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
LOCATION
  'hdfs://localhost:54310/user/hduser/data_01'
TBLPROPERTIES (
  'COLUMN_STATS_ACCURATE'='false', 
  'numFiles'='0', 
  'numRows'='-1', 
  'rawDataSize'='-1', 
  'totalSize'='0', 
  'transient_lastDdlTime'='1468334761', 
  'xmlinput.end'='</record>', 
  'xmlinput.start'='<record>')
Time taken: 0.49 seconds, Fetched: 21 row(s)

hive (default)> select * from xml_Emp1;
OK
xml_emp1.empid	xml_emp1.empname	xml_emp1.income	xml_emp1.age
82338	Ashfaq	200000	29
83993	Vivek	300000	33
83994	Vishwa	200000	33
83994	Arun	200000	30
Time taken: 1.416 seconds, Fetched: 4 row(s)

[hduser@Inceptez hive_data_01]$ hadoop fs -cat /user/hduser/data_01/serde1.xml
<records>
<record>
<empid>82338</empid>
<empname>Ashfaq</empname>
<income>200000</income>
<age>29</age>
</record>
<record>
<empid>83993</empid>
<empname>Vivek</empname>
<income>300000</income>
<age>33</age>
</record>
<record>
<empid>83994</empid>
<empname>Vishwa</empname>
<income>200000</income>
<age>33</age>
</record>
<record>
<empid>83994</empid>
<empname>Arun</empname>
<income>200000</income>
<age>30</age>
</record>
<record1>
<empid>83994</empid>
<empname>Arun</empname>
<income>200000</income>
<age>30</age>
</record1>
</records>
<recs>
<record2>
<empid>83996</empid>
<empname>Arun</empname>
<income>200000</income>
<age>30</age>
</record2>
</recs>

====================================================================================================
hive (default)> select * from xml_Emp1;
OK
xml_emp1.empid	xml_emp1.empname	xml_emp1.income	xml_emp1.age
82338	Ashfaq	200000	29
83993	Vivek	300000	33
83994	Vishwa	200000	33
83994	Arun	200000	30
Time taken: 1.416 seconds, Fetched: 4 row(s)
hive (default)> ADD JAR /home/hduser/hive/HiveUdf.jar;
Added [/home/hduser/hive/HiveUdf.jar] to class path
Added resources: [/home/hduser/hive/HiveUdf.jar]
hive (default)> create function hello as 'Inceptez.Training.Hello.helloword';
OK
Time taken: 0.575 seconds
hive (default)> select hello(empname) from xml_Emp1;                         
OK
_c0
Hello Ashfaq
Hello Vivek
Hello Vishwa
Hello Arun
Time taken: 0.403 seconds, Fetched: 4 row(s)
====================================================================================================
hive (default)> create table dkmdb01.testdml1 (id int, id_val varchar(10)) row format delimited fields terminated by ',';
OK
Time taken: 0.154 seconds
hive (default)> show create table dkmdb01.testdml1;                                                                      
OK
createtab_stmt
CREATE TABLE `dkmdb01.testdml1`(
  `id` int, 
  `id_val` varchar(10))
ROW FORMAT DELIMITED 
  FIELDS TERMINATED BY ',' 
STORED AS INPUTFORMAT 
  'org.apache.hadoop.mapred.TextInputFormat' 
OUTPUTFORMAT 
  'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
LOCATION
  'hdfs://localhost:54310/user/hive/warehouse/dkmdb01.db/testdml1'
TBLPROPERTIES (
  'transient_lastDdlTime'='1468339581')
Time taken: 0.101 seconds, Fetched: 13 row(s)

hive (dkmdb01)> select * from employee1;
OK
employee1.emp_id	employee1.emp_name	employee1.emp_sal	employee1.row_ts
1	Deepak	75000	2016-06-13 11:36:27
2	Sachin	1500000	2016-06-13 11:37:51
3	Messi	2500000	2016-06-13 11:38:04
4	Federer	2000000	2016-06-13 12:18:19
5	Ronaldo	2750000	2016-06-18 02:58:31
6	Rooney	2000000	2016-07-09 10:41:36
Time taken: 0.215 seconds, Fetched: 6 row(s)
hive (dkmdb01)> insert into table testdml1 select emp_id, emp_name from employee1;
Query ID = hduser_20160712213737_36fc933f-d63a-40f0-9635-3df78ade7d91
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1468334110122_0001, Tracking URL = http://Inceptez:8088/proxy/application_1468334110122_0001/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1468334110122_0001
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2016-07-12 21:38:21,840 Stage-1 map = 0%,  reduce = 0%
2016-07-12 21:38:58,423 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.66 sec
MapReduce Total cumulative CPU time: 2 seconds 660 msec
Ended Job = job_1468334110122_0001
Stage-4 is selected by condition resolver.
Stage-3 is filtered out by condition resolver.
Stage-5 is filtered out by condition resolver.
Moving data to: hdfs://localhost:54310/tmp/hive/hduser/2dff9650-5801-47f6-8168-fb68ac9607ab/hive_2016-07-12_21-37-50_038_6791583143460144036-1/-ext-10000
Loading data to table dkmdb01.testdml1
[Error 30017]: Skipping stats aggregation by error org.apache.hadoop.hive.ql.metadata.HiveException: [Error 30001]: StatsPublisher cannot be initialized. There was a error in the initialization of StatsPublisher, and retrying might help. If you dont want the query to fail because accurate statistics could not be collected, set hive.stats.reliable=false
Table dkmdb01.testdml1 stats: [numFiles=1, totalSize=55]
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1   Cumulative CPU: 2.66 sec   HDFS Read: 540 HDFS Write: 55 SUCCESS
Total MapReduce CPU Time Spent: 2 seconds 660 msec
OK
_col0	_col1
Time taken: 74.35 seconds
hive (dkmdb01)> select * from testdml1;
OK
testdml1.id	testdml1.id_val
1	Deepak
2	Sachin
3	Messi
4	Federer
5	Ronaldo
6	Rooney
Time taken: 0.083 seconds, Fetched: 6 row(s)

[hduser@Inceptez ~]$ hdfs dfs -ls /user/hive/warehouse/dkmdb01.db/testdml1
Found 1 items
-rw-r--r--   1 hduser supergroup         55 2016-07-12 21:38 /user/hive/warehouse/dkmdb01.db/testdml1/000000_0
[hduser@Inceptez ~]$ hdfs dfs -cat /user/hive/warehouse/dkmdb01.db/testdml1/*0_*
1,Deepak
2,Sachin
3,Messi
4,Federer
5,Ronaldo
6,Rooney

hive (dkmdb01)> update table testdml1 set id_val = 'W Rooney' where id = 6;
FAILED: ParseException line 1:13 extraneous input 'testdml1' expecting SET near '<EOF>'

hive (dkmdb01)> update testdml1 set id_val = 'W Rooney' where id = 6;      
FAILED: SemanticException [Error 10297]: Attempt to do update or delete on table dkmdb01.testdml1 that does not use an AcidOutputFormat or is not bucketed
====================================================================================================
hive (dkmdb01)> alter table testdml1 tblproperties=('transactional'=true);
FAILED: ParseException line 1:21 cannot recognize input near 'tblproperties' '=' '(' in alter table statement

hive (dkmdb01)> alter table testdml1 set tblproperties=('transactional'=true);
FAILED: ParseException line 1:38 extraneous input '=' expecting ( near '=' in specifying key/value property
line 1:56 mismatched input 'true' expecting StringLiteral near '=' in specifying key/value property

hive (dkmdb01)> alter table testdml1 set tblproperties=('transactional'='true');
FAILED: ParseException line 1:38 extraneous input '=' expecting ( near '<EOF>'

hive (dkmdb01)> alter table testdml1 set tblproperties ('transactional'='true');
OK
Time taken: 0.273 seconds
hive (dkmdb01)> show create table testdml1;                                     
OK
createtab_stmt
CREATE TABLE `testdml1`(
  `id` int, 
  `id_val` varchar(10))
ROW FORMAT DELIMITED 
  FIELDS TERMINATED BY ',' 
STORED AS INPUTFORMAT 
  'org.apache.hadoop.mapred.TextInputFormat' 
OUTPUTFORMAT 
  'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
LOCATION
  'hdfs://localhost:54310/user/hive/warehouse/dkmdb01.db/testdml1'
TBLPROPERTIES (
  'COLUMN_STATS_ACCURATE'='false', 
  'last_modified_by'='hduser', 
  'last_modified_time'='1468340261', 
  'numFiles'='1', 
  'numRows'='-1', 
  'rawDataSize'='-1', 
  'totalSize'='55', 
  'transactional'='true', 
  'transient_lastDdlTime'='1468340261')
Time taken: 0.246 seconds, Fetched: 21 row(s)

hive (dkmdb01)> update testdml1 set id_val = 'W Rooney' where id = 6;           
FAILED: SemanticException [Error 10297]: Attempt to do update or delete on table testdml1 that does not use an AcidOutputFormat or is not bucketed

hive (dkmdb01)> alter table testdml1 clustered by id into 3 buckets;            
FAILED: ParseException line 1:34 missing ( at 'id' near '<EOF>'
line 1:37 missing ) at 'into' near '<EOF>'

hive (dkmdb01)> alter table testdml1 clustered by (id) into 3 buckets;
OK
Time taken: 0.157 seconds
hive (dkmdb01)> show create table testdml1;                           
OK
createtab_stmt
CREATE TABLE `testdml1`(
  `id` int, 
  `id_val` varchar(10))
CLUSTERED BY ( 
  id) 
INTO 3 BUCKETS
ROW FORMAT DELIMITED 
  FIELDS TERMINATED BY ',' 
STORED AS INPUTFORMAT 
  'org.apache.hadoop.mapred.TextInputFormat' 
OUTPUTFORMAT 
  'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
LOCATION
  'hdfs://localhost:54310/user/hive/warehouse/dkmdb01.db/testdml1'
TBLPROPERTIES (
  'COLUMN_STATS_ACCURATE'='false', 
  'last_modified_by'='hduser', 
  'last_modified_time'='1468340949', 
  'numFiles'='1', 
  'numRows'='-1', 
  'rawDataSize'='-1', 
  'totalSize'='55', 
  'transactional'='true', 
  'transient_lastDdlTime'='1468340949')
Time taken: 0.162 seconds, Fetched: 24 row(s)

hive (dkmdb01)> update testdml1 set id_val = 'W Rooney' where id = 6; 
Query ID = hduser_20160712220000_6fefdc83-42c0-419d-ba38-f57ea890ca21
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1468334110122_0002, Tracking URL = http://Inceptez:8088/proxy/application_1468334110122_0002/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1468334110122_0002
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 3
2016-07-12 22:00:48,555 Stage-1 map = 0%,  reduce = 0%
2016-07-12 22:00:58,356 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.66 sec
2016-07-12 22:01:23,864 Stage-1 map = 100%,  reduce = 67%, Cumulative CPU 5.7 sec
2016-07-12 22:01:55,237 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 5.18 sec
MapReduce Total cumulative CPU time: 5 seconds 180 msec
Ended Job = job_1468334110122_0002 with errors
Error during job, obtaining debugging information...
Examining task ID: task_1468334110122_0002_m_000000 (and more) from job job_1468334110122_0002

Task with the most failures(4): 
-----
Task ID:
  task_1468334110122_0002_r_000000

URL:
  http://0.0.0.0:8088/taskdetails.jsp?jobid=job_1468334110122_0002&tipid=task_1468334110122_0002_r_000000
-----
Diagnostic Messages for this Task:
Error: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {"key":{"reducesinkkey0":null},"value":{"_col1":"W Rooney"}}
	at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:265)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:444)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {"key":{"reducesinkkey0":null},"value":{"_col1":"W Rooney"}}
	at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:253)
	... 7 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.hadoop.hive.serde2.SerDeException: Error: expecting 2 but asking for field 2
data=[Ljava.lang.Object;@6a1ad1a1
tableType=struct<id:int,id_val:varchar(10)>
dataType=struct<_col0:struct<transactionid:bigint,bucketid:int,rowid:bigint>,_col1:int,_col2:varchar(10)>
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.processOp(FileSinkOperator.java:725)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84)
	at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:244)
	... 7 more
Caused by: org.apache.hadoop.hive.serde2.SerDeException: Error: expecting 2 but asking for field 2
data=[Ljava.lang.Object;@6a1ad1a1
tableType=struct<id:int,id_val:varchar(10)>
dataType=struct<_col0:struct<transactionid:bigint,bucketid:int,rowid:bigint>,_col1:int,_col2:varchar(10)>
	at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.doSerialize(LazySimpleSerDe.java:415)
	at org.apache.hadoop.hive.serde2.AbstractEncodingAwareSerDe.serialize(AbstractEncodingAwareSerDe.java:50)
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.processOp(FileSinkOperator.java:660)
	... 10 more


FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 3   Cumulative CPU: 5.18 sec   HDFS Read: 280 HDFS Write: 0 FAIL
Total MapReduce CPU Time Spent: 5 seconds 180 msec

hive (dkmdb01)> select * from testdml1;                                                                                                          
OK
testdml1.id	testdml1.id_val
1	Deepak
2	Sachin
3	Messi
4	Federer
5	Ronaldo
6	Rooney
Time taken: 0.163 seconds, Fetched: 6 row(s)

[hduser@Inceptez ~]$ hdfs dfs -ls /user/hive/warehouse/dkmdb01.db/testdml1
Found 1 items
-rw-r--r--   1 hduser supergroup         55 2016-07-12 21:38 /user/hive/warehouse/dkmdb01.db/testdml1/000000_0
====================================================================================================
hive (dkmdb01)> create table testdml2 (id_val varchar(10), id int) row format delimited fields terminated by ',' clustered by (id) into 3 buckets 
              > tblproperties ('transactional'='true');                                                                                          

FAILED: ParseException line 1:97 cannot recognize input near 'clustered' 'by' '(' in serde properties specification

hive (dkmdb01)> create table testdml2 (id_val varchar(10), id int) clustered by (id) into 3 buckets row format delimited fields terminated by ','
              > tblproperties ('transactional'='true');                                                                                          
OK
Time taken: 0.355 seconds

hive (dkmdb01)> show create table testdml2;                                                                                            
OK
createtab_stmt
CREATE TABLE `testdml2`(
  `id_val` varchar(10), 
  `id` int)
CLUSTERED BY ( 
  id) 
INTO 3 BUCKETS
ROW FORMAT DELIMITED 
  FIELDS TERMINATED BY ',' 
STORED AS INPUTFORMAT 
  'org.apache.hadoop.mapred.TextInputFormat' 
OUTPUTFORMAT 
  'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
LOCATION
  'hdfs://localhost:54310/user/hive/warehouse/dkmdb01.db/testdml2'
TBLPROPERTIES (
  'transactional'='true', 
  'transient_lastDdlTime'='1468341506')
Time taken: 0.117 seconds, Fetched: 17 row(s)

hive (dkmdb01)> insert into table testdml2 select id_val, id from testdml1;
Query ID = hduser_20160712221212_c140fb76-2d59-4fa6-8b50-873ee31638a9
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1468334110122_0003, Tracking URL = http://Inceptez:8088/proxy/application_1468334110122_0003/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1468334110122_0003
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2016-07-12 22:12:38,205 Stage-1 map = 0%,  reduce = 0%
2016-07-12 22:13:07,997 Stage-1 map = 100%,  reduce = 0%
Ended Job = job_1468334110122_0003 with errors
Error during job, obtaining debugging information...
Examining task ID: task_1468334110122_0003_m_000000 (and more) from job job_1468334110122_0003

Task with the most failures(4): 
-----
Task ID:
  task_1468334110122_0003_m_000000

URL:
  http://0.0.0.0:8088/taskdetails.jsp?jobid=job_1468334110122_0003&tipid=task_1468334110122_0003_m_000000
-----
Diagnostic Messages for this Task:
Error: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {"id":1,"id_val":"Deepak"}
	at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:185)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {"id":1,"id_val":"Deepak"}
	at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:503)
	at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:176)
	... 8 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to create RecordUpdater for HiveOutputFormat that does not implement AcidOutputFormat
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.createBucketFiles(FileSinkOperator.java:517)
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.processOp(FileSinkOperator.java:615)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)
	at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:95)
	at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:157)
	at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:493)
	... 9 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to create RecordUpdater for HiveOutputFormat that does not implement AcidOutputFormat
	at org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getAcidRecordUpdater(HiveFileFormatUtils.java:327)
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.createBucketForFileIdx(FileSinkOperator.java:573)
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.createBucketFiles(FileSinkOperator.java:506)
	... 16 more


FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1   HDFS Read: 0 HDFS Write: 0 FAIL
Total MapReduce CPU Time Spent: 0 msec
hive (dkmdb01)> set hive.support.concurrency=true;                         
hive (dkmdb01)> set hive.enforce.bucketing=true;
hive (dkmdb01)> set hive.exec.dynamic.partition.mode=nonstrict;
hive (dkmdb01)> set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;
hive (dkmdb01)> set hive.compactor.initiator.on=true;
hive (dkmdb01)> set hive.compactor.worker.threads=1 ;
hive (dkmdb01)> insert into table testdml2 select id_val, id from testdml1;         
Query ID = hduser_20160712221414_8d1e79c8-0382-456b-9a8c-2c0439f26080
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1468334110122_0004, Tracking URL = http://Inceptez:8088/proxy/application_1468334110122_0004/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1468334110122_0004
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2016-07-12 22:14:34,572 Stage-1 map = 0%,  reduce = 0%
2016-07-12 22:15:03,591 Stage-1 map = 100%,  reduce = 0%
Ended Job = job_1468334110122_0004 with errors
Error during job, obtaining debugging information...
Examining task ID: task_1468334110122_0004_m_000000 (and more) from job job_1468334110122_0004

Task with the most failures(4): 
-----
Task ID:
  task_1468334110122_0004_m_000000

URL:
  http://0.0.0.0:8088/taskdetails.jsp?jobid=job_1468334110122_0004&tipid=task_1468334110122_0004_m_000000
-----
Diagnostic Messages for this Task:
Error: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {"id":1,"id_val":"Deepak"}
	at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:185)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {"id":1,"id_val":"Deepak"}
	at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:503)
	at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:176)
	... 8 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to create RecordUpdater for HiveOutputFormat that does not implement AcidOutputFormat
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.createBucketFiles(FileSinkOperator.java:517)
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.processOp(FileSinkOperator.java:615)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)
	at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:95)
	at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:157)
	at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:493)
	... 9 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to create RecordUpdater for HiveOutputFormat that does not implement AcidOutputFormat
	at org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getAcidRecordUpdater(HiveFileFormatUtils.java:327)
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.createBucketForFileIdx(FileSinkOperator.java:573)
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.createBucketFiles(FileSinkOperator.java:506)
	... 16 more


FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1   HDFS Read: 0 HDFS Write: 0 FAIL
Total MapReduce CPU Time Spent: 0 msec

[hduser@Inceptez ~]$ hdfs dfs -ls /user/hive/warehouse/dkmdb01.db/testdml2
====================================================================================================
hive (dkmdb01)> create table testdml3 (id_val varchar(10), id int) clustered by (id) into 3 buckets tblproperties (transactional=true);          
FAILED: ParseException line 1:99 cannot recognize input near 'transactional' '=' 'true' in table properties list

hive (dkmdb01)> create table testdml3 (id_val varchar(10), id int) clustered by (id) into 3 buckets tblproperties ('transactional'='true');
OK
Time taken: 0.154 seconds

hive (dkmdb01)> show create table testdml3;                                                                                                
OK
createtab_stmt
CREATE TABLE `testdml3`(
  `id_val` varchar(10), 
  `id` int)
CLUSTERED BY ( 
  id) 
INTO 3 BUCKETS
ROW FORMAT SERDE 
  'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe' 
STORED AS INPUTFORMAT 
  'org.apache.hadoop.mapred.TextInputFormat' 
OUTPUTFORMAT 
  'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
LOCATION
  'hdfs://localhost:54310/user/hive/warehouse/dkmdb01.db/testdml3'
TBLPROPERTIES (
  'transactional'='true', 
  'transient_lastDdlTime'='1468342078')
Time taken: 0.109 seconds, Fetched: 17 row(s)

hive (dkmdb01)> insert into table testdml3 select id_val, id from testdml1;                                                                
Query ID = hduser_20160712221919_36125e4c-92da-43ed-a212-61f1fa8e4eff
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1468334110122_0005, Tracking URL = http://Inceptez:8088/proxy/application_1468334110122_0005/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1468334110122_0005
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2016-07-12 22:19:55,943 Stage-1 map = 0%,  reduce = 0%
2016-07-12 22:20:25,066 Stage-1 map = 100%,  reduce = 0%
Ended Job = job_1468334110122_0005 with errors
Error during job, obtaining debugging information...
Examining task ID: task_1468334110122_0005_m_000000 (and more) from job job_1468334110122_0005

Task with the most failures(4): 
-----
Task ID:
  task_1468334110122_0005_m_000000

URL:
  http://0.0.0.0:8088/taskdetails.jsp?jobid=job_1468334110122_0005&tipid=task_1468334110122_0005_m_000000
-----
Diagnostic Messages for this Task:
Error: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {"id":1,"id_val":"Deepak"}
	at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:185)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {"id":1,"id_val":"Deepak"}
	at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:503)
	at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:176)
	... 8 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to create RecordUpdater for HiveOutputFormat that does not implement AcidOutputFormat
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.createBucketFiles(FileSinkOperator.java:517)
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.processOp(FileSinkOperator.java:615)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)
	at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:95)
	at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:157)
	at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:493)
	... 9 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to create RecordUpdater for HiveOutputFormat that does not implement AcidOutputFormat
	at org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getAcidRecordUpdater(HiveFileFormatUtils.java:327)
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.createBucketForFileIdx(FileSinkOperator.java:573)
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.createBucketFiles(FileSinkOperator.java:506)
	... 16 more


FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1   HDFS Read: 0 HDFS Write: 0 FAIL
Total MapReduce CPU Time Spent: 0 msec

hive (dkmdb01)> alter table testdml3 stored as orc;
FAILED: ParseException line 1:21 cannot recognize input near 'stored' 'as' 'orc' in alter table statement

hive (dkmdb01)> show create table testdml3;
OK
createtab_stmt
CREATE TABLE `testdml3`(
  `id_val` varchar(10), 
  `id` int)
CLUSTERED BY ( 
  id) 
INTO 3 BUCKETS
ROW FORMAT SERDE 
  'org.apache.hadoop.hive.ql.io.orc.OrcSerde' 
STORED AS INPUTFORMAT 
  'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat' 
OUTPUTFORMAT 
  'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat'
LOCATION
  'hdfs://localhost:54310/user/hive/warehouse/dkmdb01.db/testdml3'
TBLPROPERTIES (
  'transactional'='true', 
  'transient_lastDdlTime'='1468342454')
Time taken: 0.149 seconds, Fetched: 17 row(s)

[hduser@Inceptez ~]$ hdfs dfs -ls -R /user/hive/warehouse/dkmdb01.db/testdml3
drwxr-xr-x   - hduser supergroup          0 2016-07-12 22:26 /user/hive/warehouse/dkmdb01.db/testdml3/delta_0000007_0000007
-rw-r--r--   1 hduser supergroup        679 2016-07-12 22:26 /user/hive/warehouse/dkmdb01.db/testdml3/delta_0000007_0000007/bucket_00000
[hduser@Inceptez ~]$ hdfs dfs -cat /user/hive/warehouse/dkmdb01.db/testdml3/delta_0000007_0000007/buc*0
ORC
<junk>

hive (dkmdb01)> update testdml3 set id_val = 'W Rooney' where id = 6;
Query ID = hduser_20160712223333_f9d4cd8c-a5d9-4114-a736-03d9429b60af
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1468334110122_0008, Tracking URL = http://Inceptez:8088/proxy/application_1468334110122_0008/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1468334110122_0008
Hadoop job information for Stage-1: number of mappers: 3; number of reducers: 3
2016-07-12 22:34:22,119 Stage-1 map = 0%,  reduce = 0%
2016-07-12 22:35:27,931 Stage-1 map = 0%,  reduce = 0%
2016-07-12 22:36:24,517 Stage-1 map = 22%,  reduce = 0%, Cumulative CPU 5.1 sec
2016-07-12 22:36:35,131 Stage-1 map = 44%,  reduce = 0%, Cumulative CPU 6.91 sec
2016-07-12 22:36:55,579 Stage-1 map = 67%,  reduce = 0%, Cumulative CPU 7.28 sec
2016-07-12 22:36:59,265 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 9.94 sec
2016-07-12 22:38:00,378 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 10.09 sec
2016-07-12 22:38:06,312 Stage-1 map = 100%,  reduce = 67%, Cumulative CPU 11.52 sec
2016-07-12 22:38:14,483 Stage-1 map = 100%,  reduce = 78%, Cumulative CPU 12.89 sec
2016-07-12 22:38:15,574 Stage-1 map = 100%,  reduce = 89%, Cumulative CPU 14.26 sec
2016-07-12 22:38:16,707 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 16.5 sec
MapReduce Total cumulative CPU time: 16 seconds 500 msec
Ended Job = job_1468334110122_0008
Loading data to table dkmdb01.testdml3
[Error 30017]: Skipping stats aggregation by error org.apache.hadoop.hive.ql.metadata.HiveException: [Error 30001]: StatsPublisher cannot be initialized. There was a error in the initialization of StatsPublisher, and retrying might help. If you dont want the query to fail because accurate statistics could not be collected, set hive.stats.reliable=false
Table dkmdb01.testdml3 stats: [numFiles=2, totalSize=1329]
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 3  Reduce: 3   Cumulative CPU: 16.5 sec   HDFS Read: 1592 HDFS Write: 654 SUCCESS
Total MapReduce CPU Time Spent: 16 seconds 500 msec
OK
_col0	_col1	_col2
Time taken: 267.895 seconds
hive (dkmdb01)> select * from testdml3;                              
OK
testdml3.id_val	testdml3.id
Deepak	1
Sachin	2
Messi	3
Federer	4
Ronaldo	5
W Rooney	6
Time taken: 0.173 seconds, Fetched: 6 row(s)

16/07/12 22:39:00 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
drwxr-xr-x   - hduser supergroup          0 2016-07-12 22:26 /user/hive/warehouse/dkmdb01.db/testdml3/delta_0000007_0000007
-rw-r--r--   1 hduser supergroup        679 2016-07-12 22:26 /user/hive/warehouse/dkmdb01.db/testdml3/delta_0000007_0000007/bucket_00000
drwxr-xr-x   - hduser supergroup          0 2016-07-12 22:38 /user/hive/warehouse/dkmdb01.db/testdml3/delta_0000009_0000009
-rw-r--r--   1 hduser supergroup        650 2016-07-12 22:38 /user/hive/warehouse/dkmdb01.db/testdml3/delta_0000009_0000009/bucket_00000
[hduser@Inceptez ~]$ hdfs dfs -cat /user/hive/warehouse/dkmdb01.db/testdml3/delta_0000009_0000009/buc*0
ORC
<junk>

hive (dkmdb01)> delete from testdml3 where id = 6;                   
Query ID = hduser_20160712224141_7e283be5-5566-42a1-82db-353079296179
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1468334110122_0009, Tracking URL = http://Inceptez:8088/proxy/application_1468334110122_0009/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1468334110122_0009
Hadoop job information for Stage-1: number of mappers: 3; number of reducers: 3
2016-07-12 22:41:22,506 Stage-1 map = 0%,  reduce = 0%
2016-07-12 22:42:17,283 Stage-1 map = 67%,  reduce = 0%, Cumulative CPU 5.37 sec
2016-07-12 22:42:26,151 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 6.42 sec
2016-07-12 22:43:00,633 Stage-1 map = 100%,  reduce = 67%, Cumulative CPU 8.45 sec
2016-07-12 22:43:01,763 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 10.32 sec
MapReduce Total cumulative CPU time: 10 seconds 320 msec
Ended Job = job_1468334110122_0009
Loading data to table dkmdb01.testdml3
[Error 30017]: Skipping stats aggregation by error org.apache.hadoop.hive.ql.metadata.HiveException: [Error 30001]: StatsPublisher cannot be initialized. There was a error in the initialization of StatsPublisher, and retrying might help. If you dont want the query to fail because accurate statistics could not be collected, set hive.stats.reliable=false
Table dkmdb01.testdml3 stats: [numFiles=3, totalSize=1833]
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 3  Reduce: 3   Cumulative CPU: 10.32 sec   HDFS Read: 2416 HDFS Write: 508 SUCCESS
Total MapReduce CPU Time Spent: 10 seconds 320 msec
OK
row__id
Time taken: 117.973 seconds
hive (dkmdb01)> select * from testdml3;
OK
testdml3.id_val	testdml3.id
Deepak	1
Sachin	2
Messi	3
Federer	4
Ronaldo	5
Time taken: 0.397 seconds, Fetched: 5 row(s)

[hduser@Inceptez ~]$ hdfs dfs -ls -R /user/hive/warehouse/dkmdb01.db/testdml3
16/07/12 22:43:57 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
drwxr-xr-x   - hduser supergroup          0 2016-07-12 22:26 /user/hive/warehouse/dkmdb01.db/testdml3/delta_0000007_0000007
-rw-r--r--   1 hduser supergroup        679 2016-07-12 22:26 /user/hive/warehouse/dkmdb01.db/testdml3/delta_0000007_0000007/bucket_00000
drwxr-xr-x   - hduser supergroup          0 2016-07-12 22:38 /user/hive/warehouse/dkmdb01.db/testdml3/delta_0000009_0000009
-rw-r--r--   1 hduser supergroup        650 2016-07-12 22:38 /user/hive/warehouse/dkmdb01.db/testdml3/delta_0000009_0000009/bucket_00000
drwxr-xr-x   - hduser supergroup          0 2016-07-12 22:43 /user/hive/warehouse/dkmdb01.db/testdml3/delta_0000010_0000010
-rw-r--r--   1 hduser supergroup        504 2016-07-12 22:43 /user/hive/warehouse/dkmdb01.db/testdml3/delta_0000010_0000010/bucket_00000

hive (dkmdb01)> insert into table testdml3 values ('Rooney',6);
Query ID = hduser_20160712224545_c751266d-8ac6-42fb-bd58-229ce94201b7
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1468334110122_0010, Tracking URL = http://Inceptez:8088/proxy/application_1468334110122_0010/
Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1468334110122_0010
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 3
2016-07-12 22:45:13,627 Stage-1 map = 0%,  reduce = 0%
2016-07-12 22:45:22,347 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.33 sec
2016-07-12 22:45:41,994 Stage-1 map = 100%,  reduce = 32%, Cumulative CPU 2.76 sec
2016-07-12 22:45:45,429 Stage-1 map = 100%,  reduce = 33%, Cumulative CPU 3.82 sec
2016-07-12 22:45:48,938 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 8.58 sec
MapReduce Total cumulative CPU time: 8 seconds 580 msec
Ended Job = job_1468334110122_0010
Loading data to table dkmdb01.testdml3
[Error 30017]: Skipping stats aggregation by error org.apache.hadoop.hive.ql.metadata.HiveException: [Error 30001]: StatsPublisher cannot be initialized. There was a error in the initialization of StatsPublisher, and retrying might help. If you dont want the query to fail because accurate statistics could not be collected, set hive.stats.reliable=false
Table dkmdb01.testdml3 stats: [numFiles=6, totalSize=2863]
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 3   Cumulative CPU: 8.58 sec   HDFS Read: 284 HDFS Write: 1042 SUCCESS
Total MapReduce CPU Time Spent: 8 seconds 580 msec
OK
_col0	_col1
Time taken: 51.102 seconds
hive (dkmdb01)> select * from testdml3;                        
OK
testdml3.id_val	testdml3.id
Deepak	1
Sachin	2
Messi	3
Federer	4
Ronaldo	5
Rooney	6
Time taken: 0.066 seconds, Fetched: 6 row(s)

[hduser@Inceptez ~]$ hdfs dfs -ls -R /user/hive/warehouse/dkmdb01.db/testdml3
16/07/12 22:46:26 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
drwxr-xr-x   - hduser supergroup          0 2016-07-12 22:26 /user/hive/warehouse/dkmdb01.db/testdml3/delta_0000007_0000007
-rw-r--r--   1 hduser supergroup        679 2016-07-12 22:26 /user/hive/warehouse/dkmdb01.db/testdml3/delta_0000007_0000007/bucket_00000
drwxr-xr-x   - hduser supergroup          0 2016-07-12 22:38 /user/hive/warehouse/dkmdb01.db/testdml3/delta_0000009_0000009
-rw-r--r--   1 hduser supergroup        650 2016-07-12 22:38 /user/hive/warehouse/dkmdb01.db/testdml3/delta_0000009_0000009/bucket_00000
drwxr-xr-x   - hduser supergroup          0 2016-07-12 22:43 /user/hive/warehouse/dkmdb01.db/testdml3/delta_0000010_0000010
-rw-r--r--   1 hduser supergroup        504 2016-07-12 22:43 /user/hive/warehouse/dkmdb01.db/testdml3/delta_0000010_0000010/bucket_00000
drwxr-xr-x   - hduser supergroup          0 2016-07-12 22:45 /user/hive/warehouse/dkmdb01.db/testdml3/delta_0000011_0000011
-rw-r--r--   1 hduser supergroup        616 2016-07-12 22:45 /user/hive/warehouse/dkmdb01.db/testdml3/delta_0000011_0000011/bucket_00000
-rw-r--r--   1 hduser supergroup        207 2016-07-12 22:45 /user/hive/warehouse/dkmdb01.db/testdml3/delta_0000011_0000011/bucket_00001
-rw-r--r--   1 hduser supergroup        207 2016-07-12 22:45 /user/hive/warehouse/dkmdb01.db/testdml3/delta_0000011_0000011/bucket_00002

hive (dkmdb01)> select * from testdml3 tablesample(bucket 1 out of 3);;
OK
testdml3.id_val	testdml3.id
Messi	3
Rooney	6
Time taken: 0.261 seconds, Fetched: 2 row(s)
hive (dkmdb01)> select * from testdml3 tablesample(bucket 2 out of 3); 
OK
testdml3.id_val	testdml3.id
Deepak	1
Federer	4
Time taken: 0.089 seconds, Fetched: 2 row(s)
hive (dkmdb01)> select * from testdml3 tablesample(bucket 3 out of 3);
OK
testdml3.id_val	testdml3.id
Sachin	2
Ronaldo	5
Time taken: 0.39 seconds, Fetched: 2 row(s)
====================================================================================================
