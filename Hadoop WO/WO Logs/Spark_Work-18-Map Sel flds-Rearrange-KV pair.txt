scala> val org = sc.textFile("file:/home/hduser/mydata01/org.csv");
<console>:20: error: value apache is not a member of org.apache.spark.rdd.RDD[String]
  org.apache.spark.sql.catalyst.encoders.OuterScopes.addOuterScope(this)
      ^

scala> val orgz = sc.textFile("file:/home/hduser/mydata01/org.csv");
orgz: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[1] at textFile at <console>:21

scala> orgz.first()
res0: String = 10,Microsoft,US

scala> val orgz_flds = orgz.map(_.split(","))
orgz_flds: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[2] at map at <console>:23

scala> orgz_flds.printSchema
<console>:26: error: value printSchema is not a member of org.apache.spark.rdd.RDD[Array[String]]
              orgz_flds.printSchema
                        ^

scala> orgz_flds.printSchema()
<console>:26: error: value printSchema is not a member of org.apache.spark.rdd.RDD[Array[String]]
              orgz_flds.printSchema()

scala> orgz_flds.take(3)
res4: Array[Array[String]] = Array(Array(10, Microsoft, US), Array(11, Macromedia, US), Array(12, Google, US))
====================================================================================================
create key-value pair
---------------------
scala> val orgz_kv = orgz.map(_.split(",")(0),_)
<console>:23: error: missing parameter type for expanded function ((x$2) => orgz.map(((x$1) => x$1.split(",")(0)), x$2))
         val orgz_kv = orgz.map(_.split(",")(0),_)
                                                ^
<console>:23: error: too many arguments for method map: (f: String => U)(implicit evidence$3: scala.reflect.ClassTag[U])org.apache.spark.rdd.RDD[U]
         val orgz_kv = orgz.map(_.split(",")(0),_)
                               ^

scala> val orgz_kv = orgz.map(x => (x.split(",")(0),x))
orgz_kv: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[3] at map at <console>:23

scala> orgz_kv.take(3)
16/09/22 23:05:26 INFO spark.SparkContext: Starting job: take at <console>:26
16/09/22 23:05:26 INFO scheduler.DAGScheduler: Got job 2 (take at <console>:26) with 1 output partitions
16/09/22 23:05:26 INFO scheduler.DAGScheduler: Final stage: ResultStage 2 (take at <console>:26)
16/09/22 23:05:26 INFO scheduler.DAGScheduler: Parents of final stage: List()
16/09/22 23:05:26 INFO scheduler.DAGScheduler: Missing parents: List()
16/09/22 23:05:26 INFO scheduler.DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[3] at map at <console>:23), which has no missing parents
16/09/22 23:05:26 INFO storage.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 3.2 KB, free 119.2 KB)
16/09/22 23:05:26 INFO storage.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 1916.0 B, free 121.1 KB)
16/09/22 23:05:26 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:49105 (size: 1916.0 B, free: 517.4 MB)
16/09/22 23:05:26 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1006
16/09/22 23:05:26 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[3] at map at <console>:23)
16/09/22 23:05:26 INFO scheduler.TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
16/09/22 23:05:26 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 2136 bytes)
16/09/22 23:05:26 INFO executor.Executor: Running task 0.0 in stage 2.0 (TID 2)
16/09/22 23:05:26 INFO rdd.HadoopRDD: Input split: file:/home/hduser/mydata01/org.csv:0+221
16/09/22 23:05:26 INFO executor.Executor: Finished task 0.0 in stage 2.0 (TID 2). 2184 bytes result sent to driver
16/09/22 23:05:26 INFO scheduler.DAGScheduler: ResultStage 2 (take at <console>:26) finished in 0.053 s
16/09/22 23:05:26 INFO scheduler.DAGScheduler: Job 2 finished: take at <console>:26, took 0.108317 s
res5: Array[(String, String)] = Array((10,10,Microsoft,US), (11,11,Macromedia,US), (12,12,Google,US))
====================================================================================================
Map Selected fields + Rearrange
-------------------------------
scala> val orgz_01 = orgz.map(x => (x.split(",")(2),x.split(",")(1))
     | )
orgz_01: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[4] at map at <console>:23

scala> orgz_01.take(3)
16/09/22 23:08:51 INFO spark.SparkContext: Starting job: take at <console>:26
16/09/22 23:08:51 INFO scheduler.DAGScheduler: Got job 3 (take at <console>:26) with 1 output partitions
16/09/22 23:08:51 INFO scheduler.DAGScheduler: Final stage: ResultStage 3 (take at <console>:26)
16/09/22 23:08:51 INFO scheduler.DAGScheduler: Parents of final stage: List()
16/09/22 23:08:51 INFO scheduler.DAGScheduler: Missing parents: List()
16/09/22 23:08:51 INFO scheduler.DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[4] at map at <console>:23), which has no missing parents
16/09/22 23:08:51 INFO storage.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 3.2 KB, free 124.3 KB)
16/09/22 23:08:51 INFO storage.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 1921.0 B, free 126.2 KB)
16/09/22 23:08:51 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on localhost:49105 (size: 1921.0 B, free: 517.4 MB)
16/09/22 23:08:51 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1006
16/09/22 23:08:51 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[4] at map at <console>:23)
16/09/22 23:08:51 INFO scheduler.TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
16/09/22 23:08:51 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, localhost, partition 0,PROCESS_LOCAL, 2136 bytes)
16/09/22 23:08:51 INFO executor.Executor: Running task 0.0 in stage 3.0 (TID 3)
16/09/22 23:08:51 INFO rdd.HadoopRDD: Input split: file:/home/hduser/mydata01/org.csv:0+221
16/09/22 23:08:51 INFO executor.Executor: Finished task 0.0 in stage 3.0 (TID 3). 2166 bytes result sent to driver
16/09/22 23:08:51 INFO scheduler.DAGScheduler: ResultStage 3 (take at <console>:26) finished in 0.049 s
16/09/22 23:08:51 INFO scheduler.DAGScheduler: Job 3 finished: take at <console>:26, took 0.082477 s
16/09/22 23:08:51 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 46 ms on localhost (1/1)
16/09/22 23:08:51 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
res6: Array[(String, String)] = Array((US,Microsoft), (US,Macromedia), (US,Google))
====================================================================================================
Rearrange + Create Key value Pair
---------------------------------
scala> val orgz_02 = orgz.map(x => ( x.split(",")(2), x.split(",")(0) + x.split(",")(1)))
orgz_02: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[5] at map at <console>:23

scala> orgz_02.take(3)
16/09/22 23:11:44 INFO spark.SparkContext: Starting job: take at <console>:26
16/09/22 23:11:44 INFO scheduler.DAGScheduler: Got job 4 (take at <console>:26) with 1 output partitions
16/09/22 23:11:44 INFO scheduler.DAGScheduler: Final stage: ResultStage 4 (take at <console>:26)
16/09/22 23:11:44 INFO scheduler.DAGScheduler: Parents of final stage: List()
16/09/22 23:11:44 INFO scheduler.DAGScheduler: Missing parents: List()
16/09/22 23:11:44 INFO scheduler.DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[5] at map at <console>:23), which has no missing parents
16/09/22 23:11:44 INFO storage.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 3.2 KB, free 109.2 KB)
16/09/22 23:11:44 INFO storage.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 1920.0 B, free 111.1 KB)
16/09/22 23:11:44 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on localhost:49105 (size: 1920.0 B, free: 517.4 MB)
16/09/22 23:11:44 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1006
16/09/22 23:11:44 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[5] at map at <console>:23)
16/09/22 23:11:44 INFO scheduler.TaskSchedulerImpl: Adding task set 4.0 with 1 tasks
16/09/22 23:11:44 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4, localhost, partition 0,PROCESS_LOCAL, 2136 bytes)
16/09/22 23:11:44 INFO executor.Executor: Running task 0.0 in stage 4.0 (TID 4)
16/09/22 23:11:44 INFO rdd.HadoopRDD: Input split: file:/home/hduser/mydata01/org.csv:0+221
16/09/22 23:11:44 INFO executor.Executor: Finished task 0.0 in stage 4.0 (TID 4). 2172 bytes result sent to driver
16/09/22 23:11:44 INFO scheduler.DAGScheduler: ResultStage 4 (take at <console>:26) finished in 0.039 s
16/09/22 23:11:44 INFO scheduler.DAGScheduler: Job 4 finished: take at <console>:26, took 0.068446 s
res7: Array[(String, String)] = Array((US,10Microsoft), (US,11Macromedia), (US,12Google))

scala> val orgz_02 = orgz.map(x => ( x.split(",")(2), x.split(",")(0) + "-" + x.split(",")(1)))
orgz_02: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[6] at map at <console>:23

scala> orgz_02.take(3)
16/09/22 23:12:18 INFO spark.SparkContext: Starting job: take at <console>:26
16/09/22 23:12:18 INFO scheduler.DAGScheduler: Got job 5 (take at <console>:26) with 1 output partitions
16/09/22 23:12:18 INFO scheduler.DAGScheduler: Final stage: ResultStage 5 (take at <console>:26)
16/09/22 23:12:18 INFO scheduler.DAGScheduler: Parents of final stage: List()
16/09/22 23:12:18 INFO scheduler.DAGScheduler: Missing parents: List()
16/09/22 23:12:18 INFO scheduler.DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[6] at map at <console>:23), which has no missing parents
16/09/22 23:12:18 INFO storage.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 3.2 KB, free 114.3 KB)
16/09/22 23:12:18 INFO storage.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 1921.0 B, free 116.2 KB)
16/09/22 23:12:18 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:49105 (size: 1921.0 B, free: 517.4 MB)
16/09/22 23:12:18 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1006
16/09/22 23:12:18 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[6] at map at <console>:23)
16/09/22 23:12:18 INFO scheduler.TaskSchedulerImpl: Adding task set 5.0 with 1 tasks
16/09/22 23:12:18 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5, localhost, partition 0,PROCESS_LOCAL, 2136 bytes)
16/09/22 23:12:18 INFO executor.Executor: Running task 0.0 in stage 5.0 (TID 5)
16/09/22 23:12:18 INFO rdd.HadoopRDD: Input split: file:/home/hduser/mydata01/org.csv:0+221
16/09/22 23:12:18 INFO executor.Executor: Finished task 0.0 in stage 5.0 (TID 5). 2175 bytes result sent to driver
16/09/22 23:12:18 INFO scheduler.DAGScheduler: ResultStage 5 (take at <console>:26) finished in 0.064 s
16/09/22 23:12:18 INFO scheduler.DAGScheduler: Job 5 finished: take at <console>:26, took 0.107825 s
res8: Array[(String, String)] = Array((US,10-Microsoft), (US,11-Macromedia), (US,12-Google))
====================================================================================================