[hduser@Inceptez mydata01]$ vi sampledata.txt
[hduser@Inceptez mydata01]$ hdfs dfs -put sampledata.txt spark_data/
[hduser@Inceptez mydata01]$ hdfs dfs -ls spark_data/
Found 3 items
-rw-r--r--   1 hduser supergroup         81 2016-07-30 12:32 spark_data/coursedetails.txt
-rw-r--r--   1 hduser supergroup         36 2016-07-31 21:19 spark_data/sampledata.txt
-rw-r--r--   1 hduser supergroup        399 2016-07-30 12:04 spark_data/spark_input.txt
====================================================================================================
scala> val sqlContext = new org.apache.spark.sql.SQLContext(sc)
sqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@6e3197f0

scala> import sqlContext.implicits._
import sqlContext.implicits._

scala> val lines = sc.textFile("hdfs:///user/hduser/spark_data/sampledata.txt")
16/07/31 21:39:35 INFO storage.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 86.5 KB, free 86.5 KB)
16/07/31 21:39:36 INFO storage.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 19.6 KB, free 106.0 KB)
16/07/31 21:39:36 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:50643 (size: 19.6 KB, free: 517.4 MB)
16/07/31 21:39:36 INFO spark.SparkContext: Created broadcast 0 from textFile at <console>:32
lines: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[1] at textFile at <console>:32

scala> val parts = lines.map(x=>x.split(","))
parts: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[2] at map at <console>:34

scala> case class Tab(id:Int,name:String,age:Int)
defined class Tab

scala> val people = parts.map(x=>Tab(x(0).toInt,x(1),x(2).toInt))
people: org.apache.spark.rdd.RDD[Tab] = MapPartitionsRDD[3] at map at <console>:38

scala> val peopleDF = people.toDF()
peopleDF: org.apache.spark.sql.DataFrame = [id: int, name: string, age: int]

scala> peopleDF.registerTempTable("peopleDF")

scala> peopleDF.show()
16/07/31 21:42:11 INFO mapred.FileInputFormat: Total input paths to process : 1
16/07/31 21:42:11 INFO spark.SparkContext: Starting job: show at <console>:43
16/07/31 21:42:12 INFO scheduler.DAGScheduler: Got job 0 (show at <console>:43) with 1 output partitions
16/07/31 21:42:12 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (show at <console>:43)
16/07/31 21:42:12 INFO scheduler.DAGScheduler: Parents of final stage: List()
16/07/31 21:42:12 INFO scheduler.DAGScheduler: Missing parents: List()
16/07/31 21:42:12 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[5] at show at <console>:43), which has no missing parents
16/07/31 21:42:12 INFO storage.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 4.0 KB, free 110.0 KB)
16/07/31 21:42:13 INFO storage.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.2 KB, free 112.1 KB)
16/07/31 21:42:13 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:50643 (size: 2.2 KB, free: 517.4 MB)
16/07/31 21:42:13 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1006
16/07/31 21:42:13 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[5] at show at <console>:43)
16/07/31 21:42:13 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/07/31 21:42:13 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,ANY, 2162 bytes)
16/07/31 21:42:13 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
16/07/31 21:42:13 INFO rdd.HadoopRDD: Input split: hdfs://localhost:54310/user/hduser/spark_data/sampledata.txt:0+36
16/07/31 21:42:13 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
16/07/31 21:42:13 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
16/07/31 21:42:13 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
16/07/31 21:42:13 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
16/07/31 21:42:13 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id
16/07/31 21:42:14 INFO executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 2578 bytes result sent to driver
16/07/31 21:42:14 INFO scheduler.DAGScheduler: ResultStage 0 (show at <console>:43) finished in 1.448 s
16/07/31 21:42:14 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1355 ms on localhost (1/1)
16/07/31 21:42:14 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/07/31 21:42:14 INFO scheduler.DAGScheduler: Job 0 finished: show at <console>:43, took 2.869050 s
+---+----+----+
| id|name| age|
+---+----+----+
|  1|   a|1000|
|  2|   b|2000|
|  3|   c|3000|
|  4|   d|4000|
+---+----+----+

scala> val peo = sqlContext.sql("SELECT name FROM peopleDF WHERE age>1000")
peo: org.apache.spark.sql.DataFrame = [name: string]

scala> peo.collect()
16/07/31 21:43:52 INFO spark.SparkContext: Starting job: collect at <console>:35
16/07/31 21:43:52 INFO scheduler.DAGScheduler: Got job 1 (collect at <console>:35) with 1 output partitions
16/07/31 21:43:52 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (collect at <console>:35)
16/07/31 21:43:52 INFO scheduler.DAGScheduler: Parents of final stage: List()
16/07/31 21:43:52 INFO scheduler.DAGScheduler: Missing parents: List()
16/07/31 21:43:52 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[8] at collect at <console>:35), which has no missing parents
16/07/31 21:43:52 INFO storage.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 7.8 KB, free 120.0 KB)
16/07/31 21:43:52 INFO storage.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.0 KB, free 124.0 KB)
16/07/31 21:43:52 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:50643 (size: 4.0 KB, free: 517.4 MB)
16/07/31 21:43:52 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1006
16/07/31 21:43:52 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[8] at collect at <console>:35)
16/07/31 21:43:52 INFO scheduler.TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
16/07/31 21:43:52 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,ANY, 2162 bytes)
16/07/31 21:43:52 INFO executor.Executor: Running task 0.0 in stage 1.0 (TID 1)
16/07/31 21:43:52 INFO rdd.HadoopRDD: Input split: hdfs://localhost:54310/user/hduser/spark_data/sampledata.txt:0+36
16/07/31 21:43:53 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on localhost:50643 in memory (size: 2.2 KB, free: 517.4 MB)
16/07/31 21:43:53 INFO spark.ContextCleaner: Cleaned accumulator 1
16/07/31 21:43:54 INFO codegen.GeneratePredicate: Code generated in 1233.063758 ms
16/07/31 21:43:54 INFO codegen.GenerateUnsafeProjection: Code generated in 33.983944 ms
16/07/31 21:43:54 INFO executor.Executor: Finished task 0.0 in stage 1.0 (TID 1). 2663 bytes result sent to driver
16/07/31 21:43:54 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2265 ms on localhost (1/1)
16/07/31 21:43:54 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
16/07/31 21:43:54 INFO scheduler.DAGScheduler: ResultStage 1 (collect at <console>:35) finished in 2.314 s
16/07/31 21:43:54 INFO scheduler.DAGScheduler: Job 1 finished: collect at <console>:35, took 2.467153 s
res2: Array[org.apache.spark.sql.Row] = Array([b], [c], [d])

scala> peo.show()
16/07/31 21:44:07 INFO spark.SparkContext: Starting job: show at <console>:35
16/07/31 21:44:07 INFO scheduler.DAGScheduler: Got job 2 (show at <console>:35) with 1 output partitions
16/07/31 21:44:07 INFO scheduler.DAGScheduler: Final stage: ResultStage 2 (show at <console>:35)
16/07/31 21:44:07 INFO scheduler.DAGScheduler: Parents of final stage: List()
16/07/31 21:44:07 INFO scheduler.DAGScheduler: Missing parents: List()
16/07/31 21:44:07 INFO scheduler.DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[12] at show at <console>:35), which has no missing parents
16/07/31 21:44:07 INFO storage.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 8.0 KB, free 125.8 KB)
16/07/31 21:44:07 INFO storage.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.1 KB, free 129.9 KB)
16/07/31 21:44:07 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:50643 (size: 4.1 KB, free: 517.4 MB)
16/07/31 21:44:07 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1006
16/07/31 21:44:07 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[12] at show at <console>:35)
16/07/31 21:44:07 INFO scheduler.TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
16/07/31 21:44:07 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,ANY, 2162 bytes)
16/07/31 21:44:07 INFO executor.Executor: Running task 0.0 in stage 2.0 (TID 2)
16/07/31 21:44:07 INFO rdd.HadoopRDD: Input split: hdfs://localhost:54310/user/hduser/spark_data/sampledata.txt:0+36
16/07/31 21:44:07 INFO codegen.GenerateSafeProjection: Code generated in 33.417192 ms
16/07/31 21:44:07 INFO executor.Executor: Finished task 0.0 in stage 2.0 (TID 2). 2684 bytes result sent to driver
16/07/31 21:44:07 INFO scheduler.DAGScheduler: ResultStage 2 (show at <console>:35) finished in 0.150 s
16/07/31 21:44:07 INFO scheduler.DAGScheduler: Job 2 finished: show at <console>:35, took 0.166024 s
+----+
|name|
+----+
|   b|
|   c|
|   d|
+----+
====================================================================================================
[hduser@Inceptez ~]$ mkdir spark_scripts
[hduser@Inceptez ~]$ cd spark_scripts/
[hduser@Inceptez spark_scripts]$ vi wordcount.py
[hduser@Inceptez spark_scripts]$ pyspark wordcount.py
WARNING: Running python applications through 'pyspark' is deprecated as of Spark 1.0.
Use ./bin/spark-submit <python file>
16/07/31 21:56:37 INFO spark.SparkContext: Running Spark version 1.6.0
16/07/31 21:56:40 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/07/31 21:56:42 WARN util.Utils: Your hostname, Inceptez resolves to a loopback address: 127.0.0.1; using 192.168.107.133 instead (on interface eth0)
16/07/31 21:56:42 WARN util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/07/31 21:56:42 INFO spark.SecurityManager: Changing view acls to: hduser
16/07/31 21:56:42 INFO spark.SecurityManager: Changing modify acls to: hduser
16/07/31 21:56:42 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hduser); users with modify permissions: Set(hduser)
16/07/31 21:56:45 INFO util.Utils: Successfully started service 'sparkDriver' on port 38098.
16/07/31 21:56:48 INFO slf4j.Slf4jLogger: Slf4jLogger started
16/07/31 21:56:48 INFO Remoting: Starting remoting
16/07/31 21:56:51 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@192.168.107.133:33956]
16/07/31 21:56:51 INFO util.Utils: Successfully started service 'sparkDriverActorSystem' on port 33956.
16/07/31 21:56:52 INFO spark.SparkEnv: Registering MapOutputTracker
16/07/31 21:56:52 INFO spark.SparkEnv: Registering BlockManagerMaster
16/07/31 21:56:52 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-9ed8ea24-582e-41d8-a3e6-53714d1e11cd
16/07/31 21:56:52 INFO storage.MemoryStore: MemoryStore started with capacity 517.4 MB
16/07/31 21:56:53 INFO spark.SparkEnv: Registering OutputCommitCoordinator
16/07/31 21:56:58 INFO server.Server: jetty-8.y.z-SNAPSHOT
16/07/31 21:56:58 WARN component.AbstractLifeCycle: FAILED SelectChannelConnector@0.0.0.0:4040: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:463)
	at sun.nio.ch.Net.bind(Net.java:455)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.spark-project.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.spark-project.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.spark-project.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.spark-project.jetty.server.Server.doStart(Server.java:293)
	at org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:252)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:262)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:262)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1964)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1955)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:262)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:136)
	at org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:481)
	at org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:481)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:481)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:234)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)
	at py4j.Gateway.invoke(Gateway.java:214)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:79)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:68)
	at py4j.GatewayConnection.run(GatewayConnection.java:209)
	at java.lang.Thread.run(Thread.java:745)
16/07/31 21:56:58 WARN component.AbstractLifeCycle: FAILED org.spark-project.jetty.server.Server@7bdd5f1b: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:463)
	at sun.nio.ch.Net.bind(Net.java:455)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.spark-project.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.spark-project.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.spark-project.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.spark-project.jetty.server.Server.doStart(Server.java:293)
	at org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:252)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:262)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:262)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1964)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1955)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:262)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:136)
	at org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:481)
	at org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:481)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:481)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:234)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)
	at py4j.Gateway.invoke(Gateway.java:214)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:79)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:68)
	at py4j.GatewayConnection.run(GatewayConnection.java:209)
	at java.lang.Thread.run(Thread.java:745)
16/07/31 21:56:58 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}
16/07/31 21:56:58 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/api,null}
16/07/31 21:56:58 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/,null}
16/07/31 21:56:58 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/static,null}
16/07/31 21:56:58 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}
16/07/31 21:56:58 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}
16/07/31 21:56:58 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/json,null}
16/07/31 21:56:58 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors,null}
16/07/31 21:56:58 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment/json,null}
16/07/31 21:56:58 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment,null}
16/07/31 21:56:58 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}
16/07/31 21:56:58 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}
16/07/31 21:56:58 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/json,null}
16/07/31 21:56:58 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage,null}
16/07/31 21:56:58 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}
16/07/31 21:56:58 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool,null}
16/07/31 21:56:58 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}
16/07/31 21:56:58 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage,null}
16/07/31 21:56:58 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/json,null}
16/07/31 21:56:58 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages,null}
16/07/31 21:56:58 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}
16/07/31 21:56:58 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job,null}
16/07/31 21:56:58 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/json,null}
16/07/31 21:56:58 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs,null}
16/07/31 21:56:58 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
16/07/31 21:57:01 INFO server.Server: jetty-8.y.z-SNAPSHOT
16/07/31 21:57:01 WARN component.AbstractLifeCycle: FAILED SelectChannelConnector@0.0.0.0:4041: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:463)
	at sun.nio.ch.Net.bind(Net.java:455)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.spark-project.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.spark-project.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.spark-project.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.spark-project.jetty.server.Server.doStart(Server.java:293)
	at org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:252)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:262)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:262)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1964)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1955)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:262)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:136)
	at org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:481)
	at org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:481)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:481)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:234)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)
	at py4j.Gateway.invoke(Gateway.java:214)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:79)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:68)
	at py4j.GatewayConnection.run(GatewayConnection.java:209)
	at java.lang.Thread.run(Thread.java:745)
16/07/31 21:57:01 WARN component.AbstractLifeCycle: FAILED org.spark-project.jetty.server.Server@35b31fd5: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:463)
	at sun.nio.ch.Net.bind(Net.java:455)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.spark-project.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.spark-project.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.spark-project.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.spark-project.jetty.server.Server.doStart(Server.java:293)
	at org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:252)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:262)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:262)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1964)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1955)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:262)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:136)
	at org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:481)
	at org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:481)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:481)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:234)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)
	at py4j.Gateway.invoke(Gateway.java:214)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:79)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:68)
	at py4j.GatewayConnection.run(GatewayConnection.java:209)
	at java.lang.Thread.run(Thread.java:745)
16/07/31 21:57:01 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}
16/07/31 21:57:01 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/api,null}
16/07/31 21:57:01 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/,null}
16/07/31 21:57:01 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/static,null}
16/07/31 21:57:01 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}
16/07/31 21:57:01 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}
16/07/31 21:57:01 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/json,null}
16/07/31 21:57:01 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors,null}
16/07/31 21:57:01 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment/json,null}
16/07/31 21:57:01 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment,null}
16/07/31 21:57:01 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}
16/07/31 21:57:01 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}
16/07/31 21:57:01 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/json,null}
16/07/31 21:57:01 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage,null}
16/07/31 21:57:01 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}
16/07/31 21:57:01 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool,null}
16/07/31 21:57:01 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}
16/07/31 21:57:01 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage,null}
16/07/31 21:57:01 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/json,null}
16/07/31 21:57:01 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages,null}
16/07/31 21:57:01 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}
16/07/31 21:57:01 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job,null}
16/07/31 21:57:01 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/json,null}
16/07/31 21:57:01 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs,null}
16/07/31 21:57:01 WARN util.Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
16/07/31 21:57:03 INFO server.Server: jetty-8.y.z-SNAPSHOT
16/07/31 21:57:04 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:4042
16/07/31 21:57:04 INFO util.Utils: Successfully started service 'SparkUI' on port 4042.
16/07/31 21:57:04 INFO ui.SparkUI: Started SparkUI at http://192.168.107.133:4042
16/07/31 21:57:09 INFO util.Utils: Copying /home/hduser/spark_scripts/wordcount.py to /tmp/spark-235355b8-bd89-471a-a6b3-7f8d1da5f49b/userFiles-3e34f373-8eca-4d08-8acd-c54da288eadd/wordcount.py
16/07/31 21:57:09 INFO spark.SparkContext: Added file file:/home/hduser/spark_scripts/wordcount.py at file:/home/hduser/spark_scripts/wordcount.py with timestamp 1469982429194
16/07/31 21:57:10 INFO executor.Executor: Starting executor ID driver on host localhost
16/07/31 21:57:11 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50329.
16/07/31 21:57:11 INFO netty.NettyBlockTransferService: Server created on 50329
16/07/31 21:57:11 INFO storage.BlockManagerMaster: Trying to register BlockManager
16/07/31 21:57:11 INFO storage.BlockManagerMasterEndpoint: Registering block manager localhost:50329 with 517.4 MB RAM, BlockManagerId(driver, localhost, 50329)
16/07/31 21:57:11 INFO storage.BlockManagerMaster: Registered BlockManager
16/07/31 21:57:19 INFO storage.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 228.8 KB, free 228.8 KB)
16/07/31 21:57:19 INFO storage.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 19.6 KB, free 248.4 KB)
16/07/31 21:57:19 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:50329 (size: 19.6 KB, free: 517.4 MB)
16/07/31 21:57:19 INFO spark.SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:-2
16/07/31 21:57:23 INFO mapred.FileInputFormat: Total input paths to process : 1
16/07/31 21:57:25 INFO spark.SparkContext: Starting job: collect at /home/hduser/spark_scripts/wordcount.py:9
16/07/31 21:57:25 INFO scheduler.DAGScheduler: Registering RDD 3 (reduceByKey at /home/hduser/spark_scripts/wordcount.py:8)
16/07/31 21:57:25 INFO scheduler.DAGScheduler: Got job 0 (collect at /home/hduser/spark_scripts/wordcount.py:9) with 1 output partitions
16/07/31 21:57:25 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (collect at /home/hduser/spark_scripts/wordcount.py:9)
16/07/31 21:57:25 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
16/07/31 21:57:25 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 0)
16/07/31 21:57:25 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /home/hduser/spark_scripts/wordcount.py:8), which has no missing parents
16/07/31 21:57:25 INFO storage.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.2 KB, free 256.6 KB)
16/07/31 21:57:25 INFO storage.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.1 KB, free 261.7 KB)
16/07/31 21:57:25 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:50329 (size: 5.1 KB, free: 517.4 MB)
16/07/31 21:57:25 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1006
16/07/31 21:57:25 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /home/hduser/spark_scripts/wordcount.py:8)
16/07/31 21:57:25 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/07/31 21:57:26 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,ANY, 2208 bytes)
16/07/31 21:57:26 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
16/07/31 21:57:26 INFO executor.Executor: Fetching file:/home/hduser/spark_scripts/wordcount.py with timestamp 1469982429194
16/07/31 21:57:26 INFO util.Utils: /home/hduser/spark_scripts/wordcount.py has been previously copied to /tmp/spark-235355b8-bd89-471a-a6b3-7f8d1da5f49b/userFiles-3e34f373-8eca-4d08-8acd-c54da288eadd/wordcount.py
16/07/31 21:57:27 INFO rdd.HadoopRDD: Input split: hdfs://localhost:54310/user/hduser/spark_data/coursedetails.txt:0+81
16/07/31 21:57:27 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
16/07/31 21:57:27 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
16/07/31 21:57:27 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
16/07/31 21:57:27 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
16/07/31 21:57:27 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id
16/07/31 21:57:33 INFO python.PythonRunner: Times: total = 6013, boot = 4183, init = 1235, finish = 595
16/07/31 21:57:34 INFO executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 2317 bytes result sent to driver
16/07/31 21:57:34 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 8103 ms on localhost (1/1)
16/07/31 21:57:34 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/07/31 21:57:34 INFO scheduler.DAGScheduler: ShuffleMapStage 0 (reduceByKey at /home/hduser/spark_scripts/wordcount.py:8) finished in 8.252 s
16/07/31 21:57:34 INFO scheduler.DAGScheduler: looking for newly runnable stages
16/07/31 21:57:34 INFO scheduler.DAGScheduler: running: Set()
16/07/31 21:57:34 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 1)
16/07/31 21:57:34 INFO scheduler.DAGScheduler: failed: Set()
16/07/31 21:57:34 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (PythonRDD[6] at collect at /home/hduser/spark_scripts/wordcount.py:9), which has no missing parents
16/07/31 21:57:34 INFO storage.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 5.0 KB, free 266.7 KB)
16/07/31 21:57:34 INFO storage.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.1 KB, free 269.8 KB)
16/07/31 21:57:34 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:50329 (size: 3.1 KB, free: 517.4 MB)
16/07/31 21:57:34 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1006
16/07/31 21:57:34 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (PythonRDD[6] at collect at /home/hduser/spark_scripts/wordcount.py:9)
16/07/31 21:57:34 INFO scheduler.TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
16/07/31 21:57:34 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,NODE_LOCAL, 1948 bytes)
16/07/31 21:57:34 INFO executor.Executor: Running task 0.0 in stage 1.0 (TID 1)
16/07/31 21:57:34 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
16/07/31 21:57:34 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 25 ms
16/07/31 21:57:34 INFO python.PythonRunner: Times: total = 23, boot = -693, init = 716, finish = 0
16/07/31 21:57:34 INFO executor.Executor: Finished task 0.0 in stage 1.0 (TID 1). 1365 bytes result sent to driver
16/07/31 21:57:34 INFO scheduler.DAGScheduler: ResultStage 1 (collect at /home/hduser/spark_scripts/wordcount.py:9) finished in 0.218 s
16/07/31 21:57:34 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 218 ms on localhost (1/1)
16/07/31 21:57:34 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
16/07/31 21:57:34 INFO scheduler.DAGScheduler: Job 0 finished: collect at /home/hduser/spark_scripts/wordcount.py:9, took 9.397992 s
[(u'sas', 1), (u'informatica', 1), (u'mainframe', 2), (u'bigdata', 3), (u'hadoop', 3)]
16/07/31 21:57:35 INFO spark.SparkContext: Starting job: saveAsTextFile at NativeMethodAccessorImpl.java:-2
16/07/31 21:57:35 INFO spark.MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 143 bytes
16/07/31 21:57:35 INFO scheduler.DAGScheduler: Got job 1 (saveAsTextFile at NativeMethodAccessorImpl.java:-2) with 1 output partitions
16/07/31 21:57:35 INFO scheduler.DAGScheduler: Final stage: ResultStage 3 (saveAsTextFile at NativeMethodAccessorImpl.java:-2)
16/07/31 21:57:35 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
16/07/31 21:57:35 INFO scheduler.DAGScheduler: Missing parents: List()
16/07/31 21:57:35 INFO scheduler.DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[9] at saveAsTextFile at NativeMethodAccessorImpl.java:-2), which has no missing parents
16/07/31 21:57:36 INFO storage.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 68.0 KB, free 337.9 KB)
16/07/31 21:57:36 INFO storage.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 24.7 KB, free 362.6 KB)
16/07/31 21:57:36 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:50329 (size: 24.7 KB, free: 517.4 MB)
16/07/31 21:57:36 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1006
16/07/31 21:57:36 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[9] at saveAsTextFile at NativeMethodAccessorImpl.java:-2)
16/07/31 21:57:36 INFO scheduler.TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
16/07/31 21:57:36 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2, localhost, partition 0,NODE_LOCAL, 1948 bytes)
16/07/31 21:57:36 INFO executor.Executor: Running task 0.0 in stage 3.0 (TID 2)
16/07/31 21:57:36 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
16/07/31 21:57:36 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
16/07/31 21:57:37 INFO python.PythonRunner: Times: total = 45, boot = -1747, init = 1792, finish = 0
16/07/31 21:57:38 INFO output.FileOutputCommitter: Saved output of task 'attempt_201607312157_0003_m_000000_2' to hdfs://localhost:54310/user/hduser/output/_temporary/0/task_201607312157_0003_m_000000
16/07/31 21:57:38 INFO mapred.SparkHadoopMapRedUtil: attempt_201607312157_0003_m_000000_2: Committed
16/07/31 21:57:38 INFO executor.Executor: Finished task 0.0 in stage 3.0 (TID 2). 2144 bytes result sent to driver
16/07/31 21:57:38 INFO scheduler.DAGScheduler: ResultStage 3 (saveAsTextFile at NativeMethodAccessorImpl.java:-2) finished in 2.273 s
16/07/31 21:57:38 INFO scheduler.DAGScheduler: Job 1 finished: saveAsTextFile at NativeMethodAccessorImpl.java:-2, took 2.477232 s
16/07/31 21:57:38 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 2279 ms on localhost (1/1)
16/07/31 21:57:38 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
16/07/31 21:57:40 INFO spark.ContextCleaner: Cleaned accumulator 2
16/07/31 21:57:40 INFO spark.SparkContext: Invoking stop() from shutdown hook
16/07/31 21:57:41 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on localhost:50329 in memory (size: 24.7 KB, free: 517.4 MB)
16/07/31 21:57:41 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/metrics/json,null}
16/07/31 21:57:41 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}
16/07/31 21:57:41 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/api,null}
16/07/31 21:57:41 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/,null}
16/07/31 21:57:41 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/static,null}
16/07/31 21:57:41 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}
16/07/31 21:57:41 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}
16/07/31 21:57:41 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/json,null}
16/07/31 21:57:41 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors,null}
16/07/31 21:57:41 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment/json,null}
16/07/31 21:57:41 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment,null}
16/07/31 21:57:41 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}
16/07/31 21:57:41 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}
16/07/31 21:57:41 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/json,null}
16/07/31 21:57:41 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage,null}
16/07/31 21:57:41 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}
16/07/31 21:57:41 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool,null}
16/07/31 21:57:41 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}
16/07/31 21:57:41 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage,null}
16/07/31 21:57:41 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/json,null}
16/07/31 21:57:41 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages,null}
16/07/31 21:57:41 INFO spark.ContextCleaner: Cleaned accumulator 4
16/07/31 21:57:41 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on localhost:50329 in memory (size: 3.1 KB, free: 517.4 MB)
16/07/31 21:57:41 INFO spark.ContextCleaner: Cleaned accumulator 3
16/07/31 21:57:41 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on localhost:50329 in memory (size: 5.1 KB, free: 517.4 MB)
16/07/31 21:57:41 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}
16/07/31 21:57:41 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job,null}
16/07/31 21:57:41 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/json,null}
16/07/31 21:57:41 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs,null}
16/07/31 21:57:41 INFO ui.SparkUI: Stopped Spark web UI at http://192.168.107.133:4042
16/07/31 21:57:41 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/07/31 21:57:41 INFO storage.MemoryStore: MemoryStore cleared
16/07/31 21:57:41 INFO storage.BlockManager: BlockManager stopped
16/07/31 21:57:41 INFO storage.BlockManagerMaster: BlockManagerMaster stopped
16/07/31 21:57:41 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/07/31 21:57:41 INFO spark.SparkContext: Successfully stopped SparkContext
16/07/31 21:57:41 INFO util.ShutdownHookManager: Shutdown hook called
16/07/31 21:57:41 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-235355b8-bd89-471a-a6b3-7f8d1da5f49b
16/07/31 21:57:41 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-235355b8-bd89-471a-a6b3-7f8d1da5f49b/pyspark-4cdb07b5-ef4d-4299-a864-ce5410bd4e92
16/07/31 21:57:41 INFO remote.RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
16/07/31 21:57:41 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
[hduser@Inceptez spark_scripts]$ ls
wordcount.py
[hduser@Inceptez spark_scripts]$ hdfs dfs -ls
16/07/31 22:01:49 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Found 24 items
drwxr-xr-x   - hduser supergroup          0 2016-07-17 11:16 TrendingPartitioner
drwxr-xr-x   - hduser supergroup          0 2016-07-13 21:26 _sqoop
drwxr-xr-x   - hduser supergroup          0 2016-07-12 20:14 data_01
drwxr-xr-x   - hduser supergroup          0 2016-07-27 22:04 hbase
drwxr-xr-x   - hduser supergroup          0 2016-07-11 21:45 hiveexternaldata
drwxr-xr-x   - hduser supergroup          0 2016-07-17 11:31 identityred
drwxr-xr-x   - hduser supergroup          0 2016-06-18 13:29 imp_del
drwxr-xr-x   - hduser supergroup          0 2016-06-18 22:40 import_all
drwxr-xr-x   - hduser supergroup          0 2016-06-19 00:08 inc_imp_lm
drwxr-xr-x   - hduser supergroup          0 2016-06-19 00:17 inc_imp_lm1
drwxr-xr-x   - hduser supergroup          0 2016-06-19 00:22 inc_imp_lm2
drwxr-xr-x   - hduser supergroup          0 2016-06-21 21:20 inc_imp_lm3_new
drwxr-xr-x   - hduser supergroup          0 2016-07-17 10:39 mrTrending
drwxr-xr-x   - hduser supergroup          0 2016-07-17 10:52 mrTrending1
drwxr-xr-x   - hduser supergroup          0 2016-07-17 10:35 mrinput
-rw-r--r--   1 hduser supergroup          4 2016-06-18 21:42 mysql_pwd.txt
drwxr-xr-x   - hduser supergroup          0 2016-07-31 21:57 output
drwxr-xr-x   - hduser supergroup          0 2016-07-16 10:02 pig_data_01
drwxr-xr-x   - hduser supergroup          0 2016-06-18 10:25 practice
drwxr-xr-x   - hduser supergroup          0 2016-07-31 21:19 spark_data
drwxr-xr-x   - hduser supergroup          0 2016-06-28 22:44 sqoopdb
drwxr-xr-x   - hduser supergroup          0 2016-07-09 16:04 sqphv01
drwxr-xr-x   - hduser supergroup          0 2016-06-05 12:20 testing
drwxr-xr-x   - hduser supergroup          0 2016-07-27 19:59 weblogs
[hduser@Inceptez spark_scripts]$ hdfs dfs -ls output
16/07/31 22:02:11 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Found 2 items
-rw-r--r--   1 hduser supergroup          0 2016-07-31 21:57 output/_SUCCESS
-rw-r--r--   1 hduser supergroup         81 2016-07-31 21:57 output/part-00000
[hduser@Inceptez spark_scripts]$ hdfs dfs -cat output/part-00000
====================================================================================================
====================================================================================================
====================================================================================================
/usr/local/spark/examples/src/main
[hduser@Inceptez main]$ ls
java  python  r  resources  scala

usr/local/spark
bin/run-example streaming.DirectKafkaWordCount localhost:9092 test

[hduser@Inceptez spark]$ pwd
/usr/local/spark
[hduser@Inceptez spark]$ bin/run-example streaming.DirectKafkaWordCount localhost:9092 tk1
====================================================================================================

[hduser@Inceptez ~]$ start-all.sh
This script is Deprecated. Instead use start-dfs.sh and start-yarn.sh
16/08/01 21:17:30 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Starting namenodes on [localhost]
localhost: starting namenode, logging to /usr/local/hadoop/logs/hadoop-hduser-namenode-Inceptez.out
localhost: starting datanode, logging to /usr/local/hadoop/logs/hadoop-hduser-datanode-Inceptez.out
Starting secondary namenodes [0.0.0.0]
0.0.0.0: starting secondarynamenode, logging to /usr/local/hadoop/logs/hadoop-hduser-secondarynamenode-Inceptez.out
16/08/01 21:18:05 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
starting yarn daemons
starting resourcemanager, logging to /usr/local/hadoop/logs/yarn-hduser-resourcemanager-Inceptez.out
localhost: starting nodemanager, logging to /usr/local/hadoop/logs/yarn-hduser-nodemanager-Inceptez.out
[hduser@Inceptez ~]$ jps
3000 SecondaryNameNode
3417 Jps
3243 NodeManager
3142 ResourceManager
2711 NameNode
2806 DataNode
[hduser@Inceptez ~]$ cd /usr/local/kafka/bin/
[hduser@Inceptez bin]$ zookeeper-server-start.sh -daemon ../config/zookeeper.properties
[hduser@Inceptez bin]$ jps
3000 SecondaryNameNode
3243 NodeManager
3581 QuorumPeerMain
3142 ResourceManager
2711 NameNode
2806 DataNode
3602 Jps
[hduser@Inceptez bin]$ kafka-server-start.sh -daemon ../config/server.properties
[hduser@Inceptez bin]$ kafka-topics.sh --list --zookeeper localhost:2181
test
tk1
tr43
[hduser@Inceptez bin]$ kafka-console-producer.sh --broker-list localhost:9092 --topic tk1
Hi
this is an input for spark streaming
1 2 3 4 5 6 7 8 9 0
a b c b d 
^C
/usr/local/spark
[hduser@Inceptez spark]$ bin/run-example streaming.DirectKafkaWordCount localhost:9092 tk1
